[
  {
    "objectID": "workshops/usfs_aeoip/GEDI_data.html",
    "href": "workshops/usfs_aeoip/GEDI_data.html",
    "title": "How to work with GEDI Level 2B V002 Data",
    "section": "",
    "text": "This tutorial was developed as a walkthrough at the USFS - NASA JOINT APPLICATIONS WORKSHOP part of Applied Earth Observations Innovation Partneship.\n\n\nThis tutorial will show how to use Python to open GEDI L2B Version 2 files, subset layer and to a region of interest, filter by quality, and visualize GEDI Elevation, Canopy Elevation, Plant Area Index,and Canopy Height along with Tandem-X DEM and Non-vegetated area from MODIS.\n\n\nA small area of eastern part of Uinta-Wasatch-Cache National Forest is used as the ROI for this tutorial.\nNote: follow the steps provided in Prerequisites/Setup Instructions section in README.md\n\n\n1. Set Up the Working Environment and Retrieve Files\n\nImport the required packages and set the input/working directory to run this Jupyter Notebook locally.\n\nimport os\nimport h5py\nimport pandas\nimport geopandas \nfrom shapely.geometry import Point\nimport geoviews\nfrom geoviews import opts, tile_sources as gvts\nimport shapely\nimport warnings\nfrom shapely.errors import ShapelyDeprecationWarning\ngeoviews.extension('bokeh','matplotlib')\nwarnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n  \n  \n\n\n\n\n\n\n\nSet up the working environment and retrieve GEDI02_B files.\n\n\nDirect link to download GEDI02_B granule used in this workflow:\n\nhttps://e4ftl01.cr.usgs.gov//GEDI_L1_L2/GEDI/GEDI02_B.002/2022.04.30/GEDI02_B_2022120091720_O19145_02_T09106_02_003_01_V002.h5?_ga=2.154728634.387355792.1681738783-1969705792.1681738783\n\n\ninDir = os.getcwd()   # Set input directory to the current working directory\nos.chdir(inDir)  \ngedi_L2B = [g for g in os.listdir() if g.startswith('GEDI02_B') and g.endswith('.h5')]  # List all GEDI L2B .h5 files in inDir\ngedi_L2B\n\n['GEDI02_B_2022120091720_O19145_02_T09106_02_003_01_V002.h5']\n\n\n\n\nThe standard format for GEDI Version 2 filenames is as follows:\n\nGEDI02_B: Product Short Name\n2022120091720: Julian Date and Time of Acquisition (YYYYDDDHHMMSS)\nO19145: Orbit Number\n02: Sub-Orbit Granule Number (1-4)\nT09106: Track Number (Reference Ground Track)\n02: Positioning and Pointing Determination System (PPDS) type (00 is predict, 01 rapid, 02 and higher is final)\n003: PGE Version Number\n01: Granule Production Version\nV002: Product Version\n\n\n\n\n2. Open a GEDI HDF5 File and Read File Metadata\n\nRead in a GEDI HDF5 file using the h5py package and navigate the HDF5 file. The GEDI HDF5 file contains groups in which data and metadata are stored.\n\nL2B = h5py.File(gedi_L2B[0], 'r')  # Read file using h5py\nL2B\n\n&lt;HDF5 file \"GEDI02_B_2022120091720_O19145_02_T09106_02_003_01_V002.h5\" (mode r)&gt;\n\n\n\nlist(L2B.keys())\n\n['BEAM0000',\n 'BEAM0001',\n 'BEAM0010',\n 'BEAM0011',\n 'BEAM0101',\n 'BEAM0110',\n 'BEAM1000',\n 'BEAM1011',\n 'METADATA']\n\n\n#### The METADATA group contains the file-level metadata such as the creation date, PGEVersion, and VersionID. Below, print the file-level metadata attributes and their values.\n\nfor g in L2B['METADATA']['DatasetIdentification'].attrs:\n    print(g, ':', L2B['METADATA']['DatasetIdentification'].attrs[g]) \n    \n\nPGEVersion : 003\nVersionID : 01\nabstract : The GEDI L2B standard data product contains precise latitude, longitude, elevation, height, cover and vertical profile metrics for each laser footprint located on the land surface.\ncharacterSet : utf8\ncreationDate : 2022-08-24T16:16:22.315776Z\ncredit : The software that generates the L2B product was implemented within the GEDI Science Data Processing System at the NASA Goddard Space Flight Center (GSFC) in Greenbelt, Maryland in collaboration with the Department of Geographical Sciences at the University of Maryland (UMD).\nfileName : GEDI02_B_2022120091720_O19145_02_T09106_02_003_01_V002.h5\nlanguage : eng\noriginatorOrganizationName : UMD/GSFC GEDI-SDPS &gt; GEDI Science Data Processing System\npurpose : The purpose of the L2B dataset is to extract biophysical metrics from each GEDI waveform. These metrics are based on the directional gap probability profile derived from the L1B waveform and include canopy cover, Plant Area Index (PAI), Plant Area Volume Density (PAVD) and Foliage Height Diversity (FHD).\nshortName : GEDI_L2B\nspatialRepresentationType : along-track\nstatus : onGoing\ntopicCategory : geoscientificInformation\nuuid : cb5bebb2-6eb6-4d4e-b38f-a017a5b8941e\n\n\n\n\n\n3. Read SDS Metadata and Subset by Beam\n\nThe GEDI instrument consists of 3 lasers producing a total of 8 beam ground transects. The eight remaining groups contain data for each of the eight GEDI beam transects. For additional information, be sure to check out: https://gedi.umd.edu/instrument/specifications/.\n\n\nOne useful piece of metadata to retrieve from each beam transect is whether it is a full power beam or a coverage beam. GEDI coverage beams will not penetrate dense forest. The GEDI coverage beams were only designed to penetrate canopies of up to 95% canopy cover under “average” conditions, so users should preference use of GEDI power beams in the case of dense forest.\n\nfor beam in list(L2B.keys()):\n    if beam == 'METADATA':\n        continue\n    else:\n        print(beam, 'is:', L2B[beam].attrs['description'])\n\nBEAM0000 is: Coverage beam\nBEAM0001 is: Coverage beam\nBEAM0010 is: Coverage beam\nBEAM0011 is: Coverage beam\nBEAM0101 is: Full power beam\nBEAM0110 is: Full power beam\nBEAM1000 is: Full power beam\nBEAM1011 is: Full power beam\n\n\n\n\nIdentify all the datasets in the GEDI HDF5 file below.\n\n# list(L2B['BEAM1011'].keys())\nL2B_objs = []\nL2B.visit(L2B_objs.append)                                           # Retrieve list of datasets\nSDS = [o for o in L2B_objs if isinstance(L2B[o], h5py.Dataset)]  # Search for relevant SDS inside data file\nSDS\n\n['BEAM0000/algorithmrun_flag',\n 'BEAM0000/ancillary/dz',\n 'BEAM0000/ancillary/l2a_alg_count',\n 'BEAM0000/ancillary/maxheight_cuttoff',\n 'BEAM0000/ancillary/rg_eg_constraint_center_buffer',\n 'BEAM0000/ancillary/rg_eg_mpfit_max_func_evals',\n 'BEAM0000/ancillary/rg_eg_mpfit_maxiters',\n 'BEAM0000/ancillary/rg_eg_mpfit_tolerance',\n 'BEAM0000/ancillary/signal_search_buff',\n 'BEAM0000/ancillary/tx_noise_stddev_multiplier',\n 'BEAM0000/beam',\n 'BEAM0000/channel',\n 'BEAM0000/cover',\n 'BEAM0000/cover_z',\n 'BEAM0000/fhd_normal',\n 'BEAM0000/geolocation/degrade_flag',\n 'BEAM0000/geolocation/delta_time',\n 'BEAM0000/geolocation/digital_elevation_model',\n 'BEAM0000/geolocation/elev_highestreturn',\n 'BEAM0000/geolocation/elev_lowestmode',\n 'BEAM0000/geolocation/elevation_bin0',\n 'BEAM0000/geolocation/elevation_bin0_error',\n 'BEAM0000/geolocation/elevation_lastbin',\n 'BEAM0000/geolocation/elevation_lastbin_error',\n 'BEAM0000/geolocation/height_bin0',\n 'BEAM0000/geolocation/height_lastbin',\n 'BEAM0000/geolocation/lat_highestreturn',\n 'BEAM0000/geolocation/lat_lowestmode',\n 'BEAM0000/geolocation/latitude_bin0',\n 'BEAM0000/geolocation/latitude_bin0_error',\n 'BEAM0000/geolocation/latitude_lastbin',\n 'BEAM0000/geolocation/latitude_lastbin_error',\n 'BEAM0000/geolocation/local_beam_azimuth',\n 'BEAM0000/geolocation/local_beam_elevation',\n 'BEAM0000/geolocation/lon_highestreturn',\n 'BEAM0000/geolocation/lon_lowestmode',\n 'BEAM0000/geolocation/longitude_bin0',\n 'BEAM0000/geolocation/longitude_bin0_error',\n 'BEAM0000/geolocation/longitude_lastbin',\n 'BEAM0000/geolocation/longitude_lastbin_error',\n 'BEAM0000/geolocation/shot_number',\n 'BEAM0000/geolocation/solar_azimuth',\n 'BEAM0000/geolocation/solar_elevation',\n 'BEAM0000/l2a_quality_flag',\n 'BEAM0000/l2b_quality_flag',\n 'BEAM0000/land_cover_data/landsat_treecover',\n 'BEAM0000/land_cover_data/landsat_water_persistence',\n 'BEAM0000/land_cover_data/leaf_off_doy',\n 'BEAM0000/land_cover_data/leaf_off_flag',\n 'BEAM0000/land_cover_data/leaf_on_cycle',\n 'BEAM0000/land_cover_data/leaf_on_doy',\n 'BEAM0000/land_cover_data/modis_nonvegetated',\n 'BEAM0000/land_cover_data/modis_nonvegetated_sd',\n 'BEAM0000/land_cover_data/modis_treecover',\n 'BEAM0000/land_cover_data/modis_treecover_sd',\n 'BEAM0000/land_cover_data/pft_class',\n 'BEAM0000/land_cover_data/region_class',\n 'BEAM0000/land_cover_data/urban_focal_window_size',\n 'BEAM0000/land_cover_data/urban_proportion',\n 'BEAM0000/master_frac',\n 'BEAM0000/master_int',\n 'BEAM0000/num_detectedmodes',\n 'BEAM0000/omega',\n 'BEAM0000/pai',\n 'BEAM0000/pai_z',\n 'BEAM0000/pavd_z',\n 'BEAM0000/pgap_theta',\n 'BEAM0000/pgap_theta_error',\n 'BEAM0000/pgap_theta_z',\n 'BEAM0000/rg',\n 'BEAM0000/rh100',\n 'BEAM0000/rhog',\n 'BEAM0000/rhog_error',\n 'BEAM0000/rhov',\n 'BEAM0000/rhov_error',\n 'BEAM0000/rossg',\n 'BEAM0000/rv',\n 'BEAM0000/rx_processing/algorithmrun_flag_a1',\n 'BEAM0000/rx_processing/algorithmrun_flag_a2',\n 'BEAM0000/rx_processing/algorithmrun_flag_a3',\n 'BEAM0000/rx_processing/algorithmrun_flag_a4',\n 'BEAM0000/rx_processing/algorithmrun_flag_a5',\n 'BEAM0000/rx_processing/algorithmrun_flag_a6',\n 'BEAM0000/rx_processing/pgap_theta_a1',\n 'BEAM0000/rx_processing/pgap_theta_a2',\n 'BEAM0000/rx_processing/pgap_theta_a3',\n 'BEAM0000/rx_processing/pgap_theta_a4',\n 'BEAM0000/rx_processing/pgap_theta_a5',\n 'BEAM0000/rx_processing/pgap_theta_a6',\n 'BEAM0000/rx_processing/pgap_theta_error_a1',\n 'BEAM0000/rx_processing/pgap_theta_error_a2',\n 'BEAM0000/rx_processing/pgap_theta_error_a3',\n 'BEAM0000/rx_processing/pgap_theta_error_a4',\n 'BEAM0000/rx_processing/pgap_theta_error_a5',\n 'BEAM0000/rx_processing/pgap_theta_error_a6',\n 'BEAM0000/rx_processing/rg_a1',\n 'BEAM0000/rx_processing/rg_a2',\n 'BEAM0000/rx_processing/rg_a3',\n 'BEAM0000/rx_processing/rg_a4',\n 'BEAM0000/rx_processing/rg_a5',\n 'BEAM0000/rx_processing/rg_a6',\n 'BEAM0000/rx_processing/rg_eg_amplitude_a1',\n 'BEAM0000/rx_processing/rg_eg_amplitude_a2',\n 'BEAM0000/rx_processing/rg_eg_amplitude_a3',\n 'BEAM0000/rx_processing/rg_eg_amplitude_a4',\n 'BEAM0000/rx_processing/rg_eg_amplitude_a5',\n 'BEAM0000/rx_processing/rg_eg_amplitude_a6',\n 'BEAM0000/rx_processing/rg_eg_amplitude_error_a1',\n 'BEAM0000/rx_processing/rg_eg_amplitude_error_a2',\n 'BEAM0000/rx_processing/rg_eg_amplitude_error_a3',\n 'BEAM0000/rx_processing/rg_eg_amplitude_error_a4',\n 'BEAM0000/rx_processing/rg_eg_amplitude_error_a5',\n 'BEAM0000/rx_processing/rg_eg_amplitude_error_a6',\n 'BEAM0000/rx_processing/rg_eg_center_a1',\n 'BEAM0000/rx_processing/rg_eg_center_a2',\n 'BEAM0000/rx_processing/rg_eg_center_a3',\n 'BEAM0000/rx_processing/rg_eg_center_a4',\n 'BEAM0000/rx_processing/rg_eg_center_a5',\n 'BEAM0000/rx_processing/rg_eg_center_a6',\n 'BEAM0000/rx_processing/rg_eg_center_error_a1',\n 'BEAM0000/rx_processing/rg_eg_center_error_a2',\n 'BEAM0000/rx_processing/rg_eg_center_error_a3',\n 'BEAM0000/rx_processing/rg_eg_center_error_a4',\n 'BEAM0000/rx_processing/rg_eg_center_error_a5',\n 'BEAM0000/rx_processing/rg_eg_center_error_a6',\n 'BEAM0000/rx_processing/rg_eg_chisq_a1',\n 'BEAM0000/rx_processing/rg_eg_chisq_a2',\n 'BEAM0000/rx_processing/rg_eg_chisq_a3',\n 'BEAM0000/rx_processing/rg_eg_chisq_a4',\n 'BEAM0000/rx_processing/rg_eg_chisq_a5',\n 'BEAM0000/rx_processing/rg_eg_chisq_a6',\n 'BEAM0000/rx_processing/rg_eg_flag_a1',\n 'BEAM0000/rx_processing/rg_eg_flag_a2',\n 'BEAM0000/rx_processing/rg_eg_flag_a3',\n 'BEAM0000/rx_processing/rg_eg_flag_a4',\n 'BEAM0000/rx_processing/rg_eg_flag_a5',\n 'BEAM0000/rx_processing/rg_eg_flag_a6',\n 'BEAM0000/rx_processing/rg_eg_gamma_a1',\n 'BEAM0000/rx_processing/rg_eg_gamma_a2',\n 'BEAM0000/rx_processing/rg_eg_gamma_a3',\n 'BEAM0000/rx_processing/rg_eg_gamma_a4',\n 'BEAM0000/rx_processing/rg_eg_gamma_a5',\n 'BEAM0000/rx_processing/rg_eg_gamma_a6',\n 'BEAM0000/rx_processing/rg_eg_gamma_error_a1',\n 'BEAM0000/rx_processing/rg_eg_gamma_error_a2',\n 'BEAM0000/rx_processing/rg_eg_gamma_error_a3',\n 'BEAM0000/rx_processing/rg_eg_gamma_error_a4',\n 'BEAM0000/rx_processing/rg_eg_gamma_error_a5',\n 'BEAM0000/rx_processing/rg_eg_gamma_error_a6',\n 'BEAM0000/rx_processing/rg_eg_niter_a1',\n 'BEAM0000/rx_processing/rg_eg_niter_a2',\n 'BEAM0000/rx_processing/rg_eg_niter_a3',\n 'BEAM0000/rx_processing/rg_eg_niter_a4',\n 'BEAM0000/rx_processing/rg_eg_niter_a5',\n 'BEAM0000/rx_processing/rg_eg_niter_a6',\n 'BEAM0000/rx_processing/rg_eg_sigma_a1',\n 'BEAM0000/rx_processing/rg_eg_sigma_a2',\n 'BEAM0000/rx_processing/rg_eg_sigma_a3',\n 'BEAM0000/rx_processing/rg_eg_sigma_a4',\n 'BEAM0000/rx_processing/rg_eg_sigma_a5',\n 'BEAM0000/rx_processing/rg_eg_sigma_a6',\n 'BEAM0000/rx_processing/rg_eg_sigma_error_a1',\n 'BEAM0000/rx_processing/rg_eg_sigma_error_a2',\n 'BEAM0000/rx_processing/rg_eg_sigma_error_a3',\n 'BEAM0000/rx_processing/rg_eg_sigma_error_a4',\n 'BEAM0000/rx_processing/rg_eg_sigma_error_a5',\n 'BEAM0000/rx_processing/rg_eg_sigma_error_a6',\n 'BEAM0000/rx_processing/rg_error_a1',\n 'BEAM0000/rx_processing/rg_error_a2',\n 'BEAM0000/rx_processing/rg_error_a3',\n 'BEAM0000/rx_processing/rg_error_a4',\n 'BEAM0000/rx_processing/rg_error_a5',\n 'BEAM0000/rx_processing/rg_error_a6',\n 'BEAM0000/rx_processing/rv_a1',\n 'BEAM0000/rx_processing/rv_a2',\n 'BEAM0000/rx_processing/rv_a3',\n 'BEAM0000/rx_processing/rv_a4',\n 'BEAM0000/rx_processing/rv_a5',\n 'BEAM0000/rx_processing/rv_a6',\n 'BEAM0000/rx_processing/rx_energy_a1',\n 'BEAM0000/rx_processing/rx_energy_a2',\n 'BEAM0000/rx_processing/rx_energy_a3',\n 'BEAM0000/rx_processing/rx_energy_a4',\n 'BEAM0000/rx_processing/rx_energy_a5',\n 'BEAM0000/rx_processing/rx_energy_a6',\n 'BEAM0000/rx_processing/shot_number',\n 'BEAM0000/rx_range_highestreturn',\n 'BEAM0000/rx_sample_count',\n 'BEAM0000/rx_sample_start_index',\n 'BEAM0000/selected_l2a_algorithm',\n 'BEAM0000/selected_mode',\n 'BEAM0000/selected_mode_flag',\n 'BEAM0000/selected_rg_algorithm',\n 'BEAM0000/sensitivity',\n 'BEAM0000/shot_number',\n 'BEAM0000/stale_return_flag',\n 'BEAM0000/surface_flag',\n 'BEAM0001/algorithmrun_flag',\n 'BEAM0001/ancillary/dz',\n 'BEAM0001/ancillary/l2a_alg_count',\n 'BEAM0001/ancillary/maxheight_cuttoff',\n 'BEAM0001/ancillary/rg_eg_constraint_center_buffer',\n 'BEAM0001/ancillary/rg_eg_mpfit_max_func_evals',\n 'BEAM0001/ancillary/rg_eg_mpfit_maxiters',\n 'BEAM0001/ancillary/rg_eg_mpfit_tolerance',\n 'BEAM0001/ancillary/signal_search_buff',\n 'BEAM0001/ancillary/tx_noise_stddev_multiplier',\n 'BEAM0001/beam',\n 'BEAM0001/channel',\n 'BEAM0001/cover',\n 'BEAM0001/cover_z',\n 'BEAM0001/fhd_normal',\n 'BEAM0001/geolocation/degrade_flag',\n 'BEAM0001/geolocation/delta_time',\n 'BEAM0001/geolocation/digital_elevation_model',\n 'BEAM0001/geolocation/elev_highestreturn',\n 'BEAM0001/geolocation/elev_lowestmode',\n 'BEAM0001/geolocation/elevation_bin0',\n 'BEAM0001/geolocation/elevation_bin0_error',\n 'BEAM0001/geolocation/elevation_lastbin',\n 'BEAM0001/geolocation/elevation_lastbin_error',\n 'BEAM0001/geolocation/height_bin0',\n 'BEAM0001/geolocation/height_lastbin',\n 'BEAM0001/geolocation/lat_highestreturn',\n 'BEAM0001/geolocation/lat_lowestmode',\n 'BEAM0001/geolocation/latitude_bin0',\n 'BEAM0001/geolocation/latitude_bin0_error',\n 'BEAM0001/geolocation/latitude_lastbin',\n 'BEAM0001/geolocation/latitude_lastbin_error',\n 'BEAM0001/geolocation/local_beam_azimuth',\n 'BEAM0001/geolocation/local_beam_elevation',\n 'BEAM0001/geolocation/lon_highestreturn',\n 'BEAM0001/geolocation/lon_lowestmode',\n 'BEAM0001/geolocation/longitude_bin0',\n 'BEAM0001/geolocation/longitude_bin0_error',\n 'BEAM0001/geolocation/longitude_lastbin',\n 'BEAM0001/geolocation/longitude_lastbin_error',\n 'BEAM0001/geolocation/shot_number',\n 'BEAM0001/geolocation/solar_azimuth',\n 'BEAM0001/geolocation/solar_elevation',\n 'BEAM0001/l2a_quality_flag',\n 'BEAM0001/l2b_quality_flag',\n 'BEAM0001/land_cover_data/landsat_treecover',\n 'BEAM0001/land_cover_data/landsat_water_persistence',\n 'BEAM0001/land_cover_data/leaf_off_doy',\n 'BEAM0001/land_cover_data/leaf_off_flag',\n 'BEAM0001/land_cover_data/leaf_on_cycle',\n 'BEAM0001/land_cover_data/leaf_on_doy',\n 'BEAM0001/land_cover_data/modis_nonvegetated',\n 'BEAM0001/land_cover_data/modis_nonvegetated_sd',\n 'BEAM0001/land_cover_data/modis_treecover',\n 'BEAM0001/land_cover_data/modis_treecover_sd',\n 'BEAM0001/land_cover_data/pft_class',\n 'BEAM0001/land_cover_data/region_class',\n 'BEAM0001/land_cover_data/urban_focal_window_size',\n 'BEAM0001/land_cover_data/urban_proportion',\n 'BEAM0001/master_frac',\n 'BEAM0001/master_int',\n 'BEAM0001/num_detectedmodes',\n 'BEAM0001/omega',\n 'BEAM0001/pai',\n 'BEAM0001/pai_z',\n 'BEAM0001/pavd_z',\n 'BEAM0001/pgap_theta',\n 'BEAM0001/pgap_theta_error',\n 'BEAM0001/pgap_theta_z',\n 'BEAM0001/rg',\n 'BEAM0001/rh100',\n 'BEAM0001/rhog',\n 'BEAM0001/rhog_error',\n 'BEAM0001/rhov',\n 'BEAM0001/rhov_error',\n 'BEAM0001/rossg',\n 'BEAM0001/rv',\n 'BEAM0001/rx_processing/algorithmrun_flag_a1',\n 'BEAM0001/rx_processing/algorithmrun_flag_a2',\n 'BEAM0001/rx_processing/algorithmrun_flag_a3',\n 'BEAM0001/rx_processing/algorithmrun_flag_a4',\n 'BEAM0001/rx_processing/algorithmrun_flag_a5',\n 'BEAM0001/rx_processing/algorithmrun_flag_a6',\n 'BEAM0001/rx_processing/pgap_theta_a1',\n 'BEAM0001/rx_processing/pgap_theta_a2',\n 'BEAM0001/rx_processing/pgap_theta_a3',\n 'BEAM0001/rx_processing/pgap_theta_a4',\n 'BEAM0001/rx_processing/pgap_theta_a5',\n 'BEAM0001/rx_processing/pgap_theta_a6',\n 'BEAM0001/rx_processing/pgap_theta_error_a1',\n 'BEAM0001/rx_processing/pgap_theta_error_a2',\n 'BEAM0001/rx_processing/pgap_theta_error_a3',\n 'BEAM0001/rx_processing/pgap_theta_error_a4',\n 'BEAM0001/rx_processing/pgap_theta_error_a5',\n 'BEAM0001/rx_processing/pgap_theta_error_a6',\n 'BEAM0001/rx_processing/rg_a1',\n 'BEAM0001/rx_processing/rg_a2',\n 'BEAM0001/rx_processing/rg_a3',\n 'BEAM0001/rx_processing/rg_a4',\n 'BEAM0001/rx_processing/rg_a5',\n 'BEAM0001/rx_processing/rg_a6',\n 'BEAM0001/rx_processing/rg_eg_amplitude_a1',\n 'BEAM0001/rx_processing/rg_eg_amplitude_a2',\n 'BEAM0001/rx_processing/rg_eg_amplitude_a3',\n 'BEAM0001/rx_processing/rg_eg_amplitude_a4',\n 'BEAM0001/rx_processing/rg_eg_amplitude_a5',\n 'BEAM0001/rx_processing/rg_eg_amplitude_a6',\n 'BEAM0001/rx_processing/rg_eg_amplitude_error_a1',\n 'BEAM0001/rx_processing/rg_eg_amplitude_error_a2',\n 'BEAM0001/rx_processing/rg_eg_amplitude_error_a3',\n 'BEAM0001/rx_processing/rg_eg_amplitude_error_a4',\n 'BEAM0001/rx_processing/rg_eg_amplitude_error_a5',\n 'BEAM0001/rx_processing/rg_eg_amplitude_error_a6',\n 'BEAM0001/rx_processing/rg_eg_center_a1',\n 'BEAM0001/rx_processing/rg_eg_center_a2',\n 'BEAM0001/rx_processing/rg_eg_center_a3',\n 'BEAM0001/rx_processing/rg_eg_center_a4',\n 'BEAM0001/rx_processing/rg_eg_center_a5',\n 'BEAM0001/rx_processing/rg_eg_center_a6',\n 'BEAM0001/rx_processing/rg_eg_center_error_a1',\n 'BEAM0001/rx_processing/rg_eg_center_error_a2',\n 'BEAM0001/rx_processing/rg_eg_center_error_a3',\n 'BEAM0001/rx_processing/rg_eg_center_error_a4',\n 'BEAM0001/rx_processing/rg_eg_center_error_a5',\n 'BEAM0001/rx_processing/rg_eg_center_error_a6',\n 'BEAM0001/rx_processing/rg_eg_chisq_a1',\n 'BEAM0001/rx_processing/rg_eg_chisq_a2',\n 'BEAM0001/rx_processing/rg_eg_chisq_a3',\n 'BEAM0001/rx_processing/rg_eg_chisq_a4',\n 'BEAM0001/rx_processing/rg_eg_chisq_a5',\n 'BEAM0001/rx_processing/rg_eg_chisq_a6',\n 'BEAM0001/rx_processing/rg_eg_flag_a1',\n 'BEAM0001/rx_processing/rg_eg_flag_a2',\n 'BEAM0001/rx_processing/rg_eg_flag_a3',\n 'BEAM0001/rx_processing/rg_eg_flag_a4',\n 'BEAM0001/rx_processing/rg_eg_flag_a5',\n 'BEAM0001/rx_processing/rg_eg_flag_a6',\n 'BEAM0001/rx_processing/rg_eg_gamma_a1',\n 'BEAM0001/rx_processing/rg_eg_gamma_a2',\n 'BEAM0001/rx_processing/rg_eg_gamma_a3',\n 'BEAM0001/rx_processing/rg_eg_gamma_a4',\n 'BEAM0001/rx_processing/rg_eg_gamma_a5',\n 'BEAM0001/rx_processing/rg_eg_gamma_a6',\n 'BEAM0001/rx_processing/rg_eg_gamma_error_a1',\n 'BEAM0001/rx_processing/rg_eg_gamma_error_a2',\n 'BEAM0001/rx_processing/rg_eg_gamma_error_a3',\n 'BEAM0001/rx_processing/rg_eg_gamma_error_a4',\n 'BEAM0001/rx_processing/rg_eg_gamma_error_a5',\n 'BEAM0001/rx_processing/rg_eg_gamma_error_a6',\n 'BEAM0001/rx_processing/rg_eg_niter_a1',\n 'BEAM0001/rx_processing/rg_eg_niter_a2',\n 'BEAM0001/rx_processing/rg_eg_niter_a3',\n 'BEAM0001/rx_processing/rg_eg_niter_a4',\n 'BEAM0001/rx_processing/rg_eg_niter_a5',\n 'BEAM0001/rx_processing/rg_eg_niter_a6',\n 'BEAM0001/rx_processing/rg_eg_sigma_a1',\n 'BEAM0001/rx_processing/rg_eg_sigma_a2',\n 'BEAM0001/rx_processing/rg_eg_sigma_a3',\n 'BEAM0001/rx_processing/rg_eg_sigma_a4',\n 'BEAM0001/rx_processing/rg_eg_sigma_a5',\n 'BEAM0001/rx_processing/rg_eg_sigma_a6',\n 'BEAM0001/rx_processing/rg_eg_sigma_error_a1',\n 'BEAM0001/rx_processing/rg_eg_sigma_error_a2',\n 'BEAM0001/rx_processing/rg_eg_sigma_error_a3',\n 'BEAM0001/rx_processing/rg_eg_sigma_error_a4',\n 'BEAM0001/rx_processing/rg_eg_sigma_error_a5',\n 'BEAM0001/rx_processing/rg_eg_sigma_error_a6',\n 'BEAM0001/rx_processing/rg_error_a1',\n 'BEAM0001/rx_processing/rg_error_a2',\n 'BEAM0001/rx_processing/rg_error_a3',\n 'BEAM0001/rx_processing/rg_error_a4',\n 'BEAM0001/rx_processing/rg_error_a5',\n 'BEAM0001/rx_processing/rg_error_a6',\n 'BEAM0001/rx_processing/rv_a1',\n 'BEAM0001/rx_processing/rv_a2',\n 'BEAM0001/rx_processing/rv_a3',\n 'BEAM0001/rx_processing/rv_a4',\n 'BEAM0001/rx_processing/rv_a5',\n 'BEAM0001/rx_processing/rv_a6',\n 'BEAM0001/rx_processing/rx_energy_a1',\n 'BEAM0001/rx_processing/rx_energy_a2',\n 'BEAM0001/rx_processing/rx_energy_a3',\n 'BEAM0001/rx_processing/rx_energy_a4',\n 'BEAM0001/rx_processing/rx_energy_a5',\n 'BEAM0001/rx_processing/rx_energy_a6',\n 'BEAM0001/rx_processing/shot_number',\n 'BEAM0001/rx_range_highestreturn',\n 'BEAM0001/rx_sample_count',\n 'BEAM0001/rx_sample_start_index',\n 'BEAM0001/selected_l2a_algorithm',\n 'BEAM0001/selected_mode',\n 'BEAM0001/selected_mode_flag',\n 'BEAM0001/selected_rg_algorithm',\n 'BEAM0001/sensitivity',\n 'BEAM0001/shot_number',\n 'BEAM0001/stale_return_flag',\n 'BEAM0001/surface_flag',\n 'BEAM0010/algorithmrun_flag',\n 'BEAM0010/ancillary/dz',\n 'BEAM0010/ancillary/l2a_alg_count',\n 'BEAM0010/ancillary/maxheight_cuttoff',\n 'BEAM0010/ancillary/rg_eg_constraint_center_buffer',\n 'BEAM0010/ancillary/rg_eg_mpfit_max_func_evals',\n 'BEAM0010/ancillary/rg_eg_mpfit_maxiters',\n 'BEAM0010/ancillary/rg_eg_mpfit_tolerance',\n 'BEAM0010/ancillary/signal_search_buff',\n 'BEAM0010/ancillary/tx_noise_stddev_multiplier',\n 'BEAM0010/beam',\n 'BEAM0010/channel',\n 'BEAM0010/cover',\n 'BEAM0010/cover_z',\n 'BEAM0010/fhd_normal',\n 'BEAM0010/geolocation/degrade_flag',\n 'BEAM0010/geolocation/delta_time',\n 'BEAM0010/geolocation/digital_elevation_model',\n 'BEAM0010/geolocation/elev_highestreturn',\n 'BEAM0010/geolocation/elev_lowestmode',\n 'BEAM0010/geolocation/elevation_bin0',\n 'BEAM0010/geolocation/elevation_bin0_error',\n 'BEAM0010/geolocation/elevation_lastbin',\n 'BEAM0010/geolocation/elevation_lastbin_error',\n 'BEAM0010/geolocation/height_bin0',\n 'BEAM0010/geolocation/height_lastbin',\n 'BEAM0010/geolocation/lat_highestreturn',\n 'BEAM0010/geolocation/lat_lowestmode',\n 'BEAM0010/geolocation/latitude_bin0',\n 'BEAM0010/geolocation/latitude_bin0_error',\n 'BEAM0010/geolocation/latitude_lastbin',\n 'BEAM0010/geolocation/latitude_lastbin_error',\n 'BEAM0010/geolocation/local_beam_azimuth',\n 'BEAM0010/geolocation/local_beam_elevation',\n 'BEAM0010/geolocation/lon_highestreturn',\n 'BEAM0010/geolocation/lon_lowestmode',\n 'BEAM0010/geolocation/longitude_bin0',\n 'BEAM0010/geolocation/longitude_bin0_error',\n 'BEAM0010/geolocation/longitude_lastbin',\n 'BEAM0010/geolocation/longitude_lastbin_error',\n 'BEAM0010/geolocation/shot_number',\n 'BEAM0010/geolocation/solar_azimuth',\n 'BEAM0010/geolocation/solar_elevation',\n 'BEAM0010/l2a_quality_flag',\n 'BEAM0010/l2b_quality_flag',\n 'BEAM0010/land_cover_data/landsat_treecover',\n 'BEAM0010/land_cover_data/landsat_water_persistence',\n 'BEAM0010/land_cover_data/leaf_off_doy',\n 'BEAM0010/land_cover_data/leaf_off_flag',\n 'BEAM0010/land_cover_data/leaf_on_cycle',\n 'BEAM0010/land_cover_data/leaf_on_doy',\n 'BEAM0010/land_cover_data/modis_nonvegetated',\n 'BEAM0010/land_cover_data/modis_nonvegetated_sd',\n 'BEAM0010/land_cover_data/modis_treecover',\n 'BEAM0010/land_cover_data/modis_treecover_sd',\n 'BEAM0010/land_cover_data/pft_class',\n 'BEAM0010/land_cover_data/region_class',\n 'BEAM0010/land_cover_data/urban_focal_window_size',\n 'BEAM0010/land_cover_data/urban_proportion',\n 'BEAM0010/master_frac',\n 'BEAM0010/master_int',\n 'BEAM0010/num_detectedmodes',\n 'BEAM0010/omega',\n 'BEAM0010/pai',\n 'BEAM0010/pai_z',\n 'BEAM0010/pavd_z',\n 'BEAM0010/pgap_theta',\n 'BEAM0010/pgap_theta_error',\n 'BEAM0010/pgap_theta_z',\n 'BEAM0010/rg',\n 'BEAM0010/rh100',\n 'BEAM0010/rhog',\n 'BEAM0010/rhog_error',\n 'BEAM0010/rhov',\n 'BEAM0010/rhov_error',\n 'BEAM0010/rossg',\n 'BEAM0010/rv',\n 'BEAM0010/rx_processing/algorithmrun_flag_a1',\n 'BEAM0010/rx_processing/algorithmrun_flag_a2',\n 'BEAM0010/rx_processing/algorithmrun_flag_a3',\n 'BEAM0010/rx_processing/algorithmrun_flag_a4',\n 'BEAM0010/rx_processing/algorithmrun_flag_a5',\n 'BEAM0010/rx_processing/algorithmrun_flag_a6',\n 'BEAM0010/rx_processing/pgap_theta_a1',\n 'BEAM0010/rx_processing/pgap_theta_a2',\n 'BEAM0010/rx_processing/pgap_theta_a3',\n 'BEAM0010/rx_processing/pgap_theta_a4',\n 'BEAM0010/rx_processing/pgap_theta_a5',\n 'BEAM0010/rx_processing/pgap_theta_a6',\n 'BEAM0010/rx_processing/pgap_theta_error_a1',\n 'BEAM0010/rx_processing/pgap_theta_error_a2',\n 'BEAM0010/rx_processing/pgap_theta_error_a3',\n 'BEAM0010/rx_processing/pgap_theta_error_a4',\n 'BEAM0010/rx_processing/pgap_theta_error_a5',\n 'BEAM0010/rx_processing/pgap_theta_error_a6',\n 'BEAM0010/rx_processing/rg_a1',\n 'BEAM0010/rx_processing/rg_a2',\n 'BEAM0010/rx_processing/rg_a3',\n 'BEAM0010/rx_processing/rg_a4',\n 'BEAM0010/rx_processing/rg_a5',\n 'BEAM0010/rx_processing/rg_a6',\n 'BEAM0010/rx_processing/rg_eg_amplitude_a1',\n 'BEAM0010/rx_processing/rg_eg_amplitude_a2',\n 'BEAM0010/rx_processing/rg_eg_amplitude_a3',\n 'BEAM0010/rx_processing/rg_eg_amplitude_a4',\n 'BEAM0010/rx_processing/rg_eg_amplitude_a5',\n 'BEAM0010/rx_processing/rg_eg_amplitude_a6',\n 'BEAM0010/rx_processing/rg_eg_amplitude_error_a1',\n 'BEAM0010/rx_processing/rg_eg_amplitude_error_a2',\n 'BEAM0010/rx_processing/rg_eg_amplitude_error_a3',\n 'BEAM0010/rx_processing/rg_eg_amplitude_error_a4',\n 'BEAM0010/rx_processing/rg_eg_amplitude_error_a5',\n 'BEAM0010/rx_processing/rg_eg_amplitude_error_a6',\n 'BEAM0010/rx_processing/rg_eg_center_a1',\n 'BEAM0010/rx_processing/rg_eg_center_a2',\n 'BEAM0010/rx_processing/rg_eg_center_a3',\n 'BEAM0010/rx_processing/rg_eg_center_a4',\n 'BEAM0010/rx_processing/rg_eg_center_a5',\n 'BEAM0010/rx_processing/rg_eg_center_a6',\n 'BEAM0010/rx_processing/rg_eg_center_error_a1',\n 'BEAM0010/rx_processing/rg_eg_center_error_a2',\n 'BEAM0010/rx_processing/rg_eg_center_error_a3',\n 'BEAM0010/rx_processing/rg_eg_center_error_a4',\n 'BEAM0010/rx_processing/rg_eg_center_error_a5',\n 'BEAM0010/rx_processing/rg_eg_center_error_a6',\n 'BEAM0010/rx_processing/rg_eg_chisq_a1',\n 'BEAM0010/rx_processing/rg_eg_chisq_a2',\n 'BEAM0010/rx_processing/rg_eg_chisq_a3',\n 'BEAM0010/rx_processing/rg_eg_chisq_a4',\n 'BEAM0010/rx_processing/rg_eg_chisq_a5',\n 'BEAM0010/rx_processing/rg_eg_chisq_a6',\n 'BEAM0010/rx_processing/rg_eg_flag_a1',\n 'BEAM0010/rx_processing/rg_eg_flag_a2',\n 'BEAM0010/rx_processing/rg_eg_flag_a3',\n 'BEAM0010/rx_processing/rg_eg_flag_a4',\n 'BEAM0010/rx_processing/rg_eg_flag_a5',\n 'BEAM0010/rx_processing/rg_eg_flag_a6',\n 'BEAM0010/rx_processing/rg_eg_gamma_a1',\n 'BEAM0010/rx_processing/rg_eg_gamma_a2',\n 'BEAM0010/rx_processing/rg_eg_gamma_a3',\n 'BEAM0010/rx_processing/rg_eg_gamma_a4',\n 'BEAM0010/rx_processing/rg_eg_gamma_a5',\n 'BEAM0010/rx_processing/rg_eg_gamma_a6',\n 'BEAM0010/rx_processing/rg_eg_gamma_error_a1',\n 'BEAM0010/rx_processing/rg_eg_gamma_error_a2',\n 'BEAM0010/rx_processing/rg_eg_gamma_error_a3',\n 'BEAM0010/rx_processing/rg_eg_gamma_error_a4',\n 'BEAM0010/rx_processing/rg_eg_gamma_error_a5',\n 'BEAM0010/rx_processing/rg_eg_gamma_error_a6',\n 'BEAM0010/rx_processing/rg_eg_niter_a1',\n 'BEAM0010/rx_processing/rg_eg_niter_a2',\n 'BEAM0010/rx_processing/rg_eg_niter_a3',\n 'BEAM0010/rx_processing/rg_eg_niter_a4',\n 'BEAM0010/rx_processing/rg_eg_niter_a5',\n 'BEAM0010/rx_processing/rg_eg_niter_a6',\n 'BEAM0010/rx_processing/rg_eg_sigma_a1',\n 'BEAM0010/rx_processing/rg_eg_sigma_a2',\n 'BEAM0010/rx_processing/rg_eg_sigma_a3',\n 'BEAM0010/rx_processing/rg_eg_sigma_a4',\n 'BEAM0010/rx_processing/rg_eg_sigma_a5',\n 'BEAM0010/rx_processing/rg_eg_sigma_a6',\n 'BEAM0010/rx_processing/rg_eg_sigma_error_a1',\n 'BEAM0010/rx_processing/rg_eg_sigma_error_a2',\n 'BEAM0010/rx_processing/rg_eg_sigma_error_a3',\n 'BEAM0010/rx_processing/rg_eg_sigma_error_a4',\n 'BEAM0010/rx_processing/rg_eg_sigma_error_a5',\n 'BEAM0010/rx_processing/rg_eg_sigma_error_a6',\n 'BEAM0010/rx_processing/rg_error_a1',\n 'BEAM0010/rx_processing/rg_error_a2',\n 'BEAM0010/rx_processing/rg_error_a3',\n 'BEAM0010/rx_processing/rg_error_a4',\n 'BEAM0010/rx_processing/rg_error_a5',\n 'BEAM0010/rx_processing/rg_error_a6',\n 'BEAM0010/rx_processing/rv_a1',\n 'BEAM0010/rx_processing/rv_a2',\n 'BEAM0010/rx_processing/rv_a3',\n 'BEAM0010/rx_processing/rv_a4',\n 'BEAM0010/rx_processing/rv_a5',\n 'BEAM0010/rx_processing/rv_a6',\n 'BEAM0010/rx_processing/rx_energy_a1',\n 'BEAM0010/rx_processing/rx_energy_a2',\n 'BEAM0010/rx_processing/rx_energy_a3',\n 'BEAM0010/rx_processing/rx_energy_a4',\n 'BEAM0010/rx_processing/rx_energy_a5',\n 'BEAM0010/rx_processing/rx_energy_a6',\n 'BEAM0010/rx_processing/shot_number',\n 'BEAM0010/rx_range_highestreturn',\n 'BEAM0010/rx_sample_count',\n 'BEAM0010/rx_sample_start_index',\n 'BEAM0010/selected_l2a_algorithm',\n 'BEAM0010/selected_mode',\n 'BEAM0010/selected_mode_flag',\n 'BEAM0010/selected_rg_algorithm',\n 'BEAM0010/sensitivity',\n 'BEAM0010/shot_number',\n 'BEAM0010/stale_return_flag',\n 'BEAM0010/surface_flag',\n 'BEAM0011/algorithmrun_flag',\n 'BEAM0011/ancillary/dz',\n 'BEAM0011/ancillary/l2a_alg_count',\n 'BEAM0011/ancillary/maxheight_cuttoff',\n 'BEAM0011/ancillary/rg_eg_constraint_center_buffer',\n 'BEAM0011/ancillary/rg_eg_mpfit_max_func_evals',\n 'BEAM0011/ancillary/rg_eg_mpfit_maxiters',\n 'BEAM0011/ancillary/rg_eg_mpfit_tolerance',\n 'BEAM0011/ancillary/signal_search_buff',\n 'BEAM0011/ancillary/tx_noise_stddev_multiplier',\n 'BEAM0011/beam',\n 'BEAM0011/channel',\n 'BEAM0011/cover',\n 'BEAM0011/cover_z',\n 'BEAM0011/fhd_normal',\n 'BEAM0011/geolocation/degrade_flag',\n 'BEAM0011/geolocation/delta_time',\n 'BEAM0011/geolocation/digital_elevation_model',\n 'BEAM0011/geolocation/elev_highestreturn',\n 'BEAM0011/geolocation/elev_lowestmode',\n 'BEAM0011/geolocation/elevation_bin0',\n 'BEAM0011/geolocation/elevation_bin0_error',\n 'BEAM0011/geolocation/elevation_lastbin',\n 'BEAM0011/geolocation/elevation_lastbin_error',\n 'BEAM0011/geolocation/height_bin0',\n 'BEAM0011/geolocation/height_lastbin',\n 'BEAM0011/geolocation/lat_highestreturn',\n 'BEAM0011/geolocation/lat_lowestmode',\n 'BEAM0011/geolocation/latitude_bin0',\n 'BEAM0011/geolocation/latitude_bin0_error',\n 'BEAM0011/geolocation/latitude_lastbin',\n 'BEAM0011/geolocation/latitude_lastbin_error',\n 'BEAM0011/geolocation/local_beam_azimuth',\n 'BEAM0011/geolocation/local_beam_elevation',\n 'BEAM0011/geolocation/lon_highestreturn',\n 'BEAM0011/geolocation/lon_lowestmode',\n 'BEAM0011/geolocation/longitude_bin0',\n 'BEAM0011/geolocation/longitude_bin0_error',\n 'BEAM0011/geolocation/longitude_lastbin',\n 'BEAM0011/geolocation/longitude_lastbin_error',\n 'BEAM0011/geolocation/shot_number',\n 'BEAM0011/geolocation/solar_azimuth',\n 'BEAM0011/geolocation/solar_elevation',\n 'BEAM0011/l2a_quality_flag',\n 'BEAM0011/l2b_quality_flag',\n 'BEAM0011/land_cover_data/landsat_treecover',\n 'BEAM0011/land_cover_data/landsat_water_persistence',\n 'BEAM0011/land_cover_data/leaf_off_doy',\n 'BEAM0011/land_cover_data/leaf_off_flag',\n 'BEAM0011/land_cover_data/leaf_on_cycle',\n 'BEAM0011/land_cover_data/leaf_on_doy',\n 'BEAM0011/land_cover_data/modis_nonvegetated',\n 'BEAM0011/land_cover_data/modis_nonvegetated_sd',\n 'BEAM0011/land_cover_data/modis_treecover',\n 'BEAM0011/land_cover_data/modis_treecover_sd',\n 'BEAM0011/land_cover_data/pft_class',\n 'BEAM0011/land_cover_data/region_class',\n 'BEAM0011/land_cover_data/urban_focal_window_size',\n 'BEAM0011/land_cover_data/urban_proportion',\n 'BEAM0011/master_frac',\n 'BEAM0011/master_int',\n 'BEAM0011/num_detectedmodes',\n 'BEAM0011/omega',\n 'BEAM0011/pai',\n 'BEAM0011/pai_z',\n 'BEAM0011/pavd_z',\n 'BEAM0011/pgap_theta',\n 'BEAM0011/pgap_theta_error',\n 'BEAM0011/pgap_theta_z',\n 'BEAM0011/rg',\n 'BEAM0011/rh100',\n 'BEAM0011/rhog',\n 'BEAM0011/rhog_error',\n 'BEAM0011/rhov',\n 'BEAM0011/rhov_error',\n 'BEAM0011/rossg',\n 'BEAM0011/rv',\n 'BEAM0011/rx_processing/algorithmrun_flag_a1',\n 'BEAM0011/rx_processing/algorithmrun_flag_a2',\n 'BEAM0011/rx_processing/algorithmrun_flag_a3',\n 'BEAM0011/rx_processing/algorithmrun_flag_a4',\n 'BEAM0011/rx_processing/algorithmrun_flag_a5',\n 'BEAM0011/rx_processing/algorithmrun_flag_a6',\n 'BEAM0011/rx_processing/pgap_theta_a1',\n 'BEAM0011/rx_processing/pgap_theta_a2',\n 'BEAM0011/rx_processing/pgap_theta_a3',\n 'BEAM0011/rx_processing/pgap_theta_a4',\n 'BEAM0011/rx_processing/pgap_theta_a5',\n 'BEAM0011/rx_processing/pgap_theta_a6',\n 'BEAM0011/rx_processing/pgap_theta_error_a1',\n 'BEAM0011/rx_processing/pgap_theta_error_a2',\n 'BEAM0011/rx_processing/pgap_theta_error_a3',\n 'BEAM0011/rx_processing/pgap_theta_error_a4',\n 'BEAM0011/rx_processing/pgap_theta_error_a5',\n 'BEAM0011/rx_processing/pgap_theta_error_a6',\n 'BEAM0011/rx_processing/rg_a1',\n 'BEAM0011/rx_processing/rg_a2',\n 'BEAM0011/rx_processing/rg_a3',\n 'BEAM0011/rx_processing/rg_a4',\n 'BEAM0011/rx_processing/rg_a5',\n 'BEAM0011/rx_processing/rg_a6',\n 'BEAM0011/rx_processing/rg_eg_amplitude_a1',\n 'BEAM0011/rx_processing/rg_eg_amplitude_a2',\n 'BEAM0011/rx_processing/rg_eg_amplitude_a3',\n 'BEAM0011/rx_processing/rg_eg_amplitude_a4',\n 'BEAM0011/rx_processing/rg_eg_amplitude_a5',\n 'BEAM0011/rx_processing/rg_eg_amplitude_a6',\n 'BEAM0011/rx_processing/rg_eg_amplitude_error_a1',\n 'BEAM0011/rx_processing/rg_eg_amplitude_error_a2',\n 'BEAM0011/rx_processing/rg_eg_amplitude_error_a3',\n 'BEAM0011/rx_processing/rg_eg_amplitude_error_a4',\n 'BEAM0011/rx_processing/rg_eg_amplitude_error_a5',\n 'BEAM0011/rx_processing/rg_eg_amplitude_error_a6',\n 'BEAM0011/rx_processing/rg_eg_center_a1',\n 'BEAM0011/rx_processing/rg_eg_center_a2',\n 'BEAM0011/rx_processing/rg_eg_center_a3',\n 'BEAM0011/rx_processing/rg_eg_center_a4',\n 'BEAM0011/rx_processing/rg_eg_center_a5',\n 'BEAM0011/rx_processing/rg_eg_center_a6',\n 'BEAM0011/rx_processing/rg_eg_center_error_a1',\n 'BEAM0011/rx_processing/rg_eg_center_error_a2',\n 'BEAM0011/rx_processing/rg_eg_center_error_a3',\n 'BEAM0011/rx_processing/rg_eg_center_error_a4',\n 'BEAM0011/rx_processing/rg_eg_center_error_a5',\n 'BEAM0011/rx_processing/rg_eg_center_error_a6',\n 'BEAM0011/rx_processing/rg_eg_chisq_a1',\n 'BEAM0011/rx_processing/rg_eg_chisq_a2',\n 'BEAM0011/rx_processing/rg_eg_chisq_a3',\n 'BEAM0011/rx_processing/rg_eg_chisq_a4',\n 'BEAM0011/rx_processing/rg_eg_chisq_a5',\n 'BEAM0011/rx_processing/rg_eg_chisq_a6',\n 'BEAM0011/rx_processing/rg_eg_flag_a1',\n 'BEAM0011/rx_processing/rg_eg_flag_a2',\n 'BEAM0011/rx_processing/rg_eg_flag_a3',\n 'BEAM0011/rx_processing/rg_eg_flag_a4',\n 'BEAM0011/rx_processing/rg_eg_flag_a5',\n 'BEAM0011/rx_processing/rg_eg_flag_a6',\n 'BEAM0011/rx_processing/rg_eg_gamma_a1',\n 'BEAM0011/rx_processing/rg_eg_gamma_a2',\n 'BEAM0011/rx_processing/rg_eg_gamma_a3',\n 'BEAM0011/rx_processing/rg_eg_gamma_a4',\n 'BEAM0011/rx_processing/rg_eg_gamma_a5',\n 'BEAM0011/rx_processing/rg_eg_gamma_a6',\n 'BEAM0011/rx_processing/rg_eg_gamma_error_a1',\n 'BEAM0011/rx_processing/rg_eg_gamma_error_a2',\n 'BEAM0011/rx_processing/rg_eg_gamma_error_a3',\n 'BEAM0011/rx_processing/rg_eg_gamma_error_a4',\n 'BEAM0011/rx_processing/rg_eg_gamma_error_a5',\n 'BEAM0011/rx_processing/rg_eg_gamma_error_a6',\n 'BEAM0011/rx_processing/rg_eg_niter_a1',\n 'BEAM0011/rx_processing/rg_eg_niter_a2',\n 'BEAM0011/rx_processing/rg_eg_niter_a3',\n 'BEAM0011/rx_processing/rg_eg_niter_a4',\n 'BEAM0011/rx_processing/rg_eg_niter_a5',\n 'BEAM0011/rx_processing/rg_eg_niter_a6',\n 'BEAM0011/rx_processing/rg_eg_sigma_a1',\n 'BEAM0011/rx_processing/rg_eg_sigma_a2',\n 'BEAM0011/rx_processing/rg_eg_sigma_a3',\n 'BEAM0011/rx_processing/rg_eg_sigma_a4',\n 'BEAM0011/rx_processing/rg_eg_sigma_a5',\n 'BEAM0011/rx_processing/rg_eg_sigma_a6',\n 'BEAM0011/rx_processing/rg_eg_sigma_error_a1',\n 'BEAM0011/rx_processing/rg_eg_sigma_error_a2',\n 'BEAM0011/rx_processing/rg_eg_sigma_error_a3',\n 'BEAM0011/rx_processing/rg_eg_sigma_error_a4',\n 'BEAM0011/rx_processing/rg_eg_sigma_error_a5',\n 'BEAM0011/rx_processing/rg_eg_sigma_error_a6',\n 'BEAM0011/rx_processing/rg_error_a1',\n 'BEAM0011/rx_processing/rg_error_a2',\n 'BEAM0011/rx_processing/rg_error_a3',\n 'BEAM0011/rx_processing/rg_error_a4',\n 'BEAM0011/rx_processing/rg_error_a5',\n 'BEAM0011/rx_processing/rg_error_a6',\n 'BEAM0011/rx_processing/rv_a1',\n 'BEAM0011/rx_processing/rv_a2',\n 'BEAM0011/rx_processing/rv_a3',\n 'BEAM0011/rx_processing/rv_a4',\n 'BEAM0011/rx_processing/rv_a5',\n 'BEAM0011/rx_processing/rv_a6',\n 'BEAM0011/rx_processing/rx_energy_a1',\n 'BEAM0011/rx_processing/rx_energy_a2',\n 'BEAM0011/rx_processing/rx_energy_a3',\n 'BEAM0011/rx_processing/rx_energy_a4',\n 'BEAM0011/rx_processing/rx_energy_a5',\n 'BEAM0011/rx_processing/rx_energy_a6',\n 'BEAM0011/rx_processing/shot_number',\n 'BEAM0011/rx_range_highestreturn',\n 'BEAM0011/rx_sample_count',\n 'BEAM0011/rx_sample_start_index',\n 'BEAM0011/selected_l2a_algorithm',\n 'BEAM0011/selected_mode',\n 'BEAM0011/selected_mode_flag',\n 'BEAM0011/selected_rg_algorithm',\n 'BEAM0011/sensitivity',\n 'BEAM0011/shot_number',\n 'BEAM0011/stale_return_flag',\n 'BEAM0011/surface_flag',\n 'BEAM0101/algorithmrun_flag',\n 'BEAM0101/ancillary/dz',\n 'BEAM0101/ancillary/l2a_alg_count',\n 'BEAM0101/ancillary/maxheight_cuttoff',\n 'BEAM0101/ancillary/rg_eg_constraint_center_buffer',\n 'BEAM0101/ancillary/rg_eg_mpfit_max_func_evals',\n 'BEAM0101/ancillary/rg_eg_mpfit_maxiters',\n 'BEAM0101/ancillary/rg_eg_mpfit_tolerance',\n 'BEAM0101/ancillary/signal_search_buff',\n 'BEAM0101/ancillary/tx_noise_stddev_multiplier',\n 'BEAM0101/beam',\n 'BEAM0101/channel',\n 'BEAM0101/cover',\n 'BEAM0101/cover_z',\n 'BEAM0101/fhd_normal',\n 'BEAM0101/geolocation/degrade_flag',\n 'BEAM0101/geolocation/delta_time',\n 'BEAM0101/geolocation/digital_elevation_model',\n 'BEAM0101/geolocation/elev_highestreturn',\n 'BEAM0101/geolocation/elev_lowestmode',\n 'BEAM0101/geolocation/elevation_bin0',\n 'BEAM0101/geolocation/elevation_bin0_error',\n 'BEAM0101/geolocation/elevation_lastbin',\n 'BEAM0101/geolocation/elevation_lastbin_error',\n 'BEAM0101/geolocation/height_bin0',\n 'BEAM0101/geolocation/height_lastbin',\n 'BEAM0101/geolocation/lat_highestreturn',\n 'BEAM0101/geolocation/lat_lowestmode',\n 'BEAM0101/geolocation/latitude_bin0',\n 'BEAM0101/geolocation/latitude_bin0_error',\n 'BEAM0101/geolocation/latitude_lastbin',\n 'BEAM0101/geolocation/latitude_lastbin_error',\n 'BEAM0101/geolocation/local_beam_azimuth',\n 'BEAM0101/geolocation/local_beam_elevation',\n 'BEAM0101/geolocation/lon_highestreturn',\n 'BEAM0101/geolocation/lon_lowestmode',\n 'BEAM0101/geolocation/longitude_bin0',\n 'BEAM0101/geolocation/longitude_bin0_error',\n 'BEAM0101/geolocation/longitude_lastbin',\n 'BEAM0101/geolocation/longitude_lastbin_error',\n 'BEAM0101/geolocation/shot_number',\n 'BEAM0101/geolocation/solar_azimuth',\n 'BEAM0101/geolocation/solar_elevation',\n 'BEAM0101/l2a_quality_flag',\n 'BEAM0101/l2b_quality_flag',\n 'BEAM0101/land_cover_data/landsat_treecover',\n 'BEAM0101/land_cover_data/landsat_water_persistence',\n 'BEAM0101/land_cover_data/leaf_off_doy',\n 'BEAM0101/land_cover_data/leaf_off_flag',\n 'BEAM0101/land_cover_data/leaf_on_cycle',\n 'BEAM0101/land_cover_data/leaf_on_doy',\n 'BEAM0101/land_cover_data/modis_nonvegetated',\n 'BEAM0101/land_cover_data/modis_nonvegetated_sd',\n 'BEAM0101/land_cover_data/modis_treecover',\n 'BEAM0101/land_cover_data/modis_treecover_sd',\n 'BEAM0101/land_cover_data/pft_class',\n 'BEAM0101/land_cover_data/region_class',\n 'BEAM0101/land_cover_data/urban_focal_window_size',\n 'BEAM0101/land_cover_data/urban_proportion',\n 'BEAM0101/master_frac',\n 'BEAM0101/master_int',\n 'BEAM0101/num_detectedmodes',\n 'BEAM0101/omega',\n 'BEAM0101/pai',\n 'BEAM0101/pai_z',\n 'BEAM0101/pavd_z',\n 'BEAM0101/pgap_theta',\n 'BEAM0101/pgap_theta_error',\n 'BEAM0101/pgap_theta_z',\n 'BEAM0101/rg',\n 'BEAM0101/rh100',\n 'BEAM0101/rhog',\n 'BEAM0101/rhog_error',\n 'BEAM0101/rhov',\n 'BEAM0101/rhov_error',\n 'BEAM0101/rossg',\n 'BEAM0101/rv',\n 'BEAM0101/rx_processing/algorithmrun_flag_a1',\n 'BEAM0101/rx_processing/algorithmrun_flag_a2',\n 'BEAM0101/rx_processing/algorithmrun_flag_a3',\n 'BEAM0101/rx_processing/algorithmrun_flag_a4',\n 'BEAM0101/rx_processing/algorithmrun_flag_a5',\n 'BEAM0101/rx_processing/algorithmrun_flag_a6',\n 'BEAM0101/rx_processing/pgap_theta_a1',\n 'BEAM0101/rx_processing/pgap_theta_a2',\n 'BEAM0101/rx_processing/pgap_theta_a3',\n 'BEAM0101/rx_processing/pgap_theta_a4',\n 'BEAM0101/rx_processing/pgap_theta_a5',\n 'BEAM0101/rx_processing/pgap_theta_a6',\n 'BEAM0101/rx_processing/pgap_theta_error_a1',\n 'BEAM0101/rx_processing/pgap_theta_error_a2',\n 'BEAM0101/rx_processing/pgap_theta_error_a3',\n 'BEAM0101/rx_processing/pgap_theta_error_a4',\n 'BEAM0101/rx_processing/pgap_theta_error_a5',\n 'BEAM0101/rx_processing/pgap_theta_error_a6',\n 'BEAM0101/rx_processing/rg_a1',\n 'BEAM0101/rx_processing/rg_a2',\n 'BEAM0101/rx_processing/rg_a3',\n 'BEAM0101/rx_processing/rg_a4',\n 'BEAM0101/rx_processing/rg_a5',\n 'BEAM0101/rx_processing/rg_a6',\n 'BEAM0101/rx_processing/rg_eg_amplitude_a1',\n 'BEAM0101/rx_processing/rg_eg_amplitude_a2',\n 'BEAM0101/rx_processing/rg_eg_amplitude_a3',\n 'BEAM0101/rx_processing/rg_eg_amplitude_a4',\n 'BEAM0101/rx_processing/rg_eg_amplitude_a5',\n 'BEAM0101/rx_processing/rg_eg_amplitude_a6',\n 'BEAM0101/rx_processing/rg_eg_amplitude_error_a1',\n 'BEAM0101/rx_processing/rg_eg_amplitude_error_a2',\n 'BEAM0101/rx_processing/rg_eg_amplitude_error_a3',\n 'BEAM0101/rx_processing/rg_eg_amplitude_error_a4',\n 'BEAM0101/rx_processing/rg_eg_amplitude_error_a5',\n 'BEAM0101/rx_processing/rg_eg_amplitude_error_a6',\n 'BEAM0101/rx_processing/rg_eg_center_a1',\n 'BEAM0101/rx_processing/rg_eg_center_a2',\n 'BEAM0101/rx_processing/rg_eg_center_a3',\n 'BEAM0101/rx_processing/rg_eg_center_a4',\n 'BEAM0101/rx_processing/rg_eg_center_a5',\n 'BEAM0101/rx_processing/rg_eg_center_a6',\n 'BEAM0101/rx_processing/rg_eg_center_error_a1',\n 'BEAM0101/rx_processing/rg_eg_center_error_a2',\n 'BEAM0101/rx_processing/rg_eg_center_error_a3',\n 'BEAM0101/rx_processing/rg_eg_center_error_a4',\n 'BEAM0101/rx_processing/rg_eg_center_error_a5',\n 'BEAM0101/rx_processing/rg_eg_center_error_a6',\n 'BEAM0101/rx_processing/rg_eg_chisq_a1',\n 'BEAM0101/rx_processing/rg_eg_chisq_a2',\n 'BEAM0101/rx_processing/rg_eg_chisq_a3',\n 'BEAM0101/rx_processing/rg_eg_chisq_a4',\n 'BEAM0101/rx_processing/rg_eg_chisq_a5',\n 'BEAM0101/rx_processing/rg_eg_chisq_a6',\n 'BEAM0101/rx_processing/rg_eg_flag_a1',\n 'BEAM0101/rx_processing/rg_eg_flag_a2',\n 'BEAM0101/rx_processing/rg_eg_flag_a3',\n 'BEAM0101/rx_processing/rg_eg_flag_a4',\n 'BEAM0101/rx_processing/rg_eg_flag_a5',\n 'BEAM0101/rx_processing/rg_eg_flag_a6',\n 'BEAM0101/rx_processing/rg_eg_gamma_a1',\n 'BEAM0101/rx_processing/rg_eg_gamma_a2',\n 'BEAM0101/rx_processing/rg_eg_gamma_a3',\n 'BEAM0101/rx_processing/rg_eg_gamma_a4',\n 'BEAM0101/rx_processing/rg_eg_gamma_a5',\n 'BEAM0101/rx_processing/rg_eg_gamma_a6',\n 'BEAM0101/rx_processing/rg_eg_gamma_error_a1',\n 'BEAM0101/rx_processing/rg_eg_gamma_error_a2',\n 'BEAM0101/rx_processing/rg_eg_gamma_error_a3',\n 'BEAM0101/rx_processing/rg_eg_gamma_error_a4',\n 'BEAM0101/rx_processing/rg_eg_gamma_error_a5',\n 'BEAM0101/rx_processing/rg_eg_gamma_error_a6',\n 'BEAM0101/rx_processing/rg_eg_niter_a1',\n 'BEAM0101/rx_processing/rg_eg_niter_a2',\n 'BEAM0101/rx_processing/rg_eg_niter_a3',\n 'BEAM0101/rx_processing/rg_eg_niter_a4',\n 'BEAM0101/rx_processing/rg_eg_niter_a5',\n 'BEAM0101/rx_processing/rg_eg_niter_a6',\n 'BEAM0101/rx_processing/rg_eg_sigma_a1',\n 'BEAM0101/rx_processing/rg_eg_sigma_a2',\n 'BEAM0101/rx_processing/rg_eg_sigma_a3',\n 'BEAM0101/rx_processing/rg_eg_sigma_a4',\n 'BEAM0101/rx_processing/rg_eg_sigma_a5',\n 'BEAM0101/rx_processing/rg_eg_sigma_a6',\n 'BEAM0101/rx_processing/rg_eg_sigma_error_a1',\n 'BEAM0101/rx_processing/rg_eg_sigma_error_a2',\n 'BEAM0101/rx_processing/rg_eg_sigma_error_a3',\n 'BEAM0101/rx_processing/rg_eg_sigma_error_a4',\n 'BEAM0101/rx_processing/rg_eg_sigma_error_a5',\n 'BEAM0101/rx_processing/rg_eg_sigma_error_a6',\n 'BEAM0101/rx_processing/rg_error_a1',\n 'BEAM0101/rx_processing/rg_error_a2',\n 'BEAM0101/rx_processing/rg_error_a3',\n 'BEAM0101/rx_processing/rg_error_a4',\n 'BEAM0101/rx_processing/rg_error_a5',\n 'BEAM0101/rx_processing/rg_error_a6',\n 'BEAM0101/rx_processing/rv_a1',\n 'BEAM0101/rx_processing/rv_a2',\n 'BEAM0101/rx_processing/rv_a3',\n 'BEAM0101/rx_processing/rv_a4',\n 'BEAM0101/rx_processing/rv_a5',\n 'BEAM0101/rx_processing/rv_a6',\n 'BEAM0101/rx_processing/rx_energy_a1',\n 'BEAM0101/rx_processing/rx_energy_a2',\n 'BEAM0101/rx_processing/rx_energy_a3',\n 'BEAM0101/rx_processing/rx_energy_a4',\n 'BEAM0101/rx_processing/rx_energy_a5',\n 'BEAM0101/rx_processing/rx_energy_a6',\n 'BEAM0101/rx_processing/shot_number',\n 'BEAM0101/rx_range_highestreturn',\n 'BEAM0101/rx_sample_count',\n 'BEAM0101/rx_sample_start_index',\n 'BEAM0101/selected_l2a_algorithm',\n 'BEAM0101/selected_mode',\n 'BEAM0101/selected_mode_flag',\n 'BEAM0101/selected_rg_algorithm',\n 'BEAM0101/sensitivity',\n 'BEAM0101/shot_number',\n 'BEAM0101/stale_return_flag',\n 'BEAM0101/surface_flag',\n 'BEAM0110/algorithmrun_flag',\n 'BEAM0110/ancillary/dz',\n 'BEAM0110/ancillary/l2a_alg_count',\n 'BEAM0110/ancillary/maxheight_cuttoff',\n 'BEAM0110/ancillary/rg_eg_constraint_center_buffer',\n 'BEAM0110/ancillary/rg_eg_mpfit_max_func_evals',\n 'BEAM0110/ancillary/rg_eg_mpfit_maxiters',\n 'BEAM0110/ancillary/rg_eg_mpfit_tolerance',\n 'BEAM0110/ancillary/signal_search_buff',\n 'BEAM0110/ancillary/tx_noise_stddev_multiplier',\n 'BEAM0110/beam',\n 'BEAM0110/channel',\n 'BEAM0110/cover',\n 'BEAM0110/cover_z',\n 'BEAM0110/fhd_normal',\n ...]\n\n\n\nlen(SDS)\n\n1576\n\n\n\n\n\n4. Subset by Layer and Filter by Quality\n\nbelow are the list of datasets will be read and then used to generate a pandas dataframe.\n\n\n\n\n\n\n\n\nLabel\nDescription\nUnits\n\n\n\n\nlat_lowestmode\nLatitude of center of lowest mode\ndegree\n\n\nlon_lowestmode\nLongitude of center of lowest mode\ndegree\n\n\nelev_lowestmode\nelevation of center of lowest mode relative to reference ellipsoid\nm\n\n\nshot_number\nUnique shot ID\ncounter\n\n\nl2b_quality_flag\nFlag simpilfying selection of most useful data for Level 2B**\n-\n\n\ndegrade_flag\nNon-zero values indicate the shot occured during a degraded period. A non-zero tens digit indicates degraded attitude, a non-zero ones digit indicates a degraded trajectory. 3X=ADF CHU solution unavailable (ST-2); 4X=Platform attitude; 5X=Poor solution (filter covariance large); 6X=Data outage (platform attitude gap also); 7X=ST 1+2 unavailable (similar boresight FOV); 8X=ST 1+2+3 unavailable; 9X=ST 1+2+3 and ISS unavailable; X1=Maneuver; X2=GPS data gap; X3=ST blinding; X4=Other; X5=GPS receiver clock drift; X6=X5+X1; X7=X5+X2; X8=X5+X3; X9=X5+X4\n-\n\n\npai\nTotal plant area index\nm2/m2\n\n\nrh100\nHeight above ground of the received waveform signal start (rh[101] from L2A)\ncm\n\n\ndigital_elevation_model\nDigital elevation model height above the WGS84 ellipsoid. Interpolated at latitude_bin0 and longitude_bin0 from the TandemX 90m product\nm\n\n\nmodis_nonvegetated\nPercent non-vegetated from MODIS data. Interpolated at latitude_bin0 and longitude_bin0\npercent\n\n\n\n** quality_flag is a summation of several individual quality assessment parameters and other flags and is intended to provide general guidance only. A quality_flag value of 1 indicates the cover and vertical profile metrics represent the land surface and meet criteria based on waveform shot energy, sensitivity, amplitude, and real-time surface tracking quality, and the quality of extended Gaussian fitting to the lowest mode.\n\n\ncolumns= ['Beam', 'Shot Number', 'Longitude', 'Latitude', 'Quality Flag', 'Tandem-X DEM',\n                                          'Canopy Elevation (m)','Elevation (m)', 'Plant Area Index', 'Canopy Height/rh100 (cm)',\n                                          'non-vegetated from MODIS (percent)', 'Degrade Flag']\nlatslons_all = pandas.DataFrame(columns=columns)\n\nbeamNames = ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110',  'BEAM1000', 'BEAM1011' ]\nfor beamname in beamNames:\n    # Open the SDS\n    lats = L2B[f'{beamname}/geolocation/lat_lowestmode'][()]\n    lons = L2B[f'{beamname}/geolocation/lon_lowestmode'][()]\n    elevs = L2B[f'{beamname}/geolocation/elev_lowestmode'][()]\n    shots = L2B[f'{beamname}/geolocation/shot_number'][()].astype(str)\n    quality = L2B[f'{beamname}/l2b_quality_flag'][()]\n    pai = L2B[f'{beamname}/pai'][()]\n    rh100 = L2B[f'{beamname}/rh100'][()]\n    degrade_flag = L2B[f'{beamname}/geolocation/degrade_flag'][()] \n    dem =  L2B[f'{beamname}/geolocation/digital_elevation_model'][()]\n    canopy = L2B[f'{beamname}/geolocation/elev_highestreturn'][()]\n    modis_veg = L2B[f'{beamname}/land_cover_data/modis_nonvegetated'][()]\n\n    latslons = pandas.DataFrame({'Beam':beamname, 'Shot Number':shots, 'Longitude':lons, 'Latitude':lats, 'Quality Flag':quality,\n                                 'Tandem-X DEM':dem, 'Canopy Elevation (m)':canopy, 'Elevation (m)':elevs, 'Plant Area Index':pai, \n                                 'Canopy Height/rh100 (cm)':rh100, 'non-vegetated from MODIS (percent)':modis_veg, 'Degrade Flag':degrade_flag}) \n        \n\n    latslons_all = pandas.concat([latslons_all, latslons],join=\"inner\")\n\nlatslons_all\n\n\n\n\n\n\n\n\nBeam\nShot Number\nLongitude\nLatitude\nQuality Flag\nTandem-X DEM\nCanopy Elevation (m)\nElevation (m)\nPlant Area Index\nCanopy Height/rh100 (cm)\nnon-vegetated from MODIS (percent)\nDegrade Flag\n\n\n\n\n0\nBEAM0000\n191450000200044950\n-134.826959\n20.454284\n0\n-999999.000000\n8690.881836\n8690.881836\n-9999.000000\n0\n-9999.0\n0\n\n\n1\nBEAM0000\n191450000200044951\n-134.826788\n20.454486\n0\n-999999.000000\n8690.892578\n8690.892578\n-9999.000000\n0\n-9999.0\n0\n\n\n2\nBEAM0000\n191450000200044952\n-134.826616\n20.454687\n0\n-999999.000000\n8690.902344\n8690.902344\n-9999.000000\n0\n-9999.0\n0\n\n\n3\nBEAM0000\n191450000200044953\n-134.826445\n20.454889\n0\n-999999.000000\n8690.913086\n8690.913086\n-9999.000000\n0\n-9999.0\n0\n\n\n4\nBEAM0000\n191450000200044954\n-134.826274\n20.455091\n0\n-999999.000000\n8691.520508\n8691.520508\n-9999.000000\n0\n-9999.0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n116686\nBEAM1011\n191451100200157799\n-66.732469\n51.778879\n0\n575.726562\n2517.487549\n2480.030518\n-9999.000000\n3745\n11.0\n0\n\n\n116687\nBEAM1011\n191451100200157800\n-66.731667\n51.778883\n0\n577.237915\n2457.362305\n2456.914062\n-9999.000000\n43\n11.0\n0\n\n\n116688\nBEAM1011\n191451100200157801\n-66.730790\n51.778890\n0\n577.237915\n2527.554688\n2499.284668\n-9999.000000\n2827\n11.0\n0\n\n\n116689\nBEAM1011\n191451100200157802\n-66.729918\n51.778898\n0\n580.382568\n2560.982666\n2536.708496\n4.039585\n2427\n11.0\n0\n\n\n116690\nBEAM1011\n191451100200157803\n-66.729060\n51.778904\n0\n581.518860\n2568.459717\n2561.886963\n-9999.000000\n657\n25.0\n0\n\n\n\n\n948552 rows × 12 columns\n\n\n\nBelow, is the plot of entire sub-orbit included in this granule. Plotting this could take a long time, and might include information that we do not need necessarily. So lets make a subset of data to only keep the data we need.\n\n\n\nimage.png\n\n\n\n\nbelow, the shots that occured during a degraded period and low quality are removed from the dataframe.\n\nlatslons_all  = latslons_all [latslons_all ['Degrade Flag'] == 0].drop(columns = 'Degrade Flag') \nlatslons_all = latslons_all [latslons_all['Quality Flag'] == 1].drop(columns = 'Quality Flag') \n \n\n\n# reset the index and drop the NAs\n\n\nlatslons_all = latslons_all[latslons_all['Tandem-X DEM'] != -999999.0]\nlatslons_all = latslons_all[latslons_all['non-vegetated from MODIS (percent)'] != -9999.0] \nlatslons_all = latslons_all.dropna() \nlatslons_all = latslons_all.reset_index(drop=True)\nlatslons_all\n\n\n\n\n\n\n\n\nBeam\nShot Number\nLongitude\nLatitude\nTandem-X DEM\nCanopy Elevation (m)\nElevation (m)\nPlant Area Index\nCanopy Height/rh100 (cm)\nnon-vegetated from MODIS (percent)\n\n\n\n\n0\nBEAM0000\n191450000200081940\n-120.463823\n34.488103\n199.594803\n196.925323\n183.583496\n1.352260\n1334\n15.0\n\n\n1\nBEAM0000\n191450000200081941\n-120.463359\n34.488465\n199.621231\n210.334656\n203.645050\n0.295318\n667\n15.0\n\n\n2\nBEAM0000\n191450000200081949\n-120.459737\n34.491300\n213.059143\n228.421555\n209.324432\n1.824727\n1909\n9.0\n\n\n3\nBEAM0000\n191450000200081950\n-120.459272\n34.491663\n235.767044\n244.728012\n230.078156\n1.431416\n1464\n9.0\n\n\n4\nBEAM0000\n191450000200081951\n-120.458817\n34.492019\n235.767044\n243.235764\n234.864410\n0.475982\n836\n6.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n274530\nBEAM1011\n191451100200155427\n-68.703371\n51.748007\n509.907318\n513.940735\n506.507111\n1.281197\n742\n32.0\n\n\n274531\nBEAM1011\n191451100200155428\n-68.702538\n51.748026\n515.884949\n522.243958\n512.270203\n0.228271\n997\n37.0\n\n\n274532\nBEAM1011\n191451100200155429\n-68.701715\n51.748045\n514.225098\n523.028931\n510.178894\n0.794760\n1285\n37.0\n\n\n274533\nBEAM1011\n191451100200155430\n-68.700894\n51.748064\n514.225098\n519.418213\n506.007843\n0.680019\n1340\n37.0\n\n\n274534\nBEAM1011\n191451100200155431\n-68.700070\n51.748083\n509.531982\n516.337219\n504.495789\n0.486418\n1184\n37.0\n\n\n\n\n274535 rows × 10 columns\n\n\n\n\n\n\n5. Create a Geodataframe and Subset Spatially\n\nBelow, an additional column is created and called ‘geometry’ that contains a shapely point generated from each lat/lon location from the shot is created. Next, the dataframe is converted to a Geopandas GeoDataFrame.\n\n# Take the lat/lon dataframe and convert each lat/lon to a shapely point and convert to a Geodataframe\nlatslons_all = geopandas.GeoDataFrame(latslons_all, geometry=latslons_all.apply(lambda row: Point(row.Longitude, row.Latitude), axis=1))\nlatslons_all = latslons_all.set_crs('EPSG:4326')\n\n\n\nImport a GeoJSON of a small eastern section of Uinta-Wasatch-Cache National Forest as an additional GeoDataFrame.\n\nROI = geopandas.GeoDataFrame.from_file('NationalForest.geojson')\nROI.crs = 'EPSG:4326'\n\n\n\nNext, filter the shots that are within the ROI boundaries.\n\nshot_list = []\nfor num, geom in enumerate(latslons_all['geometry']):\n    if ROI.contains(geom)[0]:\n        shot_n = latslons_all.loc[num, 'Shot Number']\n        shot_list.append(shot_n)\n\n\nDF = latslons_all.where(latslons_all['Shot Number'].isin(shot_list))\nDF = DF.reset_index(drop=True).dropna()\n\n\nDF\n\n\n\n\n\n\n\n\nBeam\nShot Number\nLongitude\nLatitude\nTandem-X DEM\nCanopy Elevation (m)\nElevation (m)\nPlant Area Index\nCanopy Height/rh100 (cm)\nnon-vegetated from MODIS (percent)\ngeometry\n\n\n\n\n8944\nBEAM0000\n191450000200100445\n-111.263042\n40.692563\n2007.632080\n2015.287842\n2009.384521\n0.276865\n590\n20.0\nPOINT (-111.26304 40.69256)\n\n\n8945\nBEAM0000\n191450000200100446\n-111.262499\n40.692874\n2012.697021\n2019.093872\n2013.003784\n0.120738\n609\n20.0\nPOINT (-111.26250 40.69287)\n\n\n8946\nBEAM0000\n191450000200100447\n-111.261956\n40.693185\n2018.024170\n2022.015381\n2016.934082\n0.090340\n507\n19.0\nPOINT (-111.26196 40.69319)\n\n\n8947\nBEAM0000\n191450000200100449\n-111.260871\n40.693807\n2022.398193\n2029.190552\n2022.913696\n0.316465\n626\n19.0\nPOINT (-111.26087 40.69381)\n\n\n8948\nBEAM0000\n191450000200100450\n-111.260329\n40.694117\n2028.278809\n2031.693481\n2026.051758\n0.269103\n563\n19.0\nPOINT (-111.26033 40.69412)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n251324\nBEAM1011\n191451100200096272\n-110.799421\n40.906476\n2648.858887\n2661.588135\n2641.007568\n0.558181\n2057\n13.0\nPOINT (-110.79942 40.90648)\n\n\n251325\nBEAM1011\n191451100200096273\n-110.798881\n40.906781\n2648.858887\n2656.045166\n2635.277832\n0.276846\n2076\n12.0\nPOINT (-110.79888 40.90678)\n\n\n251326\nBEAM1011\n191451100200096274\n-110.798337\n40.907088\n2636.217285\n2651.862549\n2633.971191\n0.162534\n1788\n12.0\nPOINT (-110.79834 40.90709)\n\n\n251327\nBEAM1011\n191451100200096275\n-110.797793\n40.907395\n2636.313477\n2652.544678\n2633.196777\n0.630030\n1934\n12.0\nPOINT (-110.79779 40.90740)\n\n\n251328\nBEAM1011\n191451100200096276\n-110.797250\n40.907702\n2636.313477\n2643.638672\n2631.611572\n0.157591\n1202\n7.0\nPOINT (-110.79725 40.90770)\n\n\n\n\n5890 rows × 11 columns\n\n\n\n\nAll_DF = geopandas.GeoDataFrame(DF).drop(columns=['Longitude', 'Latitude'])\n\n\n\n\n6. Visualize a GeoDataFrame\n\nIn this section, the GeoDataFrame and the geoviews python package are used to spatially visualize the location of the GEDI shots on a basemap layer and import a GeoJSON file of the spatial region of interest for this use case example.\n\n\nDefining the vdims below will allow you to hover over specific shots and view information about them.\n\n# Create a list of geodataframe columns to be included as attributes in the output map\nvdims = []\nfor f in All_DF:\n    if f not in ['geometry']:\n        vdims.append(f)\n\n        vdims\n\n\n# Define a function for visualizing GEDI points\ndef pointVisual(features, vdims):\n    return (gvts.EsriImagery * geoviews.Points(features, vdims=vdims).options(tools=['hover'], height=500, width=900, size=4, \n                                                                        color='yellow', fontsize={'xticks': 10, 'yticks': 10, \n                                                                                                  'xlabel':16, 'ylabel': 16}))\n# Visualize GEDI data\ngeoviews.Polygons(ROI['geometry']).opts(line_color='red', color=None)* pointVisual(All_DF, vdims = vdims)\n\n\n\n\n\n  \n\n\n\n\n\n\nBelow, the shots are mapped to enable selection of datasets using dropdown menu to better visualize the spatial variations for each dataset.\n\nimport panel \npanel.extension()\n\nmask_name = panel.widgets.Select(name='Datasets',options=vdims, value='Elevation (m)', disabled_options=['Beam', 'Shot Number'])\n\n\n@panel.depends(mask_name)\ndef visual_map(mask_name):\n    map = (gvts.EsriImagery * geoviews.Points(All_DF,\n                                              vdims=vdims).options(color=mask_name,\n                                                                   cmap='gnuplot', size=4, tools=['hover'],\n                                                                   clim=(int(min(DF[mask_name])), \n                                                                         round(max(DF[mask_name]))),\n                                                                   colorbar=True, \n                                                                   title=f'{mask_name} (Wasatch-Cache National Forest): Apr 30, 2022',\n                                                                   fontsize={'xticks': 10, 'yticks': 10, 'xlabel':16, 'clabel':12,\n                                                                             'cticks':10,'title':10,\n                                                                             'ylabel':16})).options(height=500,width=700)\n    return map \n\npanel.Row(panel.WidgetBox(mask_name), visual_map)\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n7. Export Subsets as GeoJSON Files\n\nFinally, export the GeoDataFrame as a .geojson file that can be easily opened in your favorite remote sensing and/or GIS software and will include an attribute table with all of the shots/values for each of the SDS layers in the dataframe.\n\noutName = L2B.filename.replace('.h5', '.geojson')  # Create an output file name using the input file name\nprint(outName)\nAll_DF.to_file(outName, driver='GeoJSON')  # Export to GeoJSON\n\nGEDI02_B_2022120091720_O19145_02_T09106_02_003_01_V002.geojson"
  },
  {
    "objectID": "web_book/wicsis_2024.html",
    "href": "web_book/wicsis_2024.html",
    "title": "WICSIS 2024 EMIT Access Workshop",
    "section": "",
    "text": "Tuesday, 12/Nov/2024: 4:30pm - 6:00pm, (GMT +1)\nThe Earth Surface Mineral Dust Source Investigation (EMIT) instrument aboard the International Space Station (ISS) measures visible to short-wave infrared (VSWIR) wavelengths and can be used to map Earth’s surface in detail. Here we demonstrate how to access and visualize EMIT data through Earthdata search with Python. We also provide information for downloading EMIT data through various web interfaces. Participants will complete two Jupyter notebooks and be able to inspect EMIT spectra throughout the session.",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Overview"
    ]
  },
  {
    "objectID": "web_book/wicsis_2024.html#slides",
    "href": "web_book/wicsis_2024.html#slides",
    "title": "WICSIS 2024 EMIT Access Workshop",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Overview"
    ]
  },
  {
    "objectID": "web_book/wicsis_2024.html#prerequisites",
    "href": "web_book/wicsis_2024.html#prerequisites",
    "title": "WICSIS 2024 EMIT Access Workshop",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe prerequisites for this tutorial include: a basic familiarity with remote sensing and python, an Earthdata Login account, a GitHub account. All participants need to bring their laptop on the day of event.",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Overview"
    ]
  },
  {
    "objectID": "web_book/wicsis_2024.html#helpful-links",
    "href": "web_book/wicsis_2024.html#helpful-links",
    "title": "WICSIS 2024 EMIT Access Workshop",
    "section": "Helpful Links",
    "text": "Helpful Links\n\nEarthdata Search\nEMIT-Data-Resources\nVisions EMIT Data Portal\nAppEEARS\nEMIT Mission Website\nEMIT SDS Github",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Overview"
    ]
  },
  {
    "objectID": "web_book/wicsis_2024.html#contact-info",
    "href": "web_book/wicsis_2024.html#contact-info",
    "title": "WICSIS 2024 EMIT Access Workshop",
    "section": "Contact Info",
    "text": "Contact Info\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 11-07-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Overview"
    ]
  },
  {
    "objectID": "web_book/python_resources.html",
    "href": "web_book/python_resources.html",
    "title": "Python Resources",
    "section": "",
    "text": "The LPDAAC provides several Python scripts and Jupyter Notebooks for working with NASA Earthdata datasets available from the LP DAAC. This page serves as a table of contents organized by mission (i.e. EMIT, ECOSTRESS, GEDI) to direct users to resources within this book, as well as to other Repositories containing dataset specific workflows and use cases. Additionally, there is a separate web-book for the VSWIR Imaging and Thermal Applications, Learning, and Science (VITALS) Repository, which focuses on the compounded benefits of using VSWIR and TIR data together.",
    "crumbs": [
      "Python Resources",
      "Overview"
    ]
  },
  {
    "objectID": "web_book/python_resources.html#ecostress",
    "href": "web_book/python_resources.html#ecostress",
    "title": "Python Resources",
    "section": "ECOSTRESS",
    "text": "ECOSTRESS\n\n\n\nResource\nType\nSummary\n\n\n\n\nECOSTRESS_Tutorial.ipynb\nJupyter Notebook\nDemonstrates how to work with the ECOSTRESS Evapotranspiration PT-JPL Daily L3\n\n\nECOSTRESS_swath2grid.py\nCommand line executable\nDemonstrates how to converts ECOSTRESS swath data products into projected GeoTIFFs",
    "crumbs": [
      "Python Resources",
      "Overview"
    ]
  },
  {
    "objectID": "web_book/python_resources.html#emit",
    "href": "web_book/python_resources.html#emit",
    "title": "Python Resources",
    "section": "EMIT",
    "text": "EMIT\n\n\n\nResource\nType\nSummary\n\n\n\n\nGetting EMIT Data using EarthData Search\nMarkdown Guide\nA thorough walkthrough for using EarthData Search to find and download EMIT data\n\n\nExploring EMIT L2A Reflectance\nJupyter Notebook\nExplore EMIT L2A Reflectance data using interactive plots\n\n\nVisualizing Methane Plume Timeseries\nJupyter Notebook\nFind EMIT L2B CH4 Plume Data and build a timeseries of CH4 plume complexes\n\n\nGenerating_Methane_Spectral_Fingerprint\nJupyter Notebook\nExtract Radiance Spectra and build an in-plume/out-of-plume ratio to compare with CH4 absorption coefficient\n\n\nHow to find and access EMIT data\nJupyter Notebook\nUse the earthaccess Python library to find and download or stream EMIT data\n\n\nHow to Convert to ENVI Format\nJupyter Notebook\nConvert from downloaded netCDF4 (.nc) format to .envi format\n\n\nHow to Orthorectify\nJupyter Notebook\nUse the geometry lookup table (GLT) included with the EMIT netCDF4 file to project on a geospatial grid (EPSG:4326)\n\n\nHow to Extract Point Data\nJupyter Notebook\nExtract spectra using lat/lon coordinates from a .csv and build a dataframe/.csv output\n\n\nHow to Extract Area Data\nJupyter Notebook\nExtract an area defined by a .geojson or shapefile\n\n\nHow to use EMIT Quality Data\nJupyter Notebook\nBuild a mask using an EMIT L2A Mask file and apply it to an L2A Reflectance file\n\n\nHow to use Direct S3 Access with EMIT\nJupyter Notebook\nUse S3 from inside AWS us-west2 to access EMIT Data\n\n\nHow to find EMIT Data using NASA’s CMR API\nJupyter Notebook\nUse NASA’s CMR API to programmatically find EMIT Data",
    "crumbs": [
      "Python Resources",
      "Overview"
    ]
  },
  {
    "objectID": "web_book/python_resources.html#gedi",
    "href": "web_book/python_resources.html#gedi",
    "title": "Python Resources",
    "section": "GEDI",
    "text": "GEDI\n\n\n\nResource\nSummary\nPath\n\n\n\n\nGEDI_L1B_V2_Tutorial.ipynb\nJupyter Notebook tutorial demonstrating how to work with the Geolocated Waveform GEDI01_B.002 data product using Python\npython\\tutorials\n\n\nGEDI_L2A_V2_Tutorial.ipynb\nJupyter Notebook tutorial demonstrating how to work with the Geolocated Waveform GEDI02_A.002 data product using Python\npython\\tutorials\n\n\nGEDI_L2B_V2_Tutorial.ipynb\nJupyter Notebook tutorial demonstrating how to how to work with the Geolocated Waveform GEDI02_B.002 data product using Python\npython\\tutorials\n\n\nGEDI_Finder_Tutorial_Python.ipynb\nJupyter Notebook tutorial demonstrating how to perform spatial [bounding box] queries for GEDI V2 L1B, L2A, and L2B data using NASA’s CMR, and how to reformat the CMR response into a list of links that will allow users to download the intersecting GEDI V2 sub-orbit granules directly from the LP DAAC Data Pool using Python\npython\\tutorials\n\n\nGEDI_Finder_Tutorial_R.Rmd\nR Markdown tutorial demonstrating how to use R to perform spatial [bounding box] queries for GEDI V2 L1B, L2A, and L2B data using NASA’s CMR, and how to reformat the CMR response into a list of links that will allow users to download the intersecting GEDI V2 sub-orbit granules directly from the LP DAAC Data Pool\nR\n\n\nGEDI_Finder.py\nCommand line executable performing spatial [bounding box] and temporal queries for GEDI V2 L1B, L2A, and L2B data using NASA’s CMR and reformats the CMR response into a list of links that will allow users to download the intersecting GEDI V2 sub-orbit granules directly from the LP DAAC Data Pool.\npython/scripts/GEDI_Finder\n\n\nGEDI_Subsetter.py\nCommand line executable converting GEDI data products, stored in Hierarchical Data Format version 5 (HDF5, .h5) into GeoJSON files that can be loaded into GIS and Remote Sensing Software\npython/scripts/GEDI_Subsetter",
    "crumbs": [
      "Python Resources",
      "Overview"
    ]
  },
  {
    "objectID": "web_book/python_resources.html#hls",
    "href": "web_book/python_resources.html#hls",
    "title": "Python Resources",
    "section": "HLS",
    "text": "HLS\n\n\n\nResource\nType/Link\nSummary\nServices and Tools\n\n\n\n\nHLS Python Tutorial\nPython Notebook\nTutorial demonstrating how to search for, access, and process HLS data in Python\nearthaccess\n\n\nHLS SuPER Script\nPython Script\nFind, download, and subset HLS data from a command line executable\nCMR API\n\n\nHLS Bulk Download Bash Script\nBash Script\nFind and download\nCMR API\n\n\nHLS R Tutorial\nR Markdown\nTutorial demonstrating how to search for, access, and process HLS data in R\nCMR STAC API",
    "crumbs": [
      "Python Resources",
      "Overview"
    ]
  },
  {
    "objectID": "web_book/python_resources.html#other",
    "href": "web_book/python_resources.html#other",
    "title": "Python Resources",
    "section": "Other",
    "text": "Other\n\n\n\nResource\nType\nSummary\n\n\n\n\nData_Discovery_CMR_API_Request.ipynb\nJupyter Notebook\nDemonstrates how to search for Earthdata data collections and granules using CMR API and Request Python package\n\n\nData_Discovery_CMR_API_Bulk_Query.ipynb\nJupyter Notebook\nDemonstrates how to search and extract data URLs for an entire collection using Python’s asyncio package\n\n\nbulk_download_using_curl.md\nMarkdown\nDemonstrates how to bulk download LP DAAC data using Curl from command line\n\n\nbulk_download_using_wget.md\nMarkdown\nDemonstrates how to bulk download LP DAAC data using Wget from command line",
    "crumbs": [
      "Python Resources",
      "Overview"
    ]
  },
  {
    "objectID": "web_book/emit_tutorial_series.html",
    "href": "web_book/emit_tutorial_series.html",
    "title": "EMIT Tutorial Series",
    "section": "",
    "text": "These workshops are a series hosted by NASA Land Processes Distributed Active Archive Center (LP DAAC) and the Earth Mineral Dust Source Investigation (EMIT) Science Team from NASA Jet Propulsion Laboratory. They focus on introducing users to data available from the EMIT mission. Sessions include detailed explanation of the available products, underlying concepts, and example Python code in Jupyter notebooks that can be used to work with the data. All of the Jupyter Notebooks and modules mentioned have corresponding pages in the Python Resources Section, and can also be found in the EMIT-Data-Resources repository.",
    "crumbs": [
      "Past Workshops",
      "LP/JPL EMIT Tutorial Series"
    ]
  },
  {
    "objectID": "web_book/emit_tutorial_series.html#intro-to-emit-mission-overview-data-types-and-formats-how-to-access-and-download",
    "href": "web_book/emit_tutorial_series.html#intro-to-emit-mission-overview-data-types-and-formats-how-to-access-and-download",
    "title": "EMIT Tutorial Series",
    "section": "Intro to EMIT: Mission Overview, Data Types and Formats, How to Access and Download",
    "text": "Intro to EMIT: Mission Overview, Data Types and Formats, How to Access and Download\nFebruary 3, 2023",
    "crumbs": [
      "Past Workshops",
      "LP/JPL EMIT Tutorial Series"
    ]
  },
  {
    "objectID": "web_book/emit_tutorial_series.html#working-with-emit-data-basics",
    "href": "web_book/emit_tutorial_series.html#working-with-emit-data-basics",
    "title": "EMIT Tutorial Series",
    "section": "Working with EMIT Data: Basics",
    "text": "Working with EMIT Data: Basics\nFebruary 10, 2023",
    "crumbs": [
      "Past Workshops",
      "LP/JPL EMIT Tutorial Series"
    ]
  },
  {
    "objectID": "web_book/emit_tutorial_series.html#working-with-emit-data-advanced",
    "href": "web_book/emit_tutorial_series.html#working-with-emit-data-advanced",
    "title": "EMIT Tutorial Series",
    "section": "Working with EMIT Data: Advanced",
    "text": "Working with EMIT Data: Advanced\nFebruary 17, 2023",
    "crumbs": [
      "Past Workshops",
      "LP/JPL EMIT Tutorial Series"
    ]
  },
  {
    "objectID": "web_book/emit_tutorial_series.html#working-with-emit-data-mapping-methane",
    "href": "web_book/emit_tutorial_series.html#working-with-emit-data-mapping-methane",
    "title": "EMIT Tutorial Series",
    "section": "Working with EMIT Data: Mapping Methane",
    "text": "Working with EMIT Data: Mapping Methane\nMarch 14, 2024\n\n\n\nEMIT Mineralogy Webinar\nTo be Announced",
    "crumbs": [
      "Past Workshops",
      "LP/JPL EMIT Tutorial Series"
    ]
  },
  {
    "objectID": "setup/workshop_setup_python.html",
    "href": "setup/workshop_setup_python.html",
    "title": "Cloud Workspace Setup",
    "section": "",
    "text": "The Openscapes 2i2c JupyterHub Cloud Workspace is a cloud JupyterHub space provided by the LP DAAC for workshop participants. There are no additional setup requirements for the Python environment. All packages needed are included unless specified within a notebook, in which case a cell will be dedicated to installing the necessary Python libraries using the appropriate package manager."
  },
  {
    "objectID": "setup/workshop_setup_python.html#prerequisites",
    "href": "setup/workshop_setup_python.html#prerequisites",
    "title": "Cloud Workspace Setup",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow along during the workshop, or to run through the notebooks contained within the repository using the Openscapes 2i2c Cloud JupyterHub (cloud workspace), the following are required. All software or accounts are free.\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov/users/new\nRemember your username and password; you will need them to download or access data during the workshop and beyond.\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com/join. Follow optional advice on choosing your username\nYour GitHub username is used to enable you access to a cloud environment during the workshop. To gain access, please request access to the NASA Openscapes JupyterHub using this form. You will receive an email invitation to join the organization on GitHub. You must join to gain access to the workspace.\n\n\nNetrc file\n\nThis file is needed to access NASA Earthdata assets from a scripting environment like Python.\nThere are multiple methods to create a .netrc file. For this workshop, earthaccess package is used to automatically create a netrc file using your Earthdata login credentials if one does not exist. There are detailed instruction available for creating a .netrc file using other methods here.\n\nLaptop or tablet\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All workshop participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2."
  },
  {
    "objectID": "setup/workshop_setup_python.html#cloud-workspace-access",
    "href": "setup/workshop_setup_python.html#cloud-workspace-access",
    "title": "Cloud Workspace Setup",
    "section": "Cloud Workspace Access",
    "text": "Cloud Workspace Access\nVisit Openscapes 2i2c JupyterHub and log in using your GitHub account.\nBe sure to select the radio button for Python and a size of 14.8 GB RAM and up to 3.75 CPUs. Most of the notebooks will work with this size instance unless otherwise specified."
  },
  {
    "objectID": "setup/workshop_setup_python.html#troubleshooting",
    "href": "setup/workshop_setup_python.html#troubleshooting",
    "title": "Cloud Workspace Setup",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nWe recommend Shutting down all kernels after running each notebook. This will clear the memory used by the previous notebook, and is necessary to run some of the more memory intensive notebooks.\n\n\n\nNo single notebook exceeds roughly the limit using the provided data, but if you choose to use your own data in the notebook, or have 2 notebooks open and do not shut down the kernel, you may get an out of memory error.\nIf you elect to try this on your own data/ROI, you may need to select a larger server size. This will often happen if you are using the last EMIT scene from an orbit. In some cases those can be almost double the size of a normal scene. Please select the smallest possible."
  },
  {
    "objectID": "setup/workshop_setup_python.html#contact-info",
    "href": "setup/workshop_setup_python.html#contact-info",
    "title": "Cloud Workspace Setup",
    "section": "Contact Info",
    "text": "Contact Info\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 04-15-2024\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "setup/setup_instructions_python.html",
    "href": "setup/setup_instructions_python.html",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "The tutorials and how-tos in all repositories developed by LP DAAC team require a compatible Python Environment, an installation of Git. See details on prerequisites and Python environment Setup instructions below.\n\n\n\n\nTo access or download NASA Earth data, a .netrc file with your NASA Earthdata Login information is needed. You can create an account here if you do not have one. You can manually create a .netrc file but earthaccess.login(persist=True) function will prompt for your NASA Earthdata username and password to create one if one does not exist and then uses your account information for authentication purposes.\nInstall Environment Manager:\n\nIf you do not have an Environment Manager installed, we recommend mamba to manage Python packages.\n\nTo install mamba, download mambaforge for your operating system. If using Windows, be sure to check the box to “Add mamba to my PATH environment variable” to enable use of mamba directly from your command line interface.\n\nIf prefer conda Environment Manager, install Anaconda or miniconda. When installing, Anaconda or Miniconda be sure to check the box to “Add Anaconda to my PATH environment variable” to enable use of conda directly from your command line interface. Additional information on setting up and managing Conda environments.\n\nmamba typically offers higher speed and more reliable environment solutions. You still can utilize mamba with conda to manage packages. To install mamba, use your preferred command line interface (command prompt, terminal, cmder, etc.) and type the following: &gt; conda install mamba -n base -c conda-forge\n\nSee more details on installation of mamba here. Note that this may cause an issue if you have an existing mamba install through Anaconda.\n\nIf you do not have Git, you can download it here.\n\n\n\n\nThis Python environment will work for all tutorials developed by LP DAAC team existing within this repository in addition to Resource Repository directed to from this repository. All required packages are included in an .yml file stored in setup folder. Using your preferred command line interface (command prompt, terminal, cmder, etc.) follow the steps below to create a compatible Python environment.\nType the following in the command line and press enter to create a compatible environment with the most updated packages. &gt; mamba env create -f setup/lp_tutorials.yml\nIf you are using conda, replace the “mamba” with “conda” and be patient.\nTo reproducible the exact Python environment that all tutorials are tested with, use the .yml file with the versions included.\n\nWindows:\nmamba env create -f setup/lp_tutorials.yml\n\n\n\n\nIf you did the above and already have your environment activated, you can simply launch Jupyter Notebook by typing the following in command line:\n\njupyter notebook\n\nIf returning to an already created but inactive environment, using your preferred command line interface (command prompt, terminal, cmder, etc.) navigate to your local copy of the repository, then type the following to activate the Python Environment:\n\nmamba activate lpdaac_tutorials\n\nNow you can launch Jupyter Notebook to open the notebooks included.\n\njupyter notebook\n\nStill having trouble getting a compatible Python environment set up? Contact LP DAAC User Services.\n\n\n\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 11-09-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "setup/setup_instructions_python.html#prerequisites",
    "href": "setup/setup_instructions_python.html#prerequisites",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "To access or download NASA Earth data, a .netrc file with your NASA Earthdata Login information is needed. You can create an account here if you do not have one. You can manually create a .netrc file but earthaccess.login(persist=True) function will prompt for your NASA Earthdata username and password to create one if one does not exist and then uses your account information for authentication purposes.\nInstall Environment Manager:\n\nIf you do not have an Environment Manager installed, we recommend mamba to manage Python packages.\n\nTo install mamba, download mambaforge for your operating system. If using Windows, be sure to check the box to “Add mamba to my PATH environment variable” to enable use of mamba directly from your command line interface.\n\nIf prefer conda Environment Manager, install Anaconda or miniconda. When installing, Anaconda or Miniconda be sure to check the box to “Add Anaconda to my PATH environment variable” to enable use of conda directly from your command line interface. Additional information on setting up and managing Conda environments.\n\nmamba typically offers higher speed and more reliable environment solutions. You still can utilize mamba with conda to manage packages. To install mamba, use your preferred command line interface (command prompt, terminal, cmder, etc.) and type the following: &gt; conda install mamba -n base -c conda-forge\n\nSee more details on installation of mamba here. Note that this may cause an issue if you have an existing mamba install through Anaconda.\n\nIf you do not have Git, you can download it here."
  },
  {
    "objectID": "setup/setup_instructions_python.html#python-environment-setup",
    "href": "setup/setup_instructions_python.html#python-environment-setup",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "This Python environment will work for all tutorials developed by LP DAAC team existing within this repository in addition to Resource Repository directed to from this repository. All required packages are included in an .yml file stored in setup folder. Using your preferred command line interface (command prompt, terminal, cmder, etc.) follow the steps below to create a compatible Python environment.\nType the following in the command line and press enter to create a compatible environment with the most updated packages. &gt; mamba env create -f setup/lp_tutorials.yml\nIf you are using conda, replace the “mamba” with “conda” and be patient.\nTo reproducible the exact Python environment that all tutorials are tested with, use the .yml file with the versions included.\n\nWindows:\nmamba env create -f setup/lp_tutorials.yml"
  },
  {
    "objectID": "setup/setup_instructions_python.html#opening-the-notebooks",
    "href": "setup/setup_instructions_python.html#opening-the-notebooks",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "If you did the above and already have your environment activated, you can simply launch Jupyter Notebook by typing the following in command line:\n\njupyter notebook\n\nIf returning to an already created but inactive environment, using your preferred command line interface (command prompt, terminal, cmder, etc.) navigate to your local copy of the repository, then type the following to activate the Python Environment:\n\nmamba activate lpdaac_tutorials\n\nNow you can launch Jupyter Notebook to open the notebooks included.\n\njupyter notebook\n\nStill having trouble getting a compatible Python environment set up? Contact LP DAAC User Services."
  },
  {
    "objectID": "setup/setup_instructions_python.html#contact-info",
    "href": "setup/setup_instructions_python.html#contact-info",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "Email: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 11-09-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#summary",
    "href": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#summary",
    "title": "Using the CMR API and asyncio for fast CMR Queries",
    "section": "Summary",
    "text": "Summary\nThis tutorial demonstrates how to effectively perform queries and extract data download Uniform Resource Locators (URLs) for every Common Metadata Repository (CMR) metadata record within a NASA Earthdata collection. Two examples are shown. The first highlight making sequential requests for data URLs associated with specified collections. The second example demonstrates the how to leverages Python’s asyncio package to perform bulk parallel requests for the same information and highlights the increase in speed when doing so. The NASA Earthdata collections highlighted here are Harmonized Landsat Sentinel-2 Operational Land Imager Surface Refleactance and TOA Brightness Daily Global 30m (HLSL30.002) and Harmonized Landsat Sentinel-2 Multi-spectral Instrument Surface Reflactance Daily Global 30m (HLSS30.002).\n\nWhat is CMR?\nThe CMR is a metadata system that catalogs NASA’s Earth Observing System Data and Information System (EOSDIS) data and associated metadata. The CMR Application Programming Interface (API) provides programatic search capabilities through CMR’s vast metadata holdings using various parameters and keywords. When querying NASA’s CMR, there is a limit of 1 million granule matched with only 2000 granules returned per page. This guide shows how to search for CMR records using the CMR API and create a list of download URLs. This guide also shows how to leverage asynchronous, or parallel requests, to increase the speed of this process. The example below leverages the Harmonized Landsat Sentinel-2 collection archived by NASA’s LP DAAC to demonstrate how to use Python’s asyncio to perform large queries again NASA’s CMR."
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#objectives",
    "href": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#objectives",
    "title": "Using the CMR API and asyncio for fast CMR Queries",
    "section": "Objectives",
    "text": "Objectives\n\nUse the CMR API and Python to perform large queries (requests that return more than 2000 granules) against NASA’s CMR.\n\nPrepare a list of URLs to access or download assets associated with those granules.\n\nUtilize asynchronous/parallel requests to increase speed of query and list construction."
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#getting-started",
    "href": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#getting-started",
    "title": "Using the CMR API and asyncio for fast CMR Queries",
    "section": "Getting Started",
    "text": "Getting Started\nImport the required packages.\n\nimport requests\nimport math\nimport aiohttp\nimport asyncio\nimport time"
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#searching-the-cmr",
    "href": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#searching-the-cmr",
    "title": "Using the CMR API and asyncio for fast CMR Queries",
    "section": "Searching the CMR",
    "text": "Searching the CMR\nSet the CMR API Endpoint. This is the URL that we’ll use to search through the CMR.\n\nCMR_OPS = 'https://cmr.earthdata.nasa.gov/search' # CMR API Endpoint\nurl = f'{CMR_OPS}/{\"granules\"}'\n\nTo search the CMR we need to set our parameters. In this example we’ll narrow our search using Collection IDs, a range of dates and times, and the number of results we want to show per page. Spatial areas can also be used to narrow searches (example shown in HLS_Tutorial).\nHere, we are interested in both HLS Landsat-8 and Sentinel-2 collections collected from October 17-19, 2021. Specify the collections to search, set a datetime_range and set the quantity of results to return per page using the page_size parameter like below.\n\ncollections = ['C2021957657-LPCLOUD', 'C2021957295-LPCLOUD'] # Collection or concept_id specific to LPDAAC Products (HLS Landsat OLI and HLS Sentinel-2 respectively) \ndatetime_range = '2021-10-17T00:00:00Z,2021-10-19T23:59:59Z'\npage_size = 2000\n\nA CMR search can find up to 1 million items or granules, but the number returned per page is limited to 2000, meaning large searches may have several pages of results. By default, page_size is set to 10."
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#submitting-requests",
    "href": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#submitting-requests",
    "title": "Using the CMR API and asyncio for fast CMR Queries",
    "section": "Submitting Requests",
    "text": "Submitting Requests\nUsing the above search criteria we can make a request using the requests.get() function. Submit a request and print the response.status_code.\n\nresponse = requests.get(url, \n                        params={\n                            'concept_id': collections,\n                            'temporal': datetime_range,\n                            'page_size': page_size,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.status_code)\n\nA status code of 200 indicates the request has succeeded.\nTo see the number of results, print the CMR-Hits found in the returned header.\n\nprint(response.headers['CMR-Hits']) # Resulting quantity of granules/items."
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#building-a-list-of-file-urls",
    "href": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#building-a-list-of-file-urls",
    "title": "Using the CMR API and asyncio for fast CMR Queries",
    "section": "Building a List of File URLs",
    "text": "Building a List of File URLs\nWe can build a list of URLs to data assets using our search results. Notice this only uses the first page of results.\n\ngranules = response.json()['feed']['entry']\nlen(granules) # Resulting quantity of granules on page one.\n\n\nfile_list = []\nfor g in granules:\n    file_list.extend([x['href'] for x in g['links'] if 'https' in x['href'] and '.tif' in x['href']])\nlen(file_list) # Total number of assets from page one of granules.\n\nPrint part of the URLs list.\n\nfile_list[:25]\n\nThis process can be extended to all pages of search results to build a complete list of asset URLs."
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#creating-a-list-from-multiple-results-pages",
    "href": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#creating-a-list-from-multiple-results-pages",
    "title": "Using the CMR API and asyncio for fast CMR Queries",
    "section": "Creating a List from Multiple Results Pages",
    "text": "Creating a List from Multiple Results Pages\nTo create a list from multiple results pages, we first define a function to build a list of pages based upon the number of results.\n\ndef get_page_total(collections, datetime_range, page_size):\n    hits = requests.get(url, \n                        params={\n                            'concept_id': collections,\n                            'temporal': datetime_range,\n                            'page_size': page_size,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       ).headers['CMR-Hits']\n    return math.ceil(int(hits)/page_size)\n\nThen we build a list of pages called page_numbers.\n\npage_numbers = list(range(1, get_page_total(collections, datetime_range, page_size)+1))\npage_numbers\n\nAfter we have a list of pages we can iterate through page by page to make a complete list of assets matching our search.\n\ndata_urls = [] # empty list\nstart = time.time() # Begin timer\nfor n in page_numbers: # Iterate through requests page by page sequentially\n    print(f'Page: {n}') # Print Page Number\n    response = requests.get(url, # Same request function as used previously\n                            params={\n                                'concept_id': collections,\n                                'temporal': datetime_range,\n                                'page_size': page_size,\n                                'page_num': n\n                            },\n                            headers={\n                                'Accept': 'application/json'\n                            }\n                           )\n    print(f'Page {n} Resonse Code: {response.status_code}') # Show the response code for each page\n    \n    granules = response.json()['feed']['entry']\n    print(f'Number of Granules: {len(granules)}') # Show the number of granules on each page\n    \n    for g in granules:\n        data_urls.extend([x['href'] for x in g['links'] if 'https' in x['href'] and '.tif' in x['href']])\nend = time.time()\nprint(f'Total time: {end-start}') # Record the total time taken\n\nShow the total quantity of assets in our list matching search parameters.\n\nlen(data_urls)\n\nWe can also see that the first 25 assets match up from our first page only search results.\n\nfile_list[:25]==data_urls[:25]"
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#improve-speed-using-asynchronous-requests",
    "href": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#improve-speed-using-asynchronous-requests",
    "title": "Using the CMR API and asyncio for fast CMR Queries",
    "section": "Improve speed using Asynchronous Requests",
    "text": "Improve speed using Asynchronous Requests\nYou may have noticed the total time the function above took to run. For searches with a large quantity of results, we can query and build a list of asset URLs more quickly by utilizing asynchronous requests. Asynchronous requests can be run concurrently or in parallel, which typically decreases the total time of operations because a response is not needed for the prior request before a subsequent request is made. This time we’ll use a similar approach as before, except we will build a list of page URLs that can be used in asynchronous requests to populate our list of asset URLs more quickly.\nFirst we define a new function get_cmr_pages_urls() to create a list of results pages URLs, not just the page numbers like we did before, then build that list.\n\ndef get_cmr_pages_urls(collections, datetime_range, page_size): \n    response = requests.get(url,\n                       params={\n                           'concept_id': collections,\n                           'temporal': datetime_range,\n                           'page_size': page_size,\n                       },\n                       headers={\n                           'Accept': 'application/json'\n                       }\n                      )\n    hits = int(response.headers['CMR-Hits'])\n    n_pages = math.ceil(hits/page_size)\n    cmr_pages_urls = [f'{response.url}&page_num={x}'.replace('granules?', 'granules.json?') for x in list(range(1,n_pages+1))]\n    return cmr_pages_urls\n\n\nurls = get_cmr_pages_urls(collections, datetime_range, page_size)\nurls\n\nNext, we create an empty list to populate with our asset URLs.\n\nresults = []\n\nThen we define a function get_tasks() to build a list of tasks for each page number URL and a function get_url() to make the requests for each page in parallel with one another.\n\ndef get_tasks(session):\n    tasks = []\n    for l in urls:\n        tasks.append(session.get(l))\n    return tasks\n\n\nasync def get_url():\n    async with aiohttp.ClientSession() as session:\n        tasks = get_tasks(session)\n        responses = await asyncio.gather(*tasks)\n        for response in responses:\n            res = await response.json()\n            #print(res)\n            results.extend([l['href'] for g in res['feed']['entry'] for l in g['links'] if 'https' in l['href'] and '.tif' in l['href']])\n\nRun the functions to submit asynchronous/parallel requests for each page of results.\n\nstart = time.time() \n\nawait get_url()\n\nend = time.time()\n\ntotal_time = end - start\ntotal_time\n\nMuch faster than before! We can see the same quantity of results and that a subsample of the resulting asset URLs matches what we retrieved before.\n\nlen(results)\n\n\ndata_urls[2025:2125] == results[2025:2125]"
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#additional-resources",
    "href": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#additional-resources",
    "title": "Using the CMR API and asyncio for fast CMR Queries",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nNASA Earthdata CMR Search API Documentation"
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#contact-information",
    "href": "python/tutorials/Data_Discovery_CMR_API_Bulk_Query.html#contact-information",
    "title": "Using the CMR API and asyncio for fast CMR Queries",
    "section": "Contact Information",
    "text": "Contact Information\nAuthors: LP DAAC¹\nContact: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 01-25-2024\n¹Work performed under USGS contract G15PD00467 for LP DAAC under NASA contract NNG14HH33I."
  },
  {
    "objectID": "python/how-tos/Earthdata_Cloud__Download_file_from_S3.html",
    "href": "python/how-tos/Earthdata_Cloud__Download_file_from_S3.html",
    "title": "Download Files from S3 Using boto3",
    "section": "",
    "text": "import boto3\nimport requests\nfrom getpass import getpass"
  },
  {
    "objectID": "python/how-tos/Earthdata_Cloud__Download_file_from_S3.html#enter-earthdata-login-credentials",
    "href": "python/how-tos/Earthdata_Cloud__Download_file_from_S3.html#enter-earthdata-login-credentials",
    "title": "Download Files from S3 Using boto3",
    "section": "Enter Earthdata Login Credentials",
    "text": "Enter Earthdata Login Credentials\n\nuser = getpass(prompt='Enter your NASA Earthdata Login Username')\npassword = getpass(prompt='Enter your NASA Earthdata Login Password')"
  },
  {
    "objectID": "python/how-tos/Earthdata_Cloud__Download_file_from_S3.html#get-earthdata-cloud-temporary-credentials",
    "href": "python/how-tos/Earthdata_Cloud__Download_file_from_S3.html#get-earthdata-cloud-temporary-credentials",
    "title": "Download Files from S3 Using boto3",
    "section": "Get Earthdata Cloud Temporary Credentials",
    "text": "Get Earthdata Cloud Temporary Credentials\n\nurl = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\nurl = requests.get(url, allow_redirects=False).headers['Location']\ncreds = requests.get(url, auth=(user, password)).json()"
  },
  {
    "objectID": "python/how-tos/Earthdata_Cloud__Download_file_from_S3.html#create-a-boto3-session",
    "href": "python/how-tos/Earthdata_Cloud__Download_file_from_S3.html#create-a-boto3-session",
    "title": "Download Files from S3 Using boto3",
    "section": "Create a boto3 Session",
    "text": "Create a boto3 Session\nWe will use a session to store our S3 credentials and other configurations options. Our session will be used to create a boto3 client which act as our interface to AWS services used to, for example, download files or list objects in S3 specified S3 buckets.\nNOTE, it is important to specify the prefix and delimiter parameter options. The download_file method works without the options set, but other methods will fail without those options being specified.\n\nsession = boto3.Session(aws_access_key_id=creds['accessKeyId'], \n                        aws_secret_access_key=creds['secretAccessKey'], \n                        aws_session_token=creds['sessionToken'], \n                        region_name='us-west-2')\nclient = session.client('s3')\nbucket = 'lp-prod-protected'\nprefix = ''\ndelimiter = '/'"
  },
  {
    "objectID": "python/how-tos/Earthdata_Cloud__Download_file_from_S3.html#download-file-from-s3",
    "href": "python/how-tos/Earthdata_Cloud__Download_file_from_S3.html#download-file-from-s3",
    "title": "Download Files from S3 Using boto3",
    "section": "Download File from S3",
    "text": "Download File from S3\nSpecify the path to the object we want to download.\n\nkey = \"HLSS30.020/HLS.S30.T56QPM.2023001T002959.v2.0/HLS.S30.T56QPM.2023001T002959.v2.0.B03.tif\"\n\nSpecify the name of the output file.\n\nfilename = 'temp_download_example.tif'\n\nDownload our file to the current working directory.\n\nclient.download_file(Bucket=bucket, Key=key, Filename=filename)"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "LP DAAC Data Resources",
    "section": "",
    "text": "Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\nDefinitions.\n“License” shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.\n“Licensor” shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.\n“Legal Entity” shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, “control” means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.\n“You” (or “Your”) shall mean an individual or Legal Entity exercising permissions granted by this License.\n“Source” form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.\n“Object” form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.\n“Work” shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).\n“Derivative Works” shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.\n“Contribution” shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, “submitted” means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as “Not a Contribution.”\n“Contributor” shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.\nGrant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.\nGrant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.\nRedistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n\nYou must give any other recipients of the Work or Derivative Works a copy of this License; and\nYou must cause any modified files to carry prominent notices stating that You changed the files; and\nYou must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and\nIf the Work includes a “NOTICE” text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.\n\nYou may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.\nSubmission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.\nTrademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.\nDisclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.\nLimitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.\nAccepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\nAPPENDIX: How to apply the Apache License to your work.\n  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\nCopyright [yyyy] [name of copyright owner]\nLicensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n   http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
  },
  {
    "objectID": "guides/Streaming_cloud_optimized_geotiffs_using_QGIS.html",
    "href": "guides/Streaming_cloud_optimized_geotiffs_using_QGIS.html",
    "title": "Streaming NASA Earthdata Cloud-Optimized Geotiffs using QGIS",
    "section": "",
    "text": "Note: Users must have NASA Earthdata Login to stream data.\nQGIS can be used to stream NASA Earthdata cloud-optimized geotiff files. To do this QGIS uses https and the vsicurl virtual file system handler. This guide will show you how to configure QGIS to do this. There are 4 steps needed.\n\nCreate a .netrc file\nAdd Custom Environment Variables\nRestart QGIS\nAdd Data\n\n\n\nUnder the hood QGIS uses gdal and vsicurl to stream data via https. For NASA Earthdata, this requires a .netrc file to store your Earthdata login credentials. If you already have one, you can skip this step. This file should be located in your home directory. Choose one of the methods below to create the file and enter your credentials.\n\n\n\nDownload the .netrc template file and save it in your home/user/ directory, where user is your personal user directory. For example: C:\\Users\\user\\.netrc or home/user/.netrc.\n\nOpen the .netrc file in a text editor and replace  with your NASA Earthdata Login username and  with your NASA Earthdata Login password.\n\nAfter editing, the file should look something like this:\n\n\n\nExample .netrc 1\n\n\nor you can also have everything on a single line separated by spaces, like:\n\n\n\nexample .netrc 2\n\n\n\n\n\nFor Linux/MacOS:\nTo Create a .netrc file, enter the following in the command line, replacing  and  with your NASA Earthdata username and password. This will create a file in your home directory or append your NASA credentials to an existing file.\necho \"machine urs.earthdata.nasa.gov login &lt;USERNAME&gt; password &lt;PASSWORD&gt;\" &gt;&gt;~/.netrc\nFor Windows:\nTo Create a .netrc file, enter the following in the command line, replacing  and  with your NASA Earthdata username and password. This will create a file in your home directory or append your NASA credentials to an existing file.\necho machine urs.earthdata.nasa.gov login &lt;USERNAME&gt; password &lt;PASSWORD&gt; &gt;&gt; %userprofile%\\.netrc\nYou can verify that the file is correct by opening with a text editor. It should look like an example in one of the figures above.\n\n\n\n\nNext, while still in the Settings menu, select System on the left-hand side, then scroll down to Environment. Select the check box and enter the Variables and Values in the table below by clicking the plus sign to add a new variable. These variables, set up a place for cookies to be stored and read from, prevent GDAL from reading all files in the directory, specify the extensions GDAL is allowed to access over HTTPS using the vsicurl virtual file system handler, and allow use of unsafe SSL connections (use with caution).\n\n\n\nVariable\nValue\n\n\n\n\nGDAL_HTTP_COOKIEFILE\n~/cookies.txt\n\n\nGDAL_HTTP_COOKIEJAR\n~/cookies.txt\n\n\nGDAL_DISABLE_READDIR_ON_OPEN\nEMPTY_DIR\n\n\nCPL_VSIL_CURL_ALLOWED_EXTENSIONS\nTIF\n\n\nGDAL_HTTP_UNSAFESSL\nYES\n\n\n\n\nNote: These settings may affect streaming from other data sources.\n\n\n\n\nEnvironment Settings\n\n\n\n\n\nRestart QGIS to load the new environment settings.\n\n\n\nTo add some raster data, select the add raster data button from the toolbar, select ‘Protocol: HTTP(S), cloud, etc’ as the source type, then enter a URI for a cloud-optimized geotiff file and press the add button. For example:\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BCH4ENH.001/EMIT_L2B_CH4ENH_001_20240423T074559_2411405_018/EMIT_L2B_CH4ENH_001_20240423T074559_2411405_018.tif\n\n\n\nAdd raster data\n\n\nThis should add the data to your map, and will work with any NASA Earthdata hosted cloud-optimized geotiff file.\n\n\n\nAdded Scene\n\n\n\nIf these instructions do not work, please verify that your .netrc file has the correct username and password, and is formatted as shown in Section 1.\n\n\n\n\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 08-08-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "guides/Streaming_cloud_optimized_geotiffs_using_QGIS.html#create-a-.netrc-file",
    "href": "guides/Streaming_cloud_optimized_geotiffs_using_QGIS.html#create-a-.netrc-file",
    "title": "Streaming NASA Earthdata Cloud-Optimized Geotiffs using QGIS",
    "section": "",
    "text": "Under the hood QGIS uses gdal and vsicurl to stream data via https. For NASA Earthdata, this requires a .netrc file to store your Earthdata login credentials. If you already have one, you can skip this step. This file should be located in your home directory. Choose one of the methods below to create the file and enter your credentials.\n\n\n\nDownload the .netrc template file and save it in your home/user/ directory, where user is your personal user directory. For example: C:\\Users\\user\\.netrc or home/user/.netrc.\n\nOpen the .netrc file in a text editor and replace  with your NASA Earthdata Login username and  with your NASA Earthdata Login password.\n\nAfter editing, the file should look something like this:\n\n\n\nExample .netrc 1\n\n\nor you can also have everything on a single line separated by spaces, like:\n\n\n\nexample .netrc 2\n\n\n\n\n\nFor Linux/MacOS:\nTo Create a .netrc file, enter the following in the command line, replacing  and  with your NASA Earthdata username and password. This will create a file in your home directory or append your NASA credentials to an existing file.\necho \"machine urs.earthdata.nasa.gov login &lt;USERNAME&gt; password &lt;PASSWORD&gt;\" &gt;&gt;~/.netrc\nFor Windows:\nTo Create a .netrc file, enter the following in the command line, replacing  and  with your NASA Earthdata username and password. This will create a file in your home directory or append your NASA credentials to an existing file.\necho machine urs.earthdata.nasa.gov login &lt;USERNAME&gt; password &lt;PASSWORD&gt; &gt;&gt; %userprofile%\\.netrc\nYou can verify that the file is correct by opening with a text editor. It should look like an example in one of the figures above."
  },
  {
    "objectID": "guides/Streaming_cloud_optimized_geotiffs_using_QGIS.html#environment-settings",
    "href": "guides/Streaming_cloud_optimized_geotiffs_using_QGIS.html#environment-settings",
    "title": "Streaming NASA Earthdata Cloud-Optimized Geotiffs using QGIS",
    "section": "",
    "text": "Next, while still in the Settings menu, select System on the left-hand side, then scroll down to Environment. Select the check box and enter the Variables and Values in the table below by clicking the plus sign to add a new variable. These variables, set up a place for cookies to be stored and read from, prevent GDAL from reading all files in the directory, specify the extensions GDAL is allowed to access over HTTPS using the vsicurl virtual file system handler, and allow use of unsafe SSL connections (use with caution).\n\n\n\nVariable\nValue\n\n\n\n\nGDAL_HTTP_COOKIEFILE\n~/cookies.txt\n\n\nGDAL_HTTP_COOKIEJAR\n~/cookies.txt\n\n\nGDAL_DISABLE_READDIR_ON_OPEN\nEMPTY_DIR\n\n\nCPL_VSIL_CURL_ALLOWED_EXTENSIONS\nTIF\n\n\nGDAL_HTTP_UNSAFESSL\nYES\n\n\n\n\nNote: These settings may affect streaming from other data sources.\n\n\n\n\nEnvironment Settings"
  },
  {
    "objectID": "guides/Streaming_cloud_optimized_geotiffs_using_QGIS.html#restart-qgis",
    "href": "guides/Streaming_cloud_optimized_geotiffs_using_QGIS.html#restart-qgis",
    "title": "Streaming NASA Earthdata Cloud-Optimized Geotiffs using QGIS",
    "section": "",
    "text": "Restart QGIS to load the new environment settings."
  },
  {
    "objectID": "guides/Streaming_cloud_optimized_geotiffs_using_QGIS.html#add-some-data",
    "href": "guides/Streaming_cloud_optimized_geotiffs_using_QGIS.html#add-some-data",
    "title": "Streaming NASA Earthdata Cloud-Optimized Geotiffs using QGIS",
    "section": "",
    "text": "To add some raster data, select the add raster data button from the toolbar, select ‘Protocol: HTTP(S), cloud, etc’ as the source type, then enter a URI for a cloud-optimized geotiff file and press the add button. For example:\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BCH4ENH.001/EMIT_L2B_CH4ENH_001_20240423T074559_2411405_018/EMIT_L2B_CH4ENH_001_20240423T074559_2411405_018.tif\n\n\n\nAdd raster data\n\n\nThis should add the data to your map, and will work with any NASA Earthdata hosted cloud-optimized geotiff file.\n\n\n\nAdded Scene\n\n\n\nIf these instructions do not work, please verify that your .netrc file has the correct username and password, and is formatted as shown in Section 1."
  },
  {
    "objectID": "guides/Streaming_cloud_optimized_geotiffs_using_QGIS.html#contact-info",
    "href": "guides/Streaming_cloud_optimized_geotiffs_using_QGIS.html#contact-info",
    "title": "Streaming NASA Earthdata Cloud-Optimized Geotiffs using QGIS",
    "section": "",
    "text": "Email: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 08-08-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "guides/bulk_download_using_curl.html",
    "href": "guides/bulk_download_using_curl.html",
    "title": "How to bulk download LP DAAC data using Curl",
    "section": "",
    "text": "This guide shows how to bulk download LP DAAC data using cURL from the command line. cURL is a free and open source software developed under Client for URLs (cURL) project as a command line tool for transfering data using URLs.\n\n\n\nInstall cURL. View installing curl instructions for more details.\nNASA Earthdata login credentials are required to access data from all NASA DAACs. You can create an account here.\n\n\n\n\nSave download links for your data as a text file using Nasa Earthdata Search or Common Metadata Repository (CMR) API. Follow the steps in the Earthdata Search guide to find your data and save the download links. If you prefer to use an API to find your data and save the download links, a tutorial on how to use the CMR API can be found here.\n\n\n\nSet up a .netrc file in your home directory.\n\n\n\nDownload the .netrc template file and save it in your home directory.\n\nOpen the .netrc file in a text editor and replace &lt;USERNAME&gt; with your NASA Earthdata Login username and &lt;PASSWORD&gt; with your NASA Earthdata Login password.\n\n\n\n\nEnter the following in Terminal:\n\n\nTo Create a .netrc file, enter the following in the command line, replacing  and  with your NASA Earthdata username and password. This will create a file in your home directory or append your NASA credentials to an existing file.\necho machine urs.earthdata.nasa.gov login &lt;USERNAME&gt; password &lt;PASSWORD&gt; &gt;&gt; %userprofile%\\.netrc\n\nTo Create a .netrc file, enter the following in the command line, replacing  and  with your NASA Earthdata username and password. This will create a file in your home directory or append your NASA credentials to an existing file.\necho \"machine urs.earthdata.nasa.gov login &lt;USERNAME&gt; password &lt;PASSWORD&gt;\" &gt;&gt;~/.netrc\n\n\n\n\nRun Authentication for NASA Earthdata notebook to create .netrc file.\nAlternatively, you can run the EarthdataLoginSetup script in a Python interpreter or from the command line.\n\n\n\n\n\nYou should now be able to run the command to download data directly from the LP DAAC. - Navigate to the directory you want to save the data using cd Insert_Your_Directory. - To download a single file, replace the Insert_the_Download_Link in the command below with the URL to the data file you wish to download. curl -O -b ~/.urs_cookies -c ~/.urs_cookies -L -n Insert_the_Download_Link Example: tet   curl -O -b ~/.urs_cookies -c ~/.urs_cookies -L -n https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T12SWF.2023189T175919.v2.0/HLS.S30.T12SWF.2023189T175919.v2.0.B08.tif - To download multiple files, replace Insert_Text_File in the command below with the full path to the text file saved previously in Step 1. xargs -n 1 curl -O -b ~/.urs_cookies -c ~/.urs_cookies -L -n &lt; Insert_Text_File\nExample: xargs -n 1 curl -O -b ~/.urs_cookies -c ~/.urs_cookies -L -n &lt; data/Granule-DownloadLinks.txt"
  },
  {
    "objectID": "guides/bulk_download_using_curl.html#requirements",
    "href": "guides/bulk_download_using_curl.html#requirements",
    "title": "How to bulk download LP DAAC data using Curl",
    "section": "",
    "text": "Install cURL. View installing curl instructions for more details.\nNASA Earthdata login credentials are required to access data from all NASA DAACs. You can create an account here."
  },
  {
    "objectID": "guides/bulk_download_using_curl.html#step-1-save-the-download-links",
    "href": "guides/bulk_download_using_curl.html#step-1-save-the-download-links",
    "title": "How to bulk download LP DAAC data using Curl",
    "section": "",
    "text": "Save download links for your data as a text file using Nasa Earthdata Search or Common Metadata Repository (CMR) API. Follow the steps in the Earthdata Search guide to find your data and save the download links. If you prefer to use an API to find your data and save the download links, a tutorial on how to use the CMR API can be found here."
  },
  {
    "objectID": "guides/bulk_download_using_curl.html#step-2-set-up-a-.netrc-file-for-authentication",
    "href": "guides/bulk_download_using_curl.html#step-2-set-up-a-.netrc-file-for-authentication",
    "title": "How to bulk download LP DAAC data using Curl",
    "section": "",
    "text": "Set up a .netrc file in your home directory.\n\n\n\nDownload the .netrc template file and save it in your home directory.\n\nOpen the .netrc file in a text editor and replace &lt;USERNAME&gt; with your NASA Earthdata Login username and &lt;PASSWORD&gt; with your NASA Earthdata Login password.\n\n\n\n\nEnter the following in Terminal:\n\n\nTo Create a .netrc file, enter the following in the command line, replacing  and  with your NASA Earthdata username and password. This will create a file in your home directory or append your NASA credentials to an existing file.\necho machine urs.earthdata.nasa.gov login &lt;USERNAME&gt; password &lt;PASSWORD&gt; &gt;&gt; %userprofile%\\.netrc\n\nTo Create a .netrc file, enter the following in the command line, replacing  and  with your NASA Earthdata username and password. This will create a file in your home directory or append your NASA credentials to an existing file.\necho \"machine urs.earthdata.nasa.gov login &lt;USERNAME&gt; password &lt;PASSWORD&gt;\" &gt;&gt;~/.netrc\n\n\n\n\nRun Authentication for NASA Earthdata notebook to create .netrc file.\nAlternatively, you can run the EarthdataLoginSetup script in a Python interpreter or from the command line."
  },
  {
    "objectID": "guides/bulk_download_using_curl.html#step-3-download-lp-daac-data",
    "href": "guides/bulk_download_using_curl.html#step-3-download-lp-daac-data",
    "title": "How to bulk download LP DAAC data using Curl",
    "section": "",
    "text": "You should now be able to run the command to download data directly from the LP DAAC. - Navigate to the directory you want to save the data using cd Insert_Your_Directory. - To download a single file, replace the Insert_the_Download_Link in the command below with the URL to the data file you wish to download. curl -O -b ~/.urs_cookies -c ~/.urs_cookies -L -n Insert_the_Download_Link Example: tet   curl -O -b ~/.urs_cookies -c ~/.urs_cookies -L -n https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T12SWF.2023189T175919.v2.0/HLS.S30.T12SWF.2023189T175919.v2.0.B08.tif - To download multiple files, replace Insert_Text_File in the command below with the full path to the text file saved previously in Step 1. xargs -n 1 curl -O -b ~/.urs_cookies -c ~/.urs_cookies -L -n &lt; Insert_Text_File\nExample: xargs -n 1 curl -O -b ~/.urs_cookies -c ~/.urs_cookies -L -n &lt; data/Granule-DownloadLinks.txt"
  },
  {
    "objectID": "guides/bulk_download_using_curl.html#contact-info",
    "href": "guides/bulk_download_using_curl.html#contact-info",
    "title": "How to bulk download LP DAAC data using Curl",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 07-11-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "external/Working_with_EMIT_L2B_Mineralogy.html",
    "href": "external/Working_with_EMIT_L2B_Mineralogy.html",
    "title": "Working with EMIT L2B Mineralogy Data",
    "section": "",
    "text": "This notebook is from EMIT-Data-Resources\n\n\nSource: Working with EMIT L2B Mineralogy Data\n\nImported on: 2024-11-07",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "10. Working with EMIT L2B Mineralogy"
    ]
  },
  {
    "objectID": "external/Working_with_EMIT_L2B_Mineralogy.html#setup",
    "href": "external/Working_with_EMIT_L2B_Mineralogy.html#setup",
    "title": "Working with EMIT L2B Mineralogy Data",
    "section": "1. Setup",
    "text": "1. Setup\nImport the required Python libraries.\n\nimport earthaccess\nimport geopandas as gp\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nimport hvplot.xarray\nimport holoviews as hv\nimport panel as pn\nsys.path.append('../modules/')\nimport emit_tools as et\n\nLogin to your NASA Earthdata account and create a .netrc file using the login function from the earthaccess library. If you do not have an Earthdata Account, you can create one here.\n\nearthaccess.login(persist=True)\n\nFor this notebook we will download the files necessary using earthaccess. You can also access the data in place or stream it, but this can slow due to the file sizes. Provide a URL for an EMIT L2B Mineral Identification and Band Depth granule.\n\n# List the browse images from the text file output of the previous notebook.\nmin_list = '../../data/results_urls.txt'\nwith open(min_list) as f:\n    min_urls = [line.rstrip('\\n') for line in f]\nmin_urls\n\n\n# List the browse images from the text file output of the previous notebook.\nunc_list = '../../data/min_uncert_urls.txt'\nwith open(unc_list) as f:\n    unc_urls = [line.rstrip('\\n') for line in f]\nunc_urls\n\nGet an HTTPS Session using your earthdata login, set a local path to save the file, and download the granule asset, in this case we are selecting the second scene (index 1) from the list of granules for both the mineral and uncertainty files.\n\nfs = earthaccess.get_fsspec_https_session()\nfp = fs.open(min_urls[0])\nfp_un = fs.open(unc_urls[0])",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "10. Working with EMIT L2B Mineralogy"
    ]
  },
  {
    "objectID": "external/Working_with_EMIT_L2B_Mineralogy.html#downloaded-data",
    "href": "external/Working_with_EMIT_L2B_Mineralogy.html#downloaded-data",
    "title": "Working with EMIT L2B Mineralogy Data",
    "section": "1.2 Downloaded Data",
    "text": "1.2 Downloaded Data\nIf you’ve already downloaded the data using the workflow shown in Section 6 of the Finding EMIT L2B Mineralogy Data , you can just set filepaths using the cell below.\n\n#fp = '../../data/EMIT_L2B_MIN_001_20230427T173309_2311711_010.nc' # Mineral\n#fp_rgb = '../../data/EMIT_L2A_RFL_001_20230427T173309_2311711_010.png' # RGB\n#fp_un = '../../data/EMIT_L2B_MINUNCERT_001_20230427T173309_2311711_010.nc' # Mineral Uncertainty",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "10. Working with EMIT L2B Mineralogy"
    ]
  },
  {
    "objectID": "external/Working_with_EMIT_L2B_Mineralogy.html#working-with-the-l2b-mineral-identification-and-band-depth",
    "href": "external/Working_with_EMIT_L2B_Mineralogy.html#working-with-the-l2b-mineral-identification-and-band-depth",
    "title": "Working with EMIT L2B Mineralogy Data",
    "section": "2. Working with the L2B Mineral Identification and Band Depth",
    "text": "2. Working with the L2B Mineral Identification and Band Depth\nEMITL2BMIN data are distributed in a non-orthocorrected spatially raw NetCDF4 (.nc) format consisting of the data and its associated metadata. Inside the .nc file there are 3 groups. Groups can be thought of as containers to organize the data.\n\nThe root group that can be considered the main dataset contains 4 data variables data described by the downtrack, and crosstrack dimensions. These variables are group_1_mineral_id, group_1_band_depth, group_2_mineral_id, and group_2_band_depth. These contain the ID and a band depth for each mineral group. These groups do not correspond to the .netcdf file groups, but rather the spectral library groups used to identify the minerals based on which region of the spectra the mineral features correspond to.\nThe mineral_metadata group containing the spectral library entry name, index, record, group, and url for each entry.\nThe location group contains latitude and longitude values at the center of each pixel described by the crosstrack and downtrack dimensions, as well as a geometry lookup table (GLT) described by the ortho_x and ortho_y dimensions. The GLT is an orthorectified image (EPSG:4326) consisting of 2 layers containing downtrack and crosstrack indices. These index positions allow us to quickly project the raw data onto this geographic grid.\n\nTo access the .nc file, you can use the netCDF4, xarray libraries, or fuctions from the emit_tools.py library. Here we will use the emit_xarray function from this library, which will open and organize the data into an easy to work with xarray.Dataset object. We can also pass the ortho=True argument to orthorectify the data at this stage, but we will start just examining the data to get a better understanding.\n\nds_min = et.emit_xarray(fp)\nds_min\n\nIf we look at the mineral index by printing the first 5 values, we can see that values start with 1. If we look at the minimum values of the mineral IDs we can see these have 0 as a possible value.\n\nprint(ds_min.index.data[:5])\n\n\nprint(f'Group_1_minimum:{ds_min.group_1_mineral_id.data.min()} Group_2_minimum: {ds_min.group_2_mineral_id.data.min()}')\n\nThe 0 here represents no match. For convenience, let’s make a DataFrame that holds the mineral data, and add that ‘No match’ reference to it:\n\nmin_df = pd.DataFrame({x: ds_min[x].values for x in [var for var in ds_min.coords if 'mineral_name' in ds_min[var].dims]})\nmin_df.loc[-1] = {'index': 0, 'mineral_name': 'No_Match', 'record': -1.0, 'url': 'NA', 'group': 1.0, 'library': 'NA', 'spatial_ref': 0}\nmin_df = min_df.sort_index().reset_index(drop=True)\nmin_df\n\n\n2.1 Orthorectification\nThe orthorectifation process has already been done for EMIT data. Here we are just using the crosstrack and downtrack indices contained in the GLT to place our spatially raw mineralogy data a into geographic grid with the ortho_x and ortho_y dimensions.\n\nds_min = et.ortho_xr(ds_min)\nds_min\n\nWe can see from these outputs that the dimensions are now latitude and longitude.\nIn this example, we’ll just work with the group_1 mineral data. We can find the minerals present in the scene by finding unique values in the group_1_mineral_id, but first we will replace fill-values introduced during orthorectification with np.nan, to omit them from our analysis and improve visualizations.\n\n# Assign fill to np.nan\nfor var in ds_min.data_vars:\n    ds_min[var].data[ds_min[var].data == -9999] = np.nan",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "10. Working with EMIT L2B Mineralogy"
    ]
  },
  {
    "objectID": "external/Working_with_EMIT_L2B_Mineralogy.html#visualize-group-1-minerals",
    "href": "external/Working_with_EMIT_L2B_Mineralogy.html#visualize-group-1-minerals",
    "title": "Working with EMIT L2B Mineralogy Data",
    "section": "2.2 Visualize Group 1 Minerals",
    "text": "2.2 Visualize Group 1 Minerals\nTo visualize minerals present, plot Group 1 Minerals using a categorical color set. You can hover over a colored region to see the zero-based mineral id from the spectral library. Note that these values correspond to the 1-based index value.\n\nds_min['group_1_mineral_id'].hvplot.image(cmap='glasbey', geo=True, tiles='ESRI', alpha=0.8,frame_width=750).opts(title='Group 1 Mineral ID')\n\nThis figure shows the minerals present in the scene, but doesn’t really quantify how well they matched with the spectral library. For that we can look at the band depth for each mineral. We can build an interactive tool to do this using the panel and hvplot libraries. This will take a bit of time to load for each selection.\nBecause many minerals are scarce, we’ll start by updating the names to include relative fractions\n\ng1_min_percent = [np.round(np.sum(ds_min.group_1_mineral_id.data.flatten() == g1min) / np.sum(ds_min.group_1_mineral_id.data.flatten() &gt; 0),2) * 100 for g1min in range(len(min_df))]\ng1_dropdown_names = [str(g1_min_percent[_x]) + ' %: ' + x for (_x, x) in enumerate(min_df.mineral_name.tolist()) if g1_min_percent[_x] &gt; 0 and x != 'No_Match']\ng1_dropdown_names = np.array(g1_dropdown_names)[np.argsort([float(x.split(' %:')[0]) for x in g1_dropdown_names])[::-1]].tolist()\n\n\n# Interactive Panel Control For Mineral Band Depth\nmin_select = pn.widgets.Select(name='Mineral Name', options = g1_dropdown_names, value = g1_dropdown_names[0])\n@pn.depends(min_select)\ndef min_browse(min_select):\n    mask = ds_min['group_1_band_depth'].where(ds_min['group_1_mineral_id'] == min_df['mineral_name'].tolist().index(min_select.split('%: ')[-1]))\n    map = mask.hvplot.image(cmap='viridis', geo=True, tiles='ESRI', alpha=0.8,frame_width=450, clim=(0,np.nanpercentile(mask,98))).opts(title=f'{min_select} Band Depth')\n    return map\npn.Row(pn.WidgetBox(min_select),min_browse)",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "10. Working with EMIT L2B Mineralogy"
    ]
  },
  {
    "objectID": "external/Working_with_EMIT_L2B_Mineralogy.html#visualize-group-2-minerals",
    "href": "external/Working_with_EMIT_L2B_Mineralogy.html#visualize-group-2-minerals",
    "title": "Working with EMIT L2B Mineralogy Data",
    "section": "2.3 Visualize Group 2 Minerals",
    "text": "2.3 Visualize Group 2 Minerals\nWe can do the same thing with Group 2 Minerals. Group 2 will show a more diverse set of minerals in this region, including clays and carbonates.\n\nds_min['group_2_mineral_id'].hvplot.image(cmap='glasbey', geo=True, tiles='ESRI', alpha=0.8,frame_width=750).opts(title='Group 2 Mineral ID')\n\n\ng2_min_percent = [np.round(np.sum(ds_min.group_2_mineral_id.data.flatten() == g1min) / np.sum(ds_min.group_2_mineral_id.data.flatten() &gt; 0),2) * 100 for g1min in range(len(min_df))]\ng2_dropdown_names = [str(g2_min_percent[_x]) + ' %: ' + x for (_x, x) in enumerate(min_df.mineral_name.tolist()) if g2_min_percent[_x] &gt; 0 and x != 'No_Match']\ng2_dropdown_names = np.array(g2_dropdown_names)[np.argsort([float(x.split(' %:')[0]) for x in g2_dropdown_names])[::-1]].tolist()\n\n\n# Interactive Panel Control For Mineral Band Depth\nmin_select_g2 = pn.widgets.Select(name='Mineral Name', options = g2_dropdown_names, value = g2_dropdown_names[0])\n@pn.depends(min_select_g2)\ndef min_browse_g2(min_select):\n    # print(min_select)\n    mask = ds_min['group_2_band_depth'].where(ds_min['group_2_mineral_id'] == min_df['mineral_name'].tolist().index(min_select.split('%: ')[-1]))\n    map = mask.hvplot.image(cmap='viridis', geo=True, tiles='ESRI', alpha=0.8,frame_width=450, clim=(0,np.nanpercentile(mask,98))).opts(title=f'{min_select} Band Depth')\n    return map\npn.Row(pn.WidgetBox(min_select_g2),min_browse_g2)",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "10. Working with EMIT L2B Mineralogy"
    ]
  },
  {
    "objectID": "external/Working_with_EMIT_L2B_Mineralogy.html#visualizing-and-filtering-with-uncertainties",
    "href": "external/Working_with_EMIT_L2B_Mineralogy.html#visualizing-and-filtering-with-uncertainties",
    "title": "Working with EMIT L2B Mineralogy Data",
    "section": "2.4 Visualizing and filtering with uncertainties",
    "text": "2.4 Visualizing and filtering with uncertainties\nOften just as important as visualizing the core mineral detections is displaying the corresponding uncertainties. The EMIT L2B MIN proudct comes with two different uncertainties (repeated for each group):\n\nFit - this is how good of a fit the individual mineral detection was, as estimated by the correlation coefficient of the alignment between the continuum removed library reference and the continuum removed observed spectra (after scaling).\nBand depth uncertainty - this is the propogation of the reflectance uncertainty through the band depth calculation.\n\nLet’s take a look at each of these by loading the relevant datasets:\n\n# Open an Orthorectify the Uncertainty Data\nds_min_unc = et.emit_xarray(fp_un)\nds_min_unc = et.ortho_xr(ds_min_unc)\nds_min_unc\n\n\nmin_select_fit_g2 = pn.widgets.Select(name='Mineral Name', options = g1_dropdown_names, value = g1_dropdown_names[0])\n@pn.depends(min_select_fit_g2)\ndef min_browse_fit_g2(min_select):\n    mask = ds_min_unc['group_1_fit'].where(ds_min['group_1_mineral_id'] == min_df['mineral_name'].tolist().index(min_select.split('%: ')[-1]))\n    # The fits are scaled down by a factor of two in the current EMIT L2B products, so scale them back up:\n    mask *= 2\n    map = mask.hvplot.image(cmap='viridis', geo=True, tiles='ESRI', alpha=0.8,frame_width=450, clim=(0,np.nanpercentile(mask,98))).opts(title=f'{min_select} Fit')\n    return map\npn.Row(pn.WidgetBox(min_select_fit_g2),min_browse_fit_g2)",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "10. Working with EMIT L2B Mineralogy"
    ]
  },
  {
    "objectID": "external/Working_with_EMIT_L2B_Mineralogy.html#aggregating-and-mineral-abundance",
    "href": "external/Working_with_EMIT_L2B_Mineralogy.html#aggregating-and-mineral-abundance",
    "title": "Working with EMIT L2B Mineralogy Data",
    "section": "3. Aggregating and Mineral Abundance",
    "text": "3. Aggregating and Mineral Abundance\nThe above visualizations walk through the identification of individual library contituents, and visualize band depths. However, the Tetracorder library used by EMIT contains many substrates that are spectrally distinct, but which may be useful to group together for some science applications. The library also contains many mixtures - both aerial and intimate.\nTo start, the mineral_grouping_matrix from the emit-sds-l2b repository (coppied locally) contains information aggregated from laboratory XRD analyses to attempt to quantify the abundance of different minerals within each constituent. A -1 in the spreadsheet indicates an unknown but non-zero quantity, which in the few cases in the EMIT-10 columns we assume to be 100%. Let’s open that spreadsheet and take a look:\n\n# Open Mineral Groupings .csv\nmineral_groupings = pd.read_csv('../../data/mineral_grouping_matrix_20230503.csv')\n# The EMIT 10 Minerals are in columns 6 - 17.  Columns after 17 are experimental, and we'll drop for this tutorial:\nmineral_groupings = mineral_groupings.drop([x for _x, x in enumerate(mineral_groupings) if _x &gt;= 17], axis=1)\n\n# Retrieve the EMIT 10 Mineral Names from Columns 7-16 (starting with 0) in .csv\nmineral_names = [x for _x, x in enumerate(list(mineral_groupings)) if _x &gt; 6 and _x &lt; 17]\n# Use EMIT 10 Mineral Names to Subset .csv to only columns with EMIT 10 mineral_names\nmineral_abundance_ref = np.array(mineral_groupings[mineral_names])\n# Replace Some values in the .csv\nmineral_abundance_ref[np.isnan(mineral_abundance_ref)] = 0\nmineral_abundance_ref[mineral_abundance_ref == -1] = 1\n\nmineral_groupings",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "10. Working with EMIT L2B Mineralogy"
    ]
  },
  {
    "objectID": "external/Working_with_EMIT_L2B_Mineralogy.html#exporting-to-cloud-optimized-geotiffs",
    "href": "external/Working_with_EMIT_L2B_Mineralogy.html#exporting-to-cloud-optimized-geotiffs",
    "title": "Working with EMIT L2B Mineralogy Data",
    "section": "4. Exporting To Cloud-Optimized Geotiffs",
    "text": "4. Exporting To Cloud-Optimized Geotiffs\nWe can select layers from any of the xarray datasets we’ve created and export them to a Cloud-Optimized Geotiff (COG) using the rio.to_raster function from the rasterio library. This will allow us to share the data with others or use it in other GIS software.\nFirst create an output folder.\n\n# Create Out filenames and set folder\nout_folder = '../../data/output/' # may need to change based on your directory structure\n# Create out_folder if it does not exist\nif not os.path.exists(out_folder):\n   os.makedirs(out_folder)\n\n\n4.1 Mineral ID\n\n# Set output Filename\nout_name = f'{ds_min.granule_id}_group_1_mineral_id.tif'\n# Select Group to Output\ndat_out = ds_min['group_1_mineral_id']\n# Replace nan with a fill value\ndat_out.data = np.nan_to_num(dat_out.data, nan=-9999)\n# Change datatype for integers/categorical\ndat_out.data = dat_out.data.astype(int)\n# Encode nodata value\ndat_out.rio.write_nodata(-9999, encoded=True, inplace=True)\n# Write data to COG\ndat_out.rio.to_raster(raster_path=f'{out_folder}{out_name}', driver='COG')\n\n\n\n4.2 Band Depth\n\n# Set output Filename\nout_name = f'{ds_min.granule_id}_group_1_band_depth.tif'\n# Select Group to Output\ndat_out = ds_min['group_1_band_depth']\n# Replace nan with a fill value\ndat_out.data = np.nan_to_num(dat_out.data, nan=-9999)\n# Encode nodata value\ndat_out.rio.write_nodata(-9999, encoded=True, inplace=True)\n# Write data to COG\ndat_out.rio.to_raster(raster_path=f'{out_folder}{out_name}', driver='COG')\n\n\n\n4.3 Abundance\n\n# Set output Filename\nout_name = f'{mineral_abundance_xarray.granule_id}_Calcite.tif'\n# Select Layer\ndat_out = mineral_abundance_xarray['Calcite']\n# Replace nan with a fill value\ndat_out.data = np.nan_to_num(dat_out.data, nan=-9999)\n# Encode nodata value\ndat_out.rio.write_nodata(-9999, encoded=True, inplace=True)\n# Write data to COG\ndat_out.rio.to_raster(raster_path=f'{out_folder}{out_name}', driver='COG')",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "10. Working with EMIT L2B Mineralogy"
    ]
  },
  {
    "objectID": "external/Working_with_EMIT_L2B_Mineralogy.html#contact-info",
    "href": "external/Working_with_EMIT_L2B_Mineralogy.html#contact-info",
    "title": "Working with EMIT L2B Mineralogy Data",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 07-07-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "10. Working with EMIT L2B Mineralogy"
    ]
  },
  {
    "objectID": "external/setup_instructions.html",
    "href": "external/setup_instructions.html",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "The how-tos and tutorials in this repository require a NASA Earthdata account, an installation of Git, and a compatible Python Environment. Resources in the EMIT-Data-Resources repository have been developed using the Openscapes 2i2c JupyterHub cloud workspace.\nFor local Python environment setup we recommend using mamba to manage Python packages. To install mamba, download miniforge for your operating system. If using Windows, be sure to check the box to “Add mamba to my PATH environment variable” to enable use of mamba directly from your command line interface. Note that this may cause an issue if you have an existing mamba install through Anaconda.\n\n\nThese Python Environments will work for all of the guides, how-to’s, and tutorials within this repository, and the VITALS repository.\n\nUsing your preferred command line interface (command prompt, terminal, cmder, etc.) navigate to your local copy of the repository, then type the following to create a compatible Python environment.\nFor Windows:\nmamba create -n lpdaac_vitals -c conda-forge --yes python=3.10 fiona=1.8.22 gdal hvplot geoviews rioxarray rasterio jupyter geopandas earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image jupyterlab seaborn dask ray-default\nFor MacOSX*:\nmamba create -n lpdaac_vitals -c conda-forge --yes python=3.10 gdal=3.7.2 hvplot geoviews rioxarray rasterio geopandas fiona=1.9.4 jupyter earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image seaborn jupyterlab dask\n\n*MacOSX users will need to install “ray[default]” separately using pip after creating and activating the environment.\n\nNext, activate the Python Environment that you just created.\nmamba activate lpdaac_vitals \nAfter activating the environment if using MacOSX, install the “ray[default]” package using pip:\npip install ray[default]\nNow you can launch Jupyter Notebook to open the notebooks included.\njupyter notebook \n\nStill having trouble getting a compatible Python environment set up? Contact LP DAAC User Services.\n\n\n\n\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 06-24-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Setup Instructions",
      "Local Python Environment Setup"
    ]
  },
  {
    "objectID": "external/setup_instructions.html#python-environment-setup",
    "href": "external/setup_instructions.html#python-environment-setup",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "These Python Environments will work for all of the guides, how-to’s, and tutorials within this repository, and the VITALS repository.\n\nUsing your preferred command line interface (command prompt, terminal, cmder, etc.) navigate to your local copy of the repository, then type the following to create a compatible Python environment.\nFor Windows:\nmamba create -n lpdaac_vitals -c conda-forge --yes python=3.10 fiona=1.8.22 gdal hvplot geoviews rioxarray rasterio jupyter geopandas earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image jupyterlab seaborn dask ray-default\nFor MacOSX*:\nmamba create -n lpdaac_vitals -c conda-forge --yes python=3.10 gdal=3.7.2 hvplot geoviews rioxarray rasterio geopandas fiona=1.9.4 jupyter earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image seaborn jupyterlab dask\n\n*MacOSX users will need to install “ray[default]” separately using pip after creating and activating the environment.\n\nNext, activate the Python Environment that you just created.\nmamba activate lpdaac_vitals \nAfter activating the environment if using MacOSX, install the “ray[default]” package using pip:\npip install ray[default]\nNow you can launch Jupyter Notebook to open the notebooks included.\njupyter notebook \n\nStill having trouble getting a compatible Python environment set up? Contact LP DAAC User Services.",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Setup Instructions",
      "Local Python Environment Setup"
    ]
  },
  {
    "objectID": "external/setup_instructions.html#contact-info",
    "href": "external/setup_instructions.html#contact-info",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "Email: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 06-24-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Setup Instructions",
      "Local Python Environment Setup"
    ]
  },
  {
    "objectID": "external/How_to_use_EMIT_Quality_data.html",
    "href": "external/How_to_use_EMIT_Quality_data.html",
    "title": "How to: Use EMIT Quality Data",
    "section": "",
    "text": "This notebook is from EMIT-Data-Resources\n\n\nSource: How to use EMIT Quality Data\n\nImported on: 2024-11-07",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "6. Using Quality Data"
    ]
  },
  {
    "objectID": "external/How_to_use_EMIT_Quality_data.html#using-emit-quality-flag-data",
    "href": "external/How_to_use_EMIT_Quality_data.html#using-emit-quality-flag-data",
    "title": "How to: Use EMIT Quality Data",
    "section": "1. Using EMIT Quality Flag Data",
    "text": "1. Using EMIT Quality Flag Data\nImport the required Python libraries.\n\n# Import Packages\nimport os\nimport earthaccess\nimport netCDF4 as nc\nfrom osgeo import gdal\nimport numpy as np\nimport xarray as xr\nimport hvplot.xarray\nimport holoviews as hv\nimport sys\nsys.path.append('../modules/')\nimport emit_tools\n\nLogin to your NASA Earthdata account and create a .netrc file using the login function from the earthaccess library. If you do not have an Earthdata Account, you can create one here.\n\nearthaccess.login(persist=True)\n\nFor this notebook we will download the files necessary using earthaccess. You can also access the data in place or stream it, but this can slow due to the file sizes. Provide URLs for an EMIT L2A Reflectance and L2A Mask.\n\nurls = ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20220903T163129_2224611_012/EMIT_L2A_RFL_001_20220903T163129_2224611_012.nc',\n        'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20220903T163129_2224611_012/EMIT_L2A_MASK_001_20220903T163129_2224611_012.nc']\n# Get requests https Session using Earthdata Login Info\nfs = earthaccess.get_requests_https_session()\n# Retrieve granule asset ID from URL (to maintain existing naming convention)\nfor url in urls:\n    granule_asset_id = url.split('/')[-1]\n    # Define Local Filepath\n    fp = f'../../data/{granule_asset_id}'\n    # Download the Granule Asset if it doesn't exist\n    if not os.path.isfile(fp):\n        with fs.get(url,stream=True) as src:\n            with open(fp,'wb') as dst:\n                for chunk in src.iter_content(chunk_size=64*1024*1024):\n                    dst.write(chunk)\n\nSet a filepath for the reflectance and mask files downloaded.\n\nfp = '../../data/EMIT_L2A_RFL_001_20220903T163129_2224611_012.nc'\nfp_mask = '../../data/EMIT_L2A_MASK_001_20220903T163129_2224611_012.nc'\n\nThe most efficient way to utilize the mask is to apply it before orthorectification because the orthorectified datasets take up more space. To apply a mask using the L2A Mask file, we want to open it, specify which bands to use in construction of a mask, and then apply the mask.\nTo do this, first take a look at what each band will mask by reading in the sensor_band_parameters group from the mask file as an xarray.dataset then converting to a dataframe.\n\nNote: In the user guide, the bands are indexed as 1-8 not 0-7 as used here.\n\n\nmask_parameters_ds = xr.open_dataset(fp_mask,engine = 'h5netcdf', group='sensor_band_parameters')\nmask_key = mask_parameters_ds['mask_bands'].to_dataframe()\nmask_key",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "6. Using Quality Data"
    ]
  },
  {
    "objectID": "external/How_to_use_EMIT_Quality_data.html#using-emit-band-mask-data",
    "href": "external/How_to_use_EMIT_Quality_data.html#using-emit-band-mask-data",
    "title": "How to: Use EMIT Quality Data",
    "section": "2. Using EMIT Band Mask Data",
    "text": "2. Using EMIT Band Mask Data\nThe EMIT L2A Mask file also contains band_mask data, which indicates whether or not any given wavelength of any given pixel is interpolated. Interpolation occurs either due to a focal plane array bad pixel, or from saturation. This data comes as a packed unsigned integer array with 36 elements.\nUnpack the data an using the band_mask function from the emit_tools module. This function will unpack the data and create an array that can be used to mask the bands/pixels when added as an input into the emit_xarray function.\n\nbmask = emit_tools.band_mask(fp_mask)\nbmask.shape\n\nWe can quickly plot an example of a band where some pixels have been interpolated (band 234).\n\nfrom matplotlib import pyplot as plt\nplt.imshow(bmask[:,:,234])\n\nOr more helpfully, plot a representation of the crosstrack of the detector array to see where interpolation is occuring. If your research depends on spectral features contained within these interpolated crosstrack region, you may want to mask them out.\n\nfig = plt.figure(figsize=(20,10))\n\nplt.imshow(bmask[0,...].T)\nplt.xlabel('Crosstrack Element')",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "6. Using Quality Data"
    ]
  },
  {
    "objectID": "external/How_to_use_EMIT_Quality_data.html#contact-info",
    "href": "external/How_to_use_EMIT_Quality_data.html#contact-info",
    "title": "How to: Use EMIT Quality Data",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 07-03-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "6. Using Quality Data"
    ]
  },
  {
    "objectID": "external/How_to_Extract_Points.html",
    "href": "external/How_to_Extract_Points.html",
    "title": "How to: Extracting EMIT Spectra at Specified Coordinates",
    "section": "",
    "text": "This notebook is from EMIT-Data-Resources\n\n\nSource: How to Extract Points\n\nImported on: 2024-11-07",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "4. Extracting Points"
    ]
  },
  {
    "objectID": "external/How_to_Extract_Points.html#contact-info",
    "href": "external/How_to_Extract_Points.html#contact-info",
    "title": "How to: Extracting EMIT Spectra at Specified Coordinates",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 03-13-2024\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "4. Extracting Points"
    ]
  },
  {
    "objectID": "external/How_to_Convert_to_ENVI.html",
    "href": "external/How_to_Convert_to_ENVI.html",
    "title": "How To: Convert EMIT .nc to .envi",
    "section": "",
    "text": "This notebook is from EMIT-Data-Resources\n\n\nSource: How to Convert to ENVI Format\n\nImported on: 2024-11-07",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "3. Converting to ENVI format"
    ]
  },
  {
    "objectID": "external/How_to_Convert_to_ENVI.html#setup",
    "href": "external/How_to_Convert_to_ENVI.html#setup",
    "title": "How To: Convert EMIT .nc to .envi",
    "section": "Setup",
    "text": "Setup\nImport packages\n\nimport os\nimport earthaccess\n\n\nAuthenticate using Earthdata Login and Download the required Granules\nLogin to your NASA Earthdata account and create a .netrc file using the login function from the earthaccess library. If you do not have an Earthdata Account, you can create one here.\n\nearthaccess.login(persist=True)\n\nFor this notebook we will download the files necessary using earthaccess. You can also access the data in place or stream it, but this can slow due to the file sizes. Provide a URL for an EMIT L2A Reflectance granule.\n\nurl = 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20220903T163129_2224611_012/EMIT_L2A_RFL_001_20220903T163129_2224611_012.nc'\n\nGet an HTTPS Session using your earthdata login, set a local path to save the file, and download the granule asset - This may take a while, the reflectance file is approximately 1.8 GB.\n\n# Get Https Session using Earthdata Login Info\nfs = earthaccess.get_fsspec_https_session()\n# Retrieve granule asset ID from URL (to maintain existing naming convention)\ngranule_asset_id = url.split('/')[-1]\n# Define Local Filepath\nfp = f'../../data/{granule_asset_id}'\n# Download the Granule Asset if it doesn't exist\nif not os.path.isfile(fp):\n    fs.download(url, fp)\n\nNow lets create an output folder where we will save the .envi files.\n\noutpath = '../../data/envi' \nif not os.path.exists(outpath):\n    os.makedirs(outpath)",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "3. Converting to ENVI format"
    ]
  },
  {
    "objectID": "external/How_to_Convert_to_ENVI.html#method-1-using-write_envi-from-the-emit_tools-module.",
    "href": "external/How_to_Convert_to_ENVI.html#method-1-using-write_envi-from-the-emit_tools-module.",
    "title": "How To: Convert EMIT .nc to .envi",
    "section": "Method 1: Using write_envi from the emit_tools module.",
    "text": "Method 1: Using write_envi from the emit_tools module.\nImport the necessary packages for this method.\n\nimport sys\nsys.path.append('../modules/')\nimport emit_tools as et\n\nOpen the granule using the emit_xarray function. We can orthorectify here if so desired.\n\nds = et.emit_xarray(fp, ortho=True)\nds\n\nNow, write the dataset as an .envi output. If we chose not to orthorectify, you can include a glt file to orthorectify later.\n\net.write_envi(ds, outpath, overwrite=False, extension='.img', interleave='BIL', glt_file=False)",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "3. Converting to ENVI format"
    ]
  },
  {
    "objectID": "external/How_to_Convert_to_ENVI.html#method-2-using-reformat.py-from-emit-utils",
    "href": "external/How_to_Convert_to_ENVI.html#method-2-using-reformat.py-from-emit-utils",
    "title": "How To: Convert EMIT .nc to .envi",
    "section": "Method 2: Using reformat.py from emit-utils",
    "text": "Method 2: Using reformat.py from emit-utils\n\n2.1 Clone and Install emit-utils\nClone the emit-utils repository.\n\n!git clone https://github.com/emit-sds/emit-utils.git ../emit_utils/\n\nThis will copy the emit-utils repository to a folder within this repository.\nAfter you have copied it, use pip package manager to install the directory as a package to ensure you have all of the dependencies and be used in the command line.\n\nThis requires that some dependencies already be installed to work properly on Windows. If you have created the Python environment described in the setup instructions it should work.\n\n\n!pip install --editable ../emit_utils\n\nAfter successfully installing emit-utils, you can use the scripts contained within as part of your workflows.\n\n\n2.2 Executing the Reformat Script\nBefore calling the reformat.py script, make sure you have an output directory for the .envi files that will be produced.\n\nimport os\noutpath = '../../data/envi' \nif not os.path.exists(outpath):\n    os.makedirs(outpath)\n\nNow, execute the reformat.py script contained in the emit-utils repository. When executing this script, provide the path to the .nc file, followed by the directory to place the .envi files in. If you wish to apply the GLT or orthorectify, include --orthorectify as an argument.\n\n!python ../emit_utils/emit_utils/reformat.py ../../data/EMIT_L2A_RFL_001_20220903T163129_2224611_012.nc ../../data/envi/ --orthorectify\n\nThis will orthorectify the image, create an ENVI header, and save it in .envi format inside the ../data/envi folder.",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "3. Converting to ENVI format"
    ]
  },
  {
    "objectID": "external/How_to_Convert_to_ENVI.html#contact-info",
    "href": "external/How_to_Convert_to_ENVI.html#contact-info",
    "title": "How To: Convert EMIT .nc to .envi",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 07-06-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "3. Converting to ENVI format"
    ]
  },
  {
    "objectID": "external/Finding_EMIT_L2B_Data.html",
    "href": "external/Finding_EMIT_L2B_Data.html",
    "title": "Finding EMIT L2B Data",
    "section": "",
    "text": "This notebook is from EMIT-Data-Resources\n\n\nSource: Finding EMIT L2B Data\n\nImported on: 2024-11-07",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "9. Finding EMIT L2B Data"
    ]
  },
  {
    "objectID": "external/Finding_EMIT_L2B_Data.html#setup",
    "href": "external/Finding_EMIT_L2B_Data.html#setup",
    "title": "Finding EMIT L2B Data",
    "section": "1. Setup",
    "text": "1. Setup\nImport the required Python libraries.\n\n# Import required libraries\nimport os\nimport sys\nimport folium\nimport earthaccess\nimport warnings\nimport folium.plugins\nimport pandas as pd\nimport geopandas as gpd\nimport math\n\nfrom branca.element import Figure\nfrom IPython.display import display\nfrom shapely import geometry\nfrom skimage import io\nfrom datetime import timedelta\nfrom shapely.geometry.polygon import orient\nfrom matplotlib import pyplot as plt\nimport matplotlib.cm as cm\n\nsys.path.append('../modules/')\nfrom tutorial_utils import list_metadata_fields, results_to_geopandas, convert_bounds\n\n\n1.2 NASA Earthdata Login Credentials\nTo download or stream NASA data you will need an Earthdata account, you can create one here. Searching We will use the login function from the earthaccess library for authentication before downloading at the end of the notebook. This function can also be used to create a local .netrc file if it doesn’t exist or add your login info to an existing .netrc file. If no Earthdata Login credentials are found in the .netrc you’ll be prompted for them. This step is not necessary to conduct searches but is needed to download or stream data.",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "9. Finding EMIT L2B Data"
    ]
  },
  {
    "objectID": "external/Finding_EMIT_L2B_Data.html#searching-for-emit-l2b-mineralogy-data",
    "href": "external/Finding_EMIT_L2B_Data.html#searching-for-emit-l2b-mineralogy-data",
    "title": "Finding EMIT L2B Data",
    "section": "2. Searching for EMIT L2B Mineralogy Data",
    "text": "2. Searching for EMIT L2B Mineralogy Data\nTo find data we will use the earthaccess Python library. earthaccess searches NASA Common Metadata Repository (CMR) API, a metadata system that catalogs Earth Science data and associated metadata records. The results can then be used to download granules or generate lists of granule search result URLs.\nUsing earthaccess we can search based on the attributes of a granule, which can be thought of as a spatiotemporal scene from an instrument containing multiple assets (ex: Reflectance, Reflectance Uncertainty, Masks for the EMIT L2A Reflectance Collection, and EMIT ). When conducting a search we can provide a product, in this case the mineralogy product, a date-time range, and spatial constraints. This process can also be used with other EMIT products, other NASA collections.\n\n2.1 Querying for Datasets\nOur first step in searching for data is determining which collection (e.g. EMIT L2A Estimated Surface Reflectance Uncertainty and Masks, EMIT L2B Estimated Mineral Identification and Band Depth and Uncertainty) we want to search for. The best way to do this is using the collection short_name (e.g. EMITL2ARFL, EMITL2BMIN) or concept-id. In rare cases, the short_name of two collections can be the same, so we will use the concept-id which is a unique identifier for each collection. To find the concept-id we can search using some keywords.\n\n# EMIT Collection Query\nemit_collection_query = earthaccess.collection_query().keyword('EMIT L2B Mineral')\nemit_collection_query.fields(['ShortName','EntryTitle','Version']).get()\n\n[{\n   \"meta\": {\n     \"concept-id\": \"C2408034484-LPCLOUD\",\n     \"granule-count\": 87550,\n     \"provider-id\": \"LPCLOUD\"\n   },\n   \"umm\": {\n     \"ShortName\": \"EMITL2BMIN\",\n     \"EntryTitle\": \"EMIT L2B Estimated Mineral Identification and Band Depth and Uncertainty 60 m V001\",\n     \"Version\": \"001\"\n   }\n },\n {\n   \"meta\": {\n     \"concept-id\": \"C2748097305-LPCLOUD\",\n     \"granule-count\": 1748,\n     \"provider-id\": \"LPCLOUD\"\n   },\n   \"umm\": {\n     \"ShortName\": \"EMITL2BCH4ENH\",\n     \"EntryTitle\": \"EMIT L2B Methane Enhancement Data 60 m V001\",\n     \"Version\": \"001\"\n   }\n },\n {\n   \"meta\": {\n     \"concept-id\": \"C2748088093-LPCLOUD\",\n     \"granule-count\": 1285,\n     \"provider-id\": \"LPCLOUD\"\n   },\n   \"umm\": {\n     \"ShortName\": \"EMITL2BCH4PLM\",\n     \"EntryTitle\": \"EMIT L2B Estimated Methane Plume Complexes 60 m V001\",\n     \"Version\": \"001\"\n   }\n },\n {\n   \"meta\": {\n     \"concept-id\": \"C2872578364-LPCLOUD\",\n     \"granule-count\": 402,\n     \"provider-id\": \"LPCLOUD\"\n   },\n   \"umm\": {\n     \"ShortName\": \"EMITL2BCO2ENH\",\n     \"EntryTitle\": \"EMIT L2B Carbon Dioxide Enhancement Data 60 m V001\",\n     \"Version\": \"001\"\n   }\n },\n {\n   \"meta\": {\n     \"concept-id\": \"C2867824144-LPCLOUD\",\n     \"granule-count\": 173,\n     \"provider-id\": \"LPCLOUD\"\n   },\n   \"umm\": {\n     \"ShortName\": \"EMITL2BCO2PLM\",\n     \"EntryTitle\": \"EMIT L2B Estimated Carbon Dioxide Plume Complexes 60 m V001\",\n     \"Version\": \"001\"\n   }\n },\n {\n   \"meta\": {\n     \"concept-id\": \"C2408752948-LPCLOUD\",\n     \"granule-count\": 1,\n     \"provider-id\": \"LPCLOUD\"\n   },\n   \"umm\": {\n     \"ShortName\": \"EMITL3ASA\",\n     \"EntryTitle\": \"EMIT L3 Aggregated Mineral Spectral Abundance and Uncertainty 0.5 Deg V001\",\n     \"Version\": \"001\"\n   }\n }]\n\n\nFrom this list of results we can see that the concept-id for the desired mineral product is C2408034484-LPCLOUD. We can use this to define one of our search arguments.\n\nconcept_id = 'C2408034484-LPCLOUD'\n\n\n\n2.2 Define Temporal Range\nFor our date range, we’ll look at all EMIT data collected over 2023. The date_range can be specified as a pair of dates, start and end (up to, not including).\n\ndate_range = ('2023-01-01','2024-01-01')\n\n\n\n2.3 Define Spatial Region of Interest\nFor this example, our spatial region of interest will be the area around Cuprite, NV. A location where there have been several previous mineralogy studies. We can define this region using a rectangular polygon. If you want to make a polygon for a different region, you can use a tool like geojson.io.\nOpen the geojson as a geodataframe, and check the coordinate reference system (CRS) of the data.\n\nroi_gdf = gpd.read_file('../../data/cuprite_bbox.geojson')\nroi_gdf.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\nroi_gdf\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOLYGON ((-117.24309 37.59129, -117.24309 37.5...\n\n\n\n\n\n\n\nWe can see this geodataframe consists of a single polygon which we want to include in our search, but the geometry is the only information contained in the file, so lets add a column for the site name, and set the value to “Cuprite”.\n\nroi_gdf['Name'] = 'Cuprite'\n\n\nroi_gdf\n\n\n\n\n\n\n\n\ngeometry\nName\n\n\n\n\n0\nPOLYGON ((-117.24309 37.59129, -117.24309 37.5...\nCuprite\n\n\n\n\n\n\n\nWe can visualize the ROI using the folium library and the explore function from geopandas. First, we’ll create a helper function. Then we will create a new map, using Google Maps tiles as our basemap, and add the polygon to the map. We’ll also use our convert_bounds helper function to limit the map view to roughly the extent of the polygon.\n\n# Function to convert a bounding box for use in leaflet notation\n\ndef convert_bounds(bbox, invert_y=False):\n    \"\"\"\n    Helper method for changing bounding box representation to leaflet notation\n\n    ``(lon1, lat1, lon2, lat2) -&gt; ((lat1, lon1), (lat2, lon2))``\n    \"\"\"\n    x1, y1, x2, y2 = bbox\n    if invert_y:\n        y1, y2 = y2, y1\n    return ((y1, x1), (y2, x2))\n\n\nfig = Figure(width=\"750px\", height=\"375px\")\nmap1 = folium.Map(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}', attr='Google')\nfig.add_child(map1)\n\n# Add roi geodataframe\nroi_gdf.explore(\n    \"Name\",\n    popup=True,\n    categorical=True,\n    cmap='Set3',\n    style_kwds=dict(opacity=0.7, fillOpacity=0.4),\n    name=\"Regions of Interest\",\n    m=map1\n)\n\nmap1.add_child(folium.LayerControl())\nmap1.fit_bounds(bounds=convert_bounds(roi_gdf.unary_union.bounds))\ndisplay(fig)\n\n\n\n\nIn our earthaccess search, we will use the polygon argument to find where this geometry intersects with the footprint of the EMIT scenes. To do this, we need to create a list of exterior polygon vertices in counter-clockwise order to submit in our search.\n\n# Use orient to place vertices in counter-clockwise order\nroi = orient(roi_gdf.geometry[0], sign = 1.0)\n# Put the exterior coordinates in a list\nroi = list(roi.exterior.coords)\nroi\n\n[(-117.24309240198033, 37.59129385913785),\n (-117.24309240198033, 37.50102626452812),\n (-117.14631968357332, 37.50102626452812),\n (-117.14631968357332, 37.59129385913785),\n (-117.24309240198033, 37.59129385913785)]\n\n\nAfter we have all of the pieces: spatial extent, temporal range, and concept-id, we can perform a search. Note that we are limiting our search 500 results using the count argument, which doesn’t matter here.\n\nresults = earthaccess.search_data(\n    concept_id=concept_id,\n    polygon=roi,\n    temporal=date_range,\n    count=500\n)\n\nGranules found: 9\n\n\nOur search returned 9 results, which we can convert to a geodataframe for further filtering and analysis.\n\nresults\n\n[Collection: {'ShortName': 'EMITL2BMIN', 'Version': '001'}\n Spatial coverage: {'HorizontalSpatialDomain': {'Geometry': {'GPolygons': [{'Boundary': {'Points': [{'Longitude': -116.86429595947266, 'Latitude': 37.96258544921875}, {'Longitude': -117.72748565673828, 'Latitude': 37.362998962402344}, {'Longitude': -117.27104949951172, 'Latitude': 36.70587921142578}, {'Longitude': -116.4078598022461, 'Latitude': 37.30546569824219}, {'Longitude': -116.86429595947266, 'Latitude': 37.96258544921875}]}}]}}}\n Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2023-01-29T21:14:07Z', 'EndingDateTime': '2023-01-29T21:14:19Z'}}\n Size(MB): 75.91049480438232\n Data: ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230129T211407_2302914_008/EMIT_L2B_MIN_001_20230129T211407_2302914_008.nc', 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230129T211407_2302914_008/EMIT_L2B_MINUNCERT_001_20230129T211407_2302914_008.nc'],\n Collection: {'ShortName': 'EMITL2BMIN', 'Version': '001'}\n Spatial coverage: {'HorizontalSpatialDomain': {'Geometry': {'GPolygons': [{'Boundary': {'Points': [{'Longitude': -117.79698181152344, 'Latitude': 38.333797454833984}, {'Longitude': -118.24942016601562, 'Latitude': 37.6685905456543}, {'Longitude': -117.468505859375, 'Latitude': 37.1374397277832}, {'Longitude': -117.01606750488281, 'Latitude': 37.80264663696289}, {'Longitude': -117.79698181152344, 'Latitude': 38.333797454833984}]}}]}}}\n Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2023-04-27T17:32:57Z', 'EndingDateTime': '2023-04-27T17:33:09Z'}}\n Size(MB): 101.56163311004639\n Data: ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230427T173257_2311711_009/EMIT_L2B_MIN_001_20230427T173257_2311711_009.nc', 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230427T173257_2311711_009/EMIT_L2B_MINUNCERT_001_20230427T173257_2311711_009.nc'],\n Collection: {'ShortName': 'EMITL2BMIN', 'Version': '001'}\n Spatial coverage: {'HorizontalSpatialDomain': {'Geometry': {'GPolygons': [{'Boundary': {'Points': [{'Longitude': -117.09042358398438, 'Latitude': 37.85598373413086}, {'Longitude': -117.54974365234375, 'Latitude': 37.194339752197266}, {'Longitude': -116.77647399902344, 'Latitude': 36.65752029418945}, {'Longitude': -116.31715393066406, 'Latitude': 37.31916427612305}, {'Longitude': -117.09042358398438, 'Latitude': 37.85598373413086}]}}]}}}\n Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2023-04-27T17:33:09Z', 'EndingDateTime': '2023-04-27T17:33:21Z'}}\n Size(MB): 100.93134117126465\n Data: ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230427T173309_2311711_010/EMIT_L2B_MIN_001_20230427T173309_2311711_010.nc', 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230427T173309_2311711_010/EMIT_L2B_MINUNCERT_001_20230427T173309_2311711_010.nc'],\n Collection: {'ShortName': 'EMITL2BMIN', 'Version': '001'}\n Spatial coverage: {'HorizontalSpatialDomain': {'Geometry': {'GPolygons': [{'Boundary': {'Points': [{'Longitude': -117.15681457519531, 'Latitude': 38.27824783325195}, {'Longitude': -118.03521728515625, 'Latitude': 37.68120193481445}, {'Longitude': -117.58502197265625, 'Latitude': 37.018863677978516}, {'Longitude': -116.70661926269531, 'Latitude': 37.615909576416016}, {'Longitude': -117.15681457519531, 'Latitude': 38.27824783325195}]}}]}}}\n Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2023-08-04T19:16:50Z', 'EndingDateTime': '2023-08-04T19:17:02Z'}}\n Size(MB): 101.90474605560303\n Data: ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230804T191650_2321613_007/EMIT_L2B_MIN_001_20230804T191650_2321613_007.nc', 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230804T191650_2321613_007/EMIT_L2B_MINUNCERT_001_20230804T191650_2321613_007.nc'],\n Collection: {'ShortName': 'EMITL2BMIN', 'Version': '001'}\n Spatial coverage: {'HorizontalSpatialDomain': {'Geometry': {'GPolygons': [{'Boundary': {'Points': [{'Longitude': -116.7279052734375, 'Latitude': 37.98891830444336}, {'Longitude': -117.59428405761719, 'Latitude': 37.39017105102539}, {'Longitude': -117.13900756835938, 'Latitude': 36.73139572143555}, {'Longitude': -116.27262878417969, 'Latitude': 37.330142974853516}, {'Longitude': -116.7279052734375, 'Latitude': 37.98891830444336}]}}]}}}\n Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2023-08-08T17:39:53Z', 'EndingDateTime': '2023-08-08T17:40:05Z'}}\n Size(MB): 102.3443374633789\n Data: ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230808T173953_2322012_011/EMIT_L2B_MIN_001_20230808T173953_2322012_011.nc', 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230808T173953_2322012_011/EMIT_L2B_MINUNCERT_001_20230808T173953_2322012_011.nc'],\n Collection: {'ShortName': 'EMITL2BMIN', 'Version': '001'}\n Spatial coverage: {'HorizontalSpatialDomain': {'Geometry': {'GPolygons': [{'Boundary': {'Points': [{'Longitude': -117.73664093017578, 'Latitude': 38.3143196105957}, {'Longitude': -118.18502044677734, 'Latitude': 37.653934478759766}, {'Longitude': -117.4017105102539, 'Latitude': 37.12208938598633}, {'Longitude': -116.95333099365234, 'Latitude': 37.782474517822266}, {'Longitude': -117.73664093017578, 'Latitude': 38.3143196105957}]}}]}}}\n Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2023-10-23T18:39:05Z', 'EndingDateTime': '2023-10-23T18:39:17Z'}}\n Size(MB): 101.62794208526611\n Data: ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20231023T183905_2329612_009/EMIT_L2B_MIN_001_20231023T183905_2329612_009.nc', 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20231023T183905_2329612_009/EMIT_L2B_MINUNCERT_001_20231023T183905_2329612_009.nc'],\n Collection: {'ShortName': 'EMITL2BMIN', 'Version': '001'}\n Spatial coverage: {'HorizontalSpatialDomain': {'Geometry': {'GPolygons': [{'Boundary': {'Points': [{'Longitude': -117.02520751953125, 'Latitude': 37.83479690551758}, {'Longitude': -117.48287963867188, 'Latitude': 37.17849349975586}, {'Longitude': -116.71324157714844, 'Latitude': 36.641788482666016}, {'Longitude': -116.25556945800781, 'Latitude': 37.298091888427734}, {'Longitude': -117.02520751953125, 'Latitude': 37.83479690551758}]}}]}}}\n Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2023-10-23T18:39:17Z', 'EndingDateTime': '2023-10-23T18:39:29Z'}}\n Size(MB): 100.52127742767334\n Data: ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20231023T183917_2329612_010/EMIT_L2B_MIN_001_20231023T183917_2329612_010.nc', 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20231023T183917_2329612_010/EMIT_L2B_MINUNCERT_001_20231023T183917_2329612_010.nc'],\n Collection: {'ShortName': 'EMITL2BMIN', 'Version': '001'}\n Spatial coverage: {'HorizontalSpatialDomain': {'Geometry': {'GPolygons': [{'Boundary': {'Points': [{'Longitude': -117.53460693359375, 'Latitude': 38.284488677978516}, {'Longitude': -118.40948486328125, 'Latitude': 37.68711471557617}, {'Longitude': -117.95744323730469, 'Latitude': 37.025081634521484}, {'Longitude': -117.08256530761719, 'Latitude': 37.62245559692383}, {'Longitude': -117.53460693359375, 'Latitude': 38.284488677978516}]}}]}}}\n Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2023-12-01T20:10:58Z', 'EndingDateTime': '2023-12-01T20:11:10Z'}}\n Size(MB): 101.65311431884766\n Data: ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20231201T201058_2333513_006/EMIT_L2B_MIN_001_20231201T201058_2333513_006.nc', 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20231201T201058_2333513_006/EMIT_L2B_MINUNCERT_001_20231201T201058_2333513_006.nc'],\n Collection: {'ShortName': 'EMITL2BMIN', 'Version': '001'}\n Spatial coverage: {'HorizontalSpatialDomain': {'Geometry': {'GPolygons': [{'Boundary': {'Points': [{'Longitude': -116.26719665527344, 'Latitude': 39.1148796081543}, {'Longitude': -117.70616149902344, 'Latitude': 38.172176361083984}, {'Longitude': -117.26689147949219, 'Latitude': 37.50168228149414}, {'Longitude': -115.82792663574219, 'Latitude': 38.44438552856445}, {'Longitude': -116.26719665527344, 'Latitude': 39.1148796081543}]}}]}}}\n Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2023-12-01T20:11:10Z', 'EndingDateTime': '2023-12-01T20:11:31Z'}}\n Size(MB): 170.77763843536377\n Data: ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20231201T201110_2333513_007/EMIT_L2B_MIN_001_20231201T201110_2333513_007.nc', 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20231201T201110_2333513_007/EMIT_L2B_MINUNCERT_001_20231201T201110_2333513_007.nc']]",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "9. Finding EMIT L2B Data"
    ]
  },
  {
    "objectID": "external/Finding_EMIT_L2B_Data.html#advanced-filtering",
    "href": "external/Finding_EMIT_L2B_Data.html#advanced-filtering",
    "title": "Finding EMIT L2B Data",
    "section": "3. Advanced Filtering",
    "text": "3. Advanced Filtering\nNow that we have some results, we will place them into a geodataframe that includes links to browse imagery and the files, so we can do some more advanced filtering of the data.\nList the metadata fields available in the search results.\n\nlist_metadata_fields(results)\n\n['size',\n 'concept-type',\n 'concept-id',\n 'revision-id',\n 'native-id',\n 'collection-concept-id',\n 'provider-id',\n 'format',\n 'revision-date',\n '_beginning_date_time',\n '_ending_date_time',\n '_granule_ur',\n '_additional_attributes',\n '_gpolygons',\n '_provider_dates',\n '_short_name',\n '_version',\n '_pgename',\n '_pgeversion',\n '_related_urls',\n '_cloud_cover',\n '_day_night_flag',\n '_archive_and_distribution_information',\n '_production_date_time',\n '_platforms',\n '_url',\n '_name',\n '_version']\n\n\nSome datasets have unique metadata that we can choose to include when we use our results_to_geopandas function from the tutorial_utils.py module to create a geodataframe. Below is a list of default fields. We can also include additional fields by passing them as a list to the fields argument.\ndefault_fields = [\n“size”,\n“concept_id”,\n“dataset-id”,\n“native-id”,\n“provider-id”,\n“_related_urls”,\n“_single_date_time”,\n“_beginning_date_time”,\n“_ending_date_time”,\n“geometry”,\n]\nFor example, _cloud_cover is not always available. We can add it to the default fields of this function by adding it to a fields argument in list form.\n\nresults_gdf = results_to_geopandas(results, fields=['_cloud_cover'])\n\nAdd an index column so we can reference it using the explore function from geopandas\n\n# Specify index so we can reference it with gdf.explore()\nresults_gdf['index']=results_gdf.index\n\n\nresults_gdf\n\n\n\n\n\n\n\n\nsize\nnative-id\nprovider-id\n_beginning_date_time\n_ending_date_time\n_related_urls\n_cloud_cover\ngeometry\nindex\n\n\n\n\n0\n75.910495\nEMIT_L2B_MIN_001_20230129T211407_2302914_008\nLPCLOUD\n2023-01-29T21:14:07Z\n2023-01-29T21:14:19Z\n[{'URL': 'https://data.lpdaac.earthdatacloud.n...\n99\nPOLYGON ((-116.86430 37.96259, -117.72749 37.3...\n0\n\n\n1\n101.561633\nEMIT_L2B_MIN_001_20230427T173257_2311711_009\nLPCLOUD\n2023-04-27T17:32:57Z\n2023-04-27T17:33:09Z\n[{'URL': 'https://data.lpdaac.earthdatacloud.n...\n21\nPOLYGON ((-117.79698 38.33380, -118.24942 37.6...\n1\n\n\n2\n100.931341\nEMIT_L2B_MIN_001_20230427T173309_2311711_010\nLPCLOUD\n2023-04-27T17:33:09Z\n2023-04-27T17:33:21Z\n[{'URL': 'https://data.lpdaac.earthdatacloud.n...\n8\nPOLYGON ((-117.09042 37.85598, -117.54974 37.1...\n2\n\n\n3\n101.904746\nEMIT_L2B_MIN_001_20230804T191650_2321613_007\nLPCLOUD\n2023-08-04T19:16:50Z\n2023-08-04T19:17:02Z\n[{'URL': 'https://data.lpdaac.earthdatacloud.n...\n4\nPOLYGON ((-117.15681 38.27825, -118.03522 37.6...\n3\n\n\n4\n102.344337\nEMIT_L2B_MIN_001_20230808T173953_2322012_011\nLPCLOUD\n2023-08-08T17:39:53Z\n2023-08-08T17:40:05Z\n[{'URL': 'https://data.lpdaac.earthdatacloud.n...\n6\nPOLYGON ((-116.72791 37.98892, -117.59428 37.3...\n4\n\n\n5\n101.627942\nEMIT_L2B_MIN_001_20231023T183905_2329612_009\nLPCLOUD\n2023-10-23T18:39:05Z\n2023-10-23T18:39:17Z\n[{'URL': 'https://data.lpdaac.earthdatacloud.n...\n74\nPOLYGON ((-117.73664 38.31432, -118.18502 37.6...\n5\n\n\n6\n100.521277\nEMIT_L2B_MIN_001_20231023T183917_2329612_010\nLPCLOUD\n2023-10-23T18:39:17Z\n2023-10-23T18:39:29Z\n[{'URL': 'https://data.lpdaac.earthdatacloud.n...\n32\nPOLYGON ((-117.02521 37.83480, -117.48288 37.1...\n6\n\n\n7\n101.653114\nEMIT_L2B_MIN_001_20231201T201058_2333513_006\nLPCLOUD\n2023-12-01T20:10:58Z\n2023-12-01T20:11:10Z\n[{'URL': 'https://data.lpdaac.earthdatacloud.n...\n65\nPOLYGON ((-117.53461 38.28449, -118.40948 37.6...\n7\n\n\n8\n170.777638\nEMIT_L2B_MIN_001_20231201T201110_2333513_007\nLPCLOUD\n2023-12-01T20:11:10Z\n2023-12-01T20:11:31Z\n[{'URL': 'https://data.lpdaac.earthdatacloud.n...\n53\nPOLYGON ((-116.26720 39.11488, -117.70616 38.1...\n8\n\n\n\n\n\n\n\nFilter the results geodataframe by cloud cover. We’ll use a cloud cover of 10% as our threshold.\n\n# Filter Results\nresults_gdf = results_gdf[results_gdf['_cloud_cover'] &lt; 10]\nresults_gdf.reset_index(drop=True, inplace=True)\n\nVisualize the filtered results by iterating over the rows of the geodataframe and adding the geometry to the map. We do this instead of the explore function, so we add separate layers for each, allowing use to add or remove them using the LayerControl widget.\n\n# Set up Figure and Basemap tiles\nfig = Figure(width=\"1080px\",height=\"540\")\nmap1 = folium.Map(tiles=None)\nfolium.TileLayer(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',name='Google Satellite', attr='Google', overlay=True).add_to(map1)\nfolium.TileLayer(tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}.png',\n                name='ESRI World Imagery',\n                attr='Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community',\n                overlay='True').add_to(map1)\nfig.add_child(map1)\n\n\n# Create a color map for the results\ncmap = cm.get_cmap('Set3')\nn = len(results_gdf['native-id'].unique())\ncolors = [cmap(i) for i in range(n)]\ncolors = [cm.colors.rgb2hex(color) for color in colors]\n\n# Add Search Results by Row\nfor index, row in results_gdf.iterrows():\n    color = colors[index % len(colors)]\n    folium.GeoJson(row.geometry, name = row['native-id'],style_function=lambda feature, color=color: {'color': color, 'fillColor': color}).add_to(map1)\n\nfolium.GeoJson(roi_gdf,\n                name='Cuprite_ROI',\n                ).add_to(map1)\n\n# Zoom to Data\nmap1.fit_bounds(bounds=convert_bounds(results_gdf.unary_union.bounds))\n# Add Layer controls\nmap1.add_child(folium.LayerControl(collapsed=False))\ndisplay(fig)\n\nC:\\Users\\ebolch\\AppData\\Local\\Temp\\1\\ipykernel_16232\\2570979738.py:13: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n  cmap = cm.get_cmap('Set3')\n\n\n\n\n\nView the related urls for the first result. We can see that there are multiple assets available for each result, including the mineralogy data, uncertainty data, and browse images, as well as multiple ways to access the data, https or s3 links.\n\nresults_gdf._related_urls[0]\n\n[{'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230427T173309_2311711_010/EMIT_L2B_MIN_001_20230427T173309_2311711_010.nc',\n  'Description': 'Download EMIT_L2B_MIN_001_20230427T173309_2311711_010.nc',\n  'Type': 'GET DATA'},\n {'URL': 's3://lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230427T173309_2311711_010/EMIT_L2B_MIN_001_20230427T173309_2311711_010.nc',\n  'Description': 'This link provides direct download access via S3 to the granule',\n  'Type': 'GET DATA VIA DIRECT ACCESS'},\n {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230427T173309_2311711_010/EMIT_L2B_MINUNCERT_001_20230427T173309_2311711_010.nc',\n  'Description': 'Download EMIT_L2B_MINUNCERT_001_20230427T173309_2311711_010.nc',\n  'Type': 'GET DATA'},\n {'URL': 's3://lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230427T173309_2311711_010/EMIT_L2B_MINUNCERT_001_20230427T173309_2311711_010.nc',\n  'Description': 'This link provides direct download access via S3 to the granule',\n  'Type': 'GET DATA VIA DIRECT ACCESS'},\n {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230427T173309_2311711_010/EMIT_L2B_MIN_001_20230427T173309_2311711_010.png',\n  'Description': 'Download EMIT_L2B_MIN_001_20230427T173309_2311711_010.png',\n  'Type': 'GET RELATED VISUALIZATION'},\n {'URL': 's3://lp-prod-public/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230427T173309_2311711_010/EMIT_L2B_MIN_001_20230427T173309_2311711_010.png',\n  'Description': 'This link provides direct download access via S3 to the granule',\n  'Type': 'GET RELATED VISUALIZATION'}]\n\n\nWe can use a function to return the asset URLs for a given result. This function will return a dictionary with the asset names as keys and the URLs as values.\n\ndef get_asset_url(row,asset, key='Type',value='GET DATA'):\n    \"\"\"\n    Retrieve a url from the list of dictionaries for a row in the _related_urls column.\n    Asset examples: CH4PLM, CH4PLMMETA, RFL, MASK, RFLUNCERT \n    \"\"\"\n    # Add _ to asset so string matching works\n    asset = f\"_{asset}_\"\n    # Retrieve URL matching parameters\n    for _dict in row['_related_urls']:\n        if _dict.get(key) == value and asset in _dict['URL'].split('/')[-1]:\n            return _dict['URL']\n\nApply the function for to the results geodataframe to get the asset URLs for each result for the L2B_MIN asset.\n\n# Iterate over rows in the plm_gdf and get the mineral urls and store them in a list\nmin_urls = results_gdf.apply(lambda row: get_asset_url(row, asset='L2B_MIN'), axis=1).tolist()\nmin_urls\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230427T173309_2311711_010/EMIT_L2B_MIN_001_20230427T173309_2311711_010.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230804T191650_2321613_007/EMIT_L2B_MIN_001_20230804T191650_2321613_007.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230808T173953_2322012_011/EMIT_L2B_MIN_001_20230808T173953_2322012_011.nc']\n\n\nWe can repeat this for the uncertainty URLs.\n\nmin_unc_urls = results_gdf.apply(lambda row: get_asset_url(row, asset='L2B_MINUNCERT'), axis=1).tolist()\nmin_unc_urls\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230427T173309_2311711_010/EMIT_L2B_MINUNCERT_001_20230427T173309_2311711_010.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230804T191650_2321613_007/EMIT_L2B_MINUNCERT_001_20230804T191650_2321613_007.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230808T173953_2322012_011/EMIT_L2B_MINUNCERT_001_20230808T173953_2322012_011.nc']\n\n\nWith some knowledge of how the granules and assets are neamed, we can grab the rgb browse images to get an idea of what the location looks like. First retrieve the browse images for the mineral product. These, show the mineral band depth only.\n\nmin_png = results_gdf.apply(lambda row: get_asset_url(row, asset='L2B_MIN', value='GET RELATED VISUALIZATION'), axis=1).tolist()\nmin_png\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230427T173309_2311711_010/EMIT_L2B_MIN_001_20230427T173309_2311711_010.png',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230804T191650_2321613_007/EMIT_L2B_MIN_001_20230804T191650_2321613_007.png',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/EMITL2BMIN.001/EMIT_L2B_MIN_001_20230808T173953_2322012_011/EMIT_L2B_MIN_001_20230808T173953_2322012_011.png']\n\n\nWith some slight changes to these, we can retrieve the RGB browse images from the L2A Reflectance product.\n\n# Replace Collection ID\nrgb_urls = [s.replace('EMITL2BMIN', 'EMITL2ARFL') for s in min_png]\n# Update Product and Asset Names\nrgb_urls = [s.replace('EMIT_L2B_MIN', 'EMIT_L2A_RFL') for s in rgb_urls]\n# Change file extension\n#rgb_urls = [s.replace('.nc', '.png') for s in rgb_urls]\nrgb_urls\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230427T173309_2311711_010/EMIT_L2A_RFL_001_20230427T173309_2311711_010.png',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230804T191650_2321613_007/EMIT_L2A_RFL_001_20230804T191650_2321613_007.png',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230808T173953_2322012_011/EMIT_L2A_RFL_001_20230808T173953_2322012_011.png']\n\n\nVisualize the RGB browse images to get an idea for what the area we are investigating looks like.\n\ncols = 3\nrows = math.ceil(len(results_gdf)/cols)\nfig, ax = plt.subplots(rows, cols, figsize=(12,12))\nax = ax.flatten()\n\nfor _n, index in enumerate(results_gdf.index.to_list()):\n    img = io.imread(rgb_urls[index])\n    ax[_n].imshow(img)\n    ax[_n].set_title(f\"Index: {index} - {results_gdf['native-id'][index]}\", fontsize=8)\n    ax[_n].axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe black line in the third scene is caused by the on-board cloud masking.",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "9. Finding EMIT L2B Data"
    ]
  },
  {
    "objectID": "external/Finding_EMIT_L2B_Data.html#saving-lists-of-results",
    "href": "external/Finding_EMIT_L2B_Data.html#saving-lists-of-results",
    "title": "Finding EMIT L2B Data",
    "section": "5. Saving Lists of Results",
    "text": "5. Saving Lists of Results\nWe can save our lists of results URLs as a text file for later use, either to download the data, or stream it.\n\nwith open('../../data/rgb_browse_urls.txt', 'w') as f:\n    for line in rgb_urls:\n        f.write(f\"{line}\\n\")\n\n\nwith open('../../data/results_urls.txt', 'w') as f:\n    for line in min_urls:\n        f.write(f\"{line}\\n\")\n\n\nwith open('../../data/min_uncert_urls.txt', 'w') as f:\n    for line in min_unc_urls:\n        f.write(f\"{line}\\n\")",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "9. Finding EMIT L2B Data"
    ]
  },
  {
    "objectID": "external/Finding_EMIT_L2B_Data.html#streaming-or-downloading-data",
    "href": "external/Finding_EMIT_L2B_Data.html#streaming-or-downloading-data",
    "title": "Finding EMIT L2B Data",
    "section": "6. Streaming or Downloading Data",
    "text": "6. Streaming or Downloading Data\nFor the workshop, we will stream the data, but either method can be used, and each has trade-offs based on the internet speed, storage space, or use case. The EMIT files are very large due to the number of bands, so operations can take some time if streaming with a slower internet connection. Since the workshop is hosted in a Cloud workspace, we can stream the data directly to the workspace.\n\n6.1 Streaming Data Workflow\nFor an example of streaming both netCDF please see Working with EMIT L2B Mineralogy.ipynb.\nIf you plan to stream the data, you can stop here and move to the next notebook.\n\n\n6.2 Downloading Data Workflow\nTo download the scenes, we can use the earthaccess library to authenticate then download the files.\nFirst, log into Earthdata using the login function from the earthaccess library. The persist=True argument will create a local .netrc file if it doesn’t exist, or add your login info to an existing .netrc file. If no Earthdata Login credentials are found in the .netrc you’ll be prompted for them. As mentioned in section 1.2, this step is not necessary to conduct searches, but is needed to download or stream data.\nThe outputs saved in section 5 can be downloading by uncommenting and running the following cells.\n\n# # Authenticate using earthaccess\n# earthaccess.login(persist=True)\n\n\n# # Open Text File and Read Lines\n# file_list = ['../../data/rgb_browse_urls.txt','../../data/results_urls.txt']\n# urls = []\n# for file in file_list:\n#     with open(file) as f:\n#         urls.extend([line.rstrip('\\n') for line in f])\n\n\n# # Get requests https Session using Earthdata Login Info\n# fs = earthaccess.get_requests_https_session()\n# # Retrieve granule asset ID from URL (to maintain existing naming convention)\n# for url in urls:\n#     granule_asset_id = url.split('/')[-1]\n#     # Define Local Filepath\n#     fp = f'../../data/{granule_asset_id}'\n#     # Download the Granule Asset if it doesn't exist\n#     if not os.path.isfile(fp):\n#         with fs.get(url,stream=True) as src:\n#             with open(fp,'wb') as dst:\n#                 for chunk in src.iter_content(chunk_size=64*1024*1024):\n#                     dst.write(chunk)",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "9. Finding EMIT L2B Data"
    ]
  },
  {
    "objectID": "external/Finding_EMIT_L2B_Data.html#contact-info",
    "href": "external/Finding_EMIT_L2B_Data.html#contact-info",
    "title": "Finding EMIT L2B Data",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 06-28-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "9. Finding EMIT L2B Data"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "Please submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time.\n\nWe want your help! Even if you’re not a coder! There are several ways you can contribute to this repository:\n\nReport an Issue or make a recommendation\nUpdate code, documentation, notebooks, or other files (even fixing typos)\nPropose a new notebook\n\nIn the sections below we outline how to approach each of these types of contributions. If you’re new to GitHub, you can sign up here. There are a bunch of great resources on the GitHub Quickstart page. The GitHub Cheatsheet is also quite helpful, even for experienced users. Please reach out to lpdaac@usgs.gov with questions or concerns.\n\n\nIf you’ve found a problem with the repository, we want to know about it! Please submit an Issue. Before submitting, we would appreciate if you check to see if a similar issue already exists. If not, create a new issue, providing as much detail as possible. Things like screenshots and code excerpts demonstrating the problem are very helpful!\n\n\n\nTo contribute a solution to an issue or make a change to files within the repository we’ve created a typical outline of how to do that below. If you want to make a simple change, like correcting a typo within a markdown document or other documentation, there’s a great video explaining how to do that without leaving the GitHub website here. To make a more complex change to a notebook, code, or other file follow the instructions below.\n\nPlease create an Issue or comment on an existing issue describing the changes you intend to make.\n\nCreate a fork of this repository. This will create your own copy of the repository. When working from your fork, you can do whatever you want, you won’t mess up anyone else’s work so you’re safe to try things out. Worst case scenario you can delete your fork and recreate it.\n\nClone your fork to your local computer or cloud workspace using your preferred command line interface after navigating to the directory you want to place the repository in:\ngit clone your-fork-repository-url\n\nChange directories to the one you cloned\n\ncd repository-name\n\nAdd the upstream repository, this is the original repository that you want to contribute to.\n\ngit remote add upstream original-repository-url\n\nYou can use the following to view the remote repositories:\n\ngit remote -v\n\nupstream, which refers to the original repository\n\norigin, which refers to your personal fork\n\nDevelop your contribution:\n\nCreate a new branch named appropriately for the feature you want to work on:\n\ngit checkout -b new-branch-name\n\nOften, updates to an upstream repository will occur while you are developing changes on your personal fork. You can pull the latest changes from upstream\n\ngit pull upstream dev\n\nYou can check the status of your local copy of the repository to see what changes have been made using:\n\ngit status\n\nCommit locally as you progress using git add and git commit. For example, updating a readme.md file:\n\ngit add readme.md\ngit commit -m \"updated readme file\"\n\nYou can check the status of your local copy of the repository again to see what pending changes have not been added or committed using:\n\ngit status\n\nAfter making some changes, push your changes back to your fork on GitHub:\n\ngit push origin branch-name\n\nEnter username and password, depending on your settings, you may need to use a Personal access token\n\nTo submit your contribution, navigate to your forked repository GitHub page and make a pull request using the Compare &pull request green button. Make sure to select the base repository and its dev branch. Also select your forked repository as head repository and make sure compare shows your branch name. You can add your comments and press Create pull request green button. Our team will be notified and will review your suggested revisions.\n\nPlease submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time.\n\n\n\n\n\nIn the spirit of open science, we want to minimize barriers to sharing code and examples. We have added user_contributed directories to our repositories for users to share examples of their work in notebook or code form. Documentation and descriptions do not need to be as thorough as the examples we’ve created, but we ask that you provide as much as possible. Follow the instructions above, placing your new notebook or module in a suitably named directory within the user_contributed directory. Be sure to remove any large datasets and indicate where users can retrieve them.\n\n\n\nThese contributing guidelines are adapted from the NASA Transform to Open Science GitHub, available at https://github.com/nasa/Transform-to-Open-Science/blob/main/CONTRIBUTING.md.",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#report-an-issue-or-make-a-recommendation",
    "href": "CONTRIBUTING.html#report-an-issue-or-make-a-recommendation",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "If you’ve found a problem with the repository, we want to know about it! Please submit an Issue. Before submitting, we would appreciate if you check to see if a similar issue already exists. If not, create a new issue, providing as much detail as possible. Things like screenshots and code excerpts demonstrating the problem are very helpful!",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#updating-code-documentation-notebooks-or-other-files",
    "href": "CONTRIBUTING.html#updating-code-documentation-notebooks-or-other-files",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "To contribute a solution to an issue or make a change to files within the repository we’ve created a typical outline of how to do that below. If you want to make a simple change, like correcting a typo within a markdown document or other documentation, there’s a great video explaining how to do that without leaving the GitHub website here. To make a more complex change to a notebook, code, or other file follow the instructions below.\n\nPlease create an Issue or comment on an existing issue describing the changes you intend to make.\n\nCreate a fork of this repository. This will create your own copy of the repository. When working from your fork, you can do whatever you want, you won’t mess up anyone else’s work so you’re safe to try things out. Worst case scenario you can delete your fork and recreate it.\n\nClone your fork to your local computer or cloud workspace using your preferred command line interface after navigating to the directory you want to place the repository in:\ngit clone your-fork-repository-url\n\nChange directories to the one you cloned\n\ncd repository-name\n\nAdd the upstream repository, this is the original repository that you want to contribute to.\n\ngit remote add upstream original-repository-url\n\nYou can use the following to view the remote repositories:\n\ngit remote -v\n\nupstream, which refers to the original repository\n\norigin, which refers to your personal fork\n\nDevelop your contribution:\n\nCreate a new branch named appropriately for the feature you want to work on:\n\ngit checkout -b new-branch-name\n\nOften, updates to an upstream repository will occur while you are developing changes on your personal fork. You can pull the latest changes from upstream\n\ngit pull upstream dev\n\nYou can check the status of your local copy of the repository to see what changes have been made using:\n\ngit status\n\nCommit locally as you progress using git add and git commit. For example, updating a readme.md file:\n\ngit add readme.md\ngit commit -m \"updated readme file\"\n\nYou can check the status of your local copy of the repository again to see what pending changes have not been added or committed using:\n\ngit status\n\nAfter making some changes, push your changes back to your fork on GitHub:\n\ngit push origin branch-name\n\nEnter username and password, depending on your settings, you may need to use a Personal access token\n\nTo submit your contribution, navigate to your forked repository GitHub page and make a pull request using the Compare &pull request green button. Make sure to select the base repository and its dev branch. Also select your forked repository as head repository and make sure compare shows your branch name. You can add your comments and press Create pull request green button. Our team will be notified and will review your suggested revisions.\n\nPlease submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time.",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#adding-new-notebooks-or-example-workflows",
    "href": "CONTRIBUTING.html#adding-new-notebooks-or-example-workflows",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "In the spirit of open science, we want to minimize barriers to sharing code and examples. We have added user_contributed directories to our repositories for users to share examples of their work in notebook or code form. Documentation and descriptions do not need to be as thorough as the examples we’ve created, but we ask that you provide as much as possible. Follow the instructions above, placing your new notebook or module in a suitably named directory within the user_contributed directory. Be sure to remove any large datasets and indicate where users can retrieve them.",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#attribution",
    "href": "CONTRIBUTING.html#attribution",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "These contributing guidelines are adapted from the NASA Transform to Open Science GitHub, available at https://github.com/nasa/Transform-to-Open-Science/blob/main/CONTRIBUTING.md.",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CHANGE_LOG.html",
    "href": "CHANGE_LOG.html",
    "title": "Change Log",
    "section": "",
    "text": "All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. _________________________________________________________________________ ## 2024-08-08\n\n\n\nStreaming_cloud_optimized_geotiffs_using_QGIS.md and images\n\n\n\n\n\nHow to bulk download LP DAAC data using Curl to update .netrc setup instructions\n\n\n\n\n\n\n\nWeb-book resources for IGARSS Mineralogy Tutorial\n\n\n\n\n\n\n\nData_Discovery_CMR_API_Bulk_Query.ipynb tutorial\n\n\n\n\n\n\n\n\n\nThe environment set up instruction\n### Added\n\nThe YML file with versioning of Python libraries\n\n\n\n\n\n\n\n\nHow to bulk download LP DAAC data using Curl\n\nHow to bulk download LP DAAC data using wget\n\n\n\n\n\n\n\n\nCMR API Data Discovery using Request Package\n\n\n\n\n\n\n\n\nDownload Files from S3 Using boto3 Python how-to\n\nListing Objects in S3 Using to boto3 Python how-to\n### Changed\n\nUpdated the repository structure\n\n\n\n\n\n\n\n\nCHANGE_LOG.md"
  },
  {
    "objectID": "CHANGE_LOG.html#section",
    "href": "CHANGE_LOG.html#section",
    "title": "Change Log",
    "section": "",
    "text": "Web-book resources for IGARSS Mineralogy Tutorial"
  },
  {
    "objectID": "CHANGE_LOG.html#section-1",
    "href": "CHANGE_LOG.html#section-1",
    "title": "Change Log",
    "section": "",
    "text": "Data_Discovery_CMR_API_Bulk_Query.ipynb tutorial"
  },
  {
    "objectID": "CHANGE_LOG.html#section-2",
    "href": "CHANGE_LOG.html#section-2",
    "title": "Change Log",
    "section": "",
    "text": "The environment set up instruction\n### Added\n\nThe YML file with versioning of Python libraries"
  },
  {
    "objectID": "CHANGE_LOG.html#section-3",
    "href": "CHANGE_LOG.html#section-3",
    "title": "Change Log",
    "section": "",
    "text": "How to bulk download LP DAAC data using Curl\n\nHow to bulk download LP DAAC data using wget"
  },
  {
    "objectID": "CHANGE_LOG.html#section-4",
    "href": "CHANGE_LOG.html#section-4",
    "title": "Change Log",
    "section": "",
    "text": "CMR API Data Discovery using Request Package"
  },
  {
    "objectID": "CHANGE_LOG.html#section-5",
    "href": "CHANGE_LOG.html#section-5",
    "title": "Change Log",
    "section": "",
    "text": "Download Files from S3 Using boto3 Python how-to\n\nListing Objects in S3 Using to boto3 Python how-to\n### Changed\n\nUpdated the repository structure"
  },
  {
    "objectID": "CHANGE_LOG.html#section-6",
    "href": "CHANGE_LOG.html#section-6",
    "title": "Change Log",
    "section": "",
    "text": "CHANGE_LOG.md"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at LPDAAC@usgs.gov. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the [Zarr Developers][Github], available at [https://github.com/zarr-developers/.github/blob/main/CODE_OF_CONDUCT.md] and from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.",
    "crumbs": [
      "Contributing",
      "Contributor Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.",
    "crumbs": [
      "Contributing",
      "Contributor Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting",
    "crumbs": [
      "Contributing",
      "Contributor Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.",
    "crumbs": [
      "Contributing",
      "Contributor Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.",
    "crumbs": [
      "Contributing",
      "Contributor Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at LPDAAC@usgs.gov. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.",
    "crumbs": [
      "Contributing",
      "Contributor Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.",
    "crumbs": [
      "Contributing",
      "Contributor Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the [Zarr Developers][Github], available at [https://github.com/zarr-developers/.github/blob/main/CODE_OF_CONDUCT.md] and from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.",
    "crumbs": [
      "Contributing",
      "Contributor Code of Conduct"
    ]
  },
  {
    "objectID": "external/Exploring_EMIT_L2A_Reflectance.html",
    "href": "external/Exploring_EMIT_L2A_Reflectance.html",
    "title": "Exploring L2A Reflectance",
    "section": "",
    "text": "This notebook is from EMIT-Data-Resources\n\n\nSource: Exploring EMIT L2A Reflectance\n\nImported on: 2024-11-07",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "2. Exploring EMIT L2A Reflectance"
    ]
  },
  {
    "objectID": "external/Exploring_EMIT_L2A_Reflectance.html#setup",
    "href": "external/Exploring_EMIT_L2A_Reflectance.html#setup",
    "title": "Exploring L2A Reflectance",
    "section": "1.1 Setup",
    "text": "1.1 Setup\nImport the required Python libraries.\n\nimport earthaccess\nimport os\nimport warnings\nimport csv\nfrom osgeo import gdal\nimport numpy as np\nimport math\nimport rasterio as rio\nimport xarray as xr\nimport holoviews as hv\nimport hvplot.xarray\nimport netCDF4 as nc\n\n# This will ignore some warnings caused by holoviews\nwarnings.simplefilter('ignore') \n\nLogin to your NASA Earthdata account and create a .netrc file using the login function from the earthaccess library. If you do not have an Earthdata Account, you can create one here.\n\nearthaccess.login(persist=True)\n\nFor this notebook we will download the files necessary using earthaccess. You can also access the data in place or stream it, but this can slow due to the file sizes. Provide a URL for an EMIT L2A Reflectance granule.\n\nurl = 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20220903T163129_2224611_012/EMIT_L2A_RFL_001_20220903T163129_2224611_012.nc'\n\nGet an HTTPS Session using your earthdata login, set a local path to save the file, and download the granule asset - This may take a while, the reflectance file is approximately 1.8 GB.\n\n# Get requests https Session using Earthdata Login Info\nfs = earthaccess.get_requests_https_session()\n# Retrieve granule asset ID from URL (to maintain existing naming convention)\ngranule_asset_id = url.split('/')[-1]\n# Define Local Filepath\nfp = f'../../data/{granule_asset_id}'\n# Download the Granule Asset if it doesn't exist\nif not os.path.isfile(fp):\n    with fs.get(url,stream=True) as src:\n        with open(fp,'wb') as dst:\n            for chunk in src.iter_content(chunk_size=64*1024*1024):\n                dst.write(chunk)",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "2. Exploring EMIT L2A Reflectance"
    ]
  },
  {
    "objectID": "external/Exploring_EMIT_L2A_Reflectance.html#opening-emit-data",
    "href": "external/Exploring_EMIT_L2A_Reflectance.html#opening-emit-data",
    "title": "Exploring L2A Reflectance",
    "section": "1.2 Opening EMIT Data",
    "text": "1.2 Opening EMIT Data\nEMIT L2A Reflectance Data are distributed in a non-orthocorrected spatially raw NetCDF4 (.nc) format consisting of the data and its associated metadata. Inside the L2A Reflectance .nc file there are 3 groups. Groups can be thought of as containers to organize the data.\n\nThe root group that can be considered the main dataset contains the reflectance data described by the downtrack, crosstrack, and bands dimensions.\n\nThe sensor_band_parameters group containing the wavelength center and the full-width half maximum (FWHM) of each band.\n\nThe location group contains latitude and longitude values at the center of each pixel described by the crosstrack and downtrack dimensions, as well as a geometry lookup table (GLT) described by the ortho_x and ortho_y dimensions. The GLT is an orthorectified image (EPSG:4326) consisting of 2 layers containing downtrack and crosstrack indices. These index positions allow us to quickly project the raw data onto this geographic grid.\n\nTo access the .nc file we will use the netCDF4 and xarray libraries. The netCDF4 library will be used to explore thee data structure, then we will use xarray to work with the data. xarray is a python package for working with labelled multi-dimensional arrays. It provides a data model where data, dimensions, and attributes together in an easily interpretable way.\n\nds_nc = nc.Dataset(fp)\nds_nc\n\n\nds_nc['location']\n\nFrom this output, we can see the reflectance variable, and the sensor_band_parameters and location groups. We can also see the dimensions, their sizes, and file metadata.\nNow that we have a better understanding of the structure of the file, read the EMIT data as an xarray.Dataset and preview it.\n\nds = xr.open_dataset(fp)\nds\n\nThis xarray dataset only contains the reflectance variable and attributes metadata, not the data from the other groups in the file. This is because xarray only supports reading non-hierarchical (flat) datasets, meaning that when loading a NetCDF into an xarray.Dataset, only the root group is added. The other groups will have to be read into xarray separately. We can list them using the netCDF4 library to get the group names, then use that to add them to new xarray datasets.\n\nds_nc.groups.keys()\n\nNow that we know the other group names, read the sensor_band_parameters and location groups into their own xarray datasets.\n\nwvl = xr.open_dataset(fp,group='sensor_band_parameters')\nwvl\n\n\nloc = xr.open_dataset(fp,group='location')\nloc\n\nWe could merge all 3 datasets, but since sensor_band_parameters and location describe various aspects of the reflectance variable we can simply add them as coordinates, along with a downtrack and crosstrack dimension to describe the reflectance data array. This will allow us to utilize some additional features of xarray.\n\n# Create coordinates and an index for the downtrack and crosstrack dimensions, then unpack the variables from the wvl and loc datasets and set them as coordinates for ds\nds = ds.assign_coords({'downtrack':(['downtrack'], ds.downtrack.data),'crosstrack':(['crosstrack'],ds.crosstrack.data), **wvl.variables, **loc.variables})\nds\n\nAnother step we can take is to swap the ‘bands’ dimension with wavelengths. Doing this will allow us to index based on the wavelength of the band, and remove ‘bands’ as a dimension. We can do this since bands is just a 3rd dimension that will is defined based on the ‘sensor_band_parameters’ group (i.e. ‘wavelengths’ for reflectance, or ‘mask_bands’ for mask data).\n\nds = ds.swap_dims({'bands':'wavelengths'})\nds\n\nNow we have an xarray.Dataset containing all of the information from EMIT netCDF file. Since these datasets are large, we can go ahead and delete objects we won’t be using to conserve memory.\n\ndel wvl\ndel loc",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "2. Exploring EMIT L2A Reflectance"
    ]
  },
  {
    "objectID": "external/Exploring_EMIT_L2A_Reflectance.html#visualizing-spectra---non-orthorectified",
    "href": "external/Exploring_EMIT_L2A_Reflectance.html#visualizing-spectra---non-orthorectified",
    "title": "Exploring L2A Reflectance",
    "section": "1.3 Visualizing Spectra - Non-Orthorectified",
    "text": "1.3 Visualizing Spectra - Non-Orthorectified\nPick a random downtrack and crosstrack location. Here we chose 660, 370 (downtrack,crosstrack). Next use the sel() function from xarray and the hvplot.line() functions to first select the spatial position and then plot a line showing the reflectance at that location.\n\nexample = ds['reflectance'].sel(downtrack=660,crosstrack=370)\nexample.hvplot.line(y='reflectance',x='wavelengths', color='black', frame_height=400, frame_width=600)\n\nWe can see some flat regions in the spectral curve around 1320-1440 nm and 1770-1970 nm. These are where water absoption features in these regions were removed. Typically this data is noisy due to the moisture present in the atmosphere; therefore, these spectral regions offer little information about targets and can be excluded from calculations.\nWe can set reflectance values where the good_wavelenghts is 0 (these will have a reflectance of -0.1) to np.nan do mask them out and improve visualization.\n\nds['reflectance'].data[:,:,ds['good_wavelengths'].data==0] = np.nan\n\nPlot the filtered reflectance values using the same downtrack and crosstrack position as above.\n\nds['reflectance'].sel(downtrack=660,crosstrack=370).hvplot.line(y='reflectance',x='wavelengths', color='black', frame_height=400, frame_width=600)\n\nWithout these data we can better interpret the spectral curve and hvplot will do a better job automatically scaling our axes.\nWe can also plot the data spatially. Since we changed our dimension and index to wavelengths we can use the sel() function to spectrally subset to the wavelength nearest to 850nm in the NIR, then plot the data spatially using hvplot.image() to view the reflectance at 850nm of each pixel across the acquired region.\n\nrefl850 = ds.sel(wavelengths=850, method='nearest')\n\n\nrefl850.hvplot.image(cmap='viridis', aspect = 'equal', frame_width=720).opts(title=f\"{refl850.wavelengths.values:.3f} {refl850.wavelengths.units}\")",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "2. Exploring EMIT L2A Reflectance"
    ]
  },
  {
    "objectID": "external/Exploring_EMIT_L2A_Reflectance.html#orthorectification",
    "href": "external/Exploring_EMIT_L2A_Reflectance.html#orthorectification",
    "title": "Exploring L2A Reflectance",
    "section": "1.4 Orthorectification",
    "text": "1.4 Orthorectification\nThe ‘real’ orthorectifation process has already been done for EMIT data. Here we are using the crosstrack and downtrack indices contained in the GLT to place our spatially raw reflectance data a into geographic grid with the ortho_x and ortho_y dimensions. As previously mentioned a Geometry Lookup Table (GLT) is included in the location group of the netCDF4 file. Applying the GLT will orthorectify the data and give us Latitude and Longitude positional information.\nBefore using the GLT to orthorectify the data, examine the location group from the dataset by reading it into xarray.\n\nloc = xr.open_dataset(fp,group='location')\nloc\n\nWe can see that each downtrack and crosstrack position has a latitude, longitude, and elevation, and the ortho_x and ortho_y data make up glt_x and glt_y arrays with a different shape. These arrays contain crosstrack and downtrack index values to quickly reproject the data. We will use these indexes to build an array of 2009x2353x285 (lat,lon,bands), filling it with the data from the reflectance dataset.\nGo ahead and remove this dataset. We will use a function in the provided emit_tools module to orthorectify the data and place it into an xarray.Dataset.\n\ndel loc\ndel example\n\nImport the emit_tools module and call use the help function to see how it can be used.\n\nNote: This function currently works with L1B Radiance and L2A Reflectance Data.\n\n\nimport sys\nsys.path.append('../modules/')\nfrom emit_tools import emit_xarray\nhelp(emit_xarray)\n\nWe can see that the emit_xarray function will automatically apply the GLT to orthorectify the data unless ortho  = False. The function will also apply masks if desired during construction of the output xarray.Dataset. EMIT L2A Masks files provides a quality mask and a band_mask indicating if values were interpolated. For more about masking, see the How_to_use_EMIT_Quality_data.ipynb.\nUse the emit_xarray function to read in and orthorectify the L2A reflectance data.\n\nFor a detailed walkthrough of the orthorectification process using the GLT see section 2 of the How_to_Orthorectify.ipynb in the how-tos folder.\n\n\nds_geo = emit_xarray(fp, ortho=True)\nds_geo\n\n\nnon_ortho_fig = ds.sel(wavelengths=850, method='nearest').hvplot.image(cmap='Viridis', aspect = 'equal',frame_height=600).opts(\n         title=f\"Reflectance at {refl850.wavelengths.values:.3f} {refl850.wavelengths.units}\")\n\nWhen we orthorectify the scene, locations in the grid without data will be filled with the default fill value of -9999. To improve visualizations we can assign these locations to np.nan to mask them (make them transparent).\n\nds_geo.reflectance.data[ds_geo.reflectance.data == -9999] = np.nan\n\n\nds_geo.sel(wavelengths=850, method='nearest').hvplot.image(cmap='Viridis', geo=True, tiles='ESRI', alpha=0.8, frame_height=600).opts(\n    title=f\"Reflectance at {refl850.wavelengths.values:.3f} {refl850.wavelengths.units} (Orthorectified)\")",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "2. Exploring EMIT L2A Reflectance"
    ]
  },
  {
    "objectID": "external/Exploring_EMIT_L2A_Reflectance.html#plotting-data---orthorectified",
    "href": "external/Exploring_EMIT_L2A_Reflectance.html#plotting-data---orthorectified",
    "title": "Exploring L2A Reflectance",
    "section": "1.5 Plotting Data - Orthorectified",
    "text": "1.5 Plotting Data - Orthorectified\nNow that the data has been orthorectified, plot the georeferenced dataset using the same single wavelength (850nm) as above alongside the uncorrected image. We an also plot the orthorectified data against an imagery tile using the geo=True and tiles= parameters instead of aspect='equal'. Any tile source available in geoviews should work here. This will change the axis names, but that can be fixed by adding them manually in the opts, like below.\n\n(ds_geo.sel(wavelengths=850, method='nearest').hvplot.image(cmap='Viridis', geo=True, tiles='ESRI', alpha=0.8, frame_height=600).opts(\n    title=f\"Reflectance at {refl850.wavelengths.values:.3f} {refl850.wavelengths.units} (Orthorectified)\") +\\\n     ds.sel(wavelengths=850, method='nearest').hvplot.image(cmap='Viridis', aspect = 'equal',frame_height=600).opts(\n         title=f\"Reflectance at {refl850.wavelengths.values:.3f} {refl850.wavelengths.units}\"))\n\nWe can see that the orthorectification step placed the data on a geogrpahic grid that matches pretty well with ESRI tiles. Now that we have a better idea of what the target area looks like, we can also plot the spectra using the georeferenced data. First, filter out the water absorption bands like we did earlier. By limiting the third dimension of the array to good_wavelengths.\n\nds_geo['reflectance'].data[:,:,ds_geo['good_wavelengths'].data==0] = np.nan\n\nNow, plot the spectra at the Lat/Lon coordinates provided below.\n\npoint = ds_geo.sel(longitude=-61.833,latitude=-39.710,method='nearest')\npoint.hvplot.line(y='reflectance',x='wavelengths', color='black', frame_height=400, frame_width=600).opts(\n    title = f'Latitude = {point.latitude.values.round(3)}, Longitude = {point.longitude.values.round(3)}')",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "2. Exploring EMIT L2A Reflectance"
    ]
  },
  {
    "objectID": "external/Exploring_EMIT_L2A_Reflectance.html#writing-an-orthorectified-output",
    "href": "external/Exploring_EMIT_L2A_Reflectance.html#writing-an-orthorectified-output",
    "title": "Exploring L2A Reflectance",
    "section": "1.6 Writing an Orthorectified Output",
    "text": "1.6 Writing an Orthorectified Output\nAt this point, the ds_geo orthorectified EMIT data can also be written as a flattened netCDF4 output that can be read using the xarray.open_dataset function, if desired. Before doing this, we can transpose the dimension order so the bands are the first dimension so that the data is readable by software like QGIS. This file will be larger than the original EMIT granule since it has been orthorectified. If we do write an output file, the format will be such that it can be read in using xarray.\nTranspose the dimensions order and write an output.\n\n# Transpose dimensions\n# ds_geo = ds_geo.transpose('wavelengths','latitude','longitude')\n# ds_geo.to_netcdf('../../data/geo_ds_out.nc')\n\n# Example for Opening \n# ds = xr.open_dataset('../data/geo_ds_out.nc')",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "2. Exploring EMIT L2A Reflectance"
    ]
  },
  {
    "objectID": "external/Exploring_EMIT_L2A_Reflectance.html#interactive-spatial-and-spectral-plots",
    "href": "external/Exploring_EMIT_L2A_Reflectance.html#interactive-spatial-and-spectral-plots",
    "title": "Exploring L2A Reflectance",
    "section": "1.7 Interactive Spatial and Spectral Plots",
    "text": "1.7 Interactive Spatial and Spectral Plots\nCombining the Spatial and Spectral information into a single visualization can be a powerful tool for exploring and inspecting imaging spectroscopy data. Using the streams module from Holoviews we can link a spatial map to a plot of spectra.\nWe could plot a single band image as we previously have, but using a multiband image, like an RGB may help infer what targets we’re examining. Build an RGB image following the steps below.\nSelect bands to represent red (650 nm), green (560 nm), and blue (470 nm) by finding the nearest to a wavelength chosen to represent that color.\n\nNote that if subsetting by bands like this example, it is more memory efficient to subset before orthorectifying. Instead of using ortho=True in the emit_xarray function, select bands first, then apply the orthorectification using the ortho_xr function from emit_tools.py (requires a separate import).\n\n\nrgb = ds_geo.sel(wavelengths=[650, 560, 470], method='nearest')\nrgb\n\nNext, write a function to scale the values using a gamma correction. Without applying this scaling the majority of the image would be very dark, with the reflectance data being skewed by the few pixels with very high reflectance. &gt; Note: This has no impact on analysis or data, just visualizing the RGB map.\n\n# Function to adjust gamma across all bands - adjust brightness\ndef gamma_adjust(rgb_ds, bright=0.2, white_background=False):\n    array = rgb_ds.reflectance.data\n    gamma = math.log(bright)/math.log(np.nanmean(array)) # Create exponent for gamma scaling - can be adjusted by changing 0.2 \n    scaled = np.power(array,gamma).clip(0,1) # Apply scaling and clip to 0-1 range\n    if white_background == True:\n        scaled = np.nan_to_num(scaled, nan = 1) # Assign NA's to 1 so they appear white in plots\n    rgb_ds.reflectance.data = scaled\n    return rgb_ds\n\n\nrgb = gamma_adjust(rgb, white_background=True)\n\nNow that we have an RGB dataset, use it to build our spatial plot.\n\nmap = rgb.hvplot.rgb(x='longitude', y='latitude', bands='wavelengths', aspect = 'equal', frame_height=500)\n\nTo visualize the spectral and spatial data side-by-side, we use the Point Draw tool from the holoviews library.\nDefine a limit to the quantity of points and spectra we will plot, a list of colors to cycle through, and an initial point. We use the input from the PointerXYto show the spectra where our mouse cursor is. Then use the input from the Tap function to provide clicked x and y positions on the map. These retrieve spectra from the dataset at those coordinates.\nClick in the RGB image to add spectra to the plot. You can also click and hold the mouse button then drag previously place points. To remove a point click and hold the mouse button down, then press the backspace key.\n\n# Set Point Limit\nPOINT_LIMIT = 10\n\n# Set up  Color Cycling\ncolor_cycle = hv.Cycle('Category20')\ncolors = [color_cycle[i] for i in range(5)]\n\n# Get center coordinates of image\nxmid = ds_geo.longitude.values[int(len(ds_geo.longitude) / 2)]\nymid = ds_geo.latitude.values[int(len(ds_geo.latitude) / 2)]\n\n#\nfirst_point = ([xmid], [ymid], [0])\npoints = hv.Points(first_point, vdims='id')\npoints_stream = hv.streams.PointDraw(\n    data=points.columns(),\n    source=points,\n    drag=True,\n    num_objects=POINT_LIMIT,\n    styles={'fill_color': color_cycle.values[1:POINT_LIMIT+1], 'line_color': 'gray'}\n)\n\nposxy = hv.streams.PointerXY(source=map, x=xmid, y=ymid)\nclickxy = hv.streams.Tap(source=map, x=xmid, y=ymid)\n\n# Function to build spectral plot of clicked location to show on hover stream plot\ndef click_spectra(data):\n    coordinates = []\n    if data is None or not any(len(d) for d in data.values()):\n        coordinates.append(clicked_points[0][0], clicked_points[1][0])\n    else:\n        coordinates = [c for c in zip(data['x'], data['y'])]\n    \n    plots = []\n    for i, coords in enumerate(coordinates):\n        x, y = coords\n        data = ds_geo.sel(longitude=x, latitude=y, method=\"nearest\")\n        plots.append(\n            data.hvplot.line(\n                y=\"reflectance\",\n                x=\"wavelengths\",\n                color=color_cycle,\n                label=f\"{i}\"\n            )\n        )\n        points_stream.data[\"id\"][i] = i\n    return hv.Overlay(plots)\n\ndef hover_spectra(x,y):\n    return ds_geo.sel(longitude=x,latitude=y,method='nearest').hvplot.line(y='reflectance',x='wavelengths',\n                                                                           color='black', frame_width=400)\n# Define the Dynamic Maps\nclick_dmap = hv.DynamicMap(click_spectra, streams=[points_stream])\nhover_dmap = hv.DynamicMap(hover_spectra, streams=[posxy])\n\n# Plot the Map and Dynamic Map side by side\nhv.Layout(hover_dmap*click_dmap + map * points).cols(2).opts(\n    hv.opts.Points(active_tools=['point_draw'], size=10, tools=['hover'], color='white', line_color='gray'),\n    hv.opts.Overlay(show_legend=False, show_title=False, fontscale=1.5, frame_height=480)\n)\n\nAfter selecting a number of points we can build a dictionary of points and spectra, then export the spectra to a .csv file.\n\ndata = points_stream.data\nwavelengths = ds_geo.wavelengths.values\n\nrows = [[\"id\", \"x\", \"y\"] + [str(i) for i in wavelengths]]\n \nfor p in zip(data['x'], data['y'], data['id']):\n    x, y, i = p\n    spectra = ds_geo.sel(longitude=x, latitude=y, method=\"nearest\").reflectance.values\n    row = [i, x, y] + list(spectra)\n    rows.append(row)\n\n\nwith open('../../data/interactive_plot_data.csv', 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerows(rows)",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "2. Exploring EMIT L2A Reflectance"
    ]
  },
  {
    "objectID": "external/Exploring_EMIT_L2A_Reflectance.html#contact-info",
    "href": "external/Exploring_EMIT_L2A_Reflectance.html#contact-info",
    "title": "Exploring L2A Reflectance",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 11-27-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "2. Exploring EMIT L2A Reflectance"
    ]
  },
  {
    "objectID": "external/Generating_Methane_Spectral_Fingerprint.html",
    "href": "external/Generating_Methane_Spectral_Fingerprint.html",
    "title": "Generating Methane Spectral Fingerprint",
    "section": "",
    "text": "This notebook is from EMIT-Data-Resources\n\n\nSource: Generating Methane Spectral Fingerprint\n\nImported on: 2024-11-07",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "7. Generating Methane Spectral Fingerprint"
    ]
  },
  {
    "objectID": "external/Generating_Methane_Spectral_Fingerprint.html#setup",
    "href": "external/Generating_Methane_Spectral_Fingerprint.html#setup",
    "title": "Generating Methane Spectral Fingerprint",
    "section": "1. Setup",
    "text": "1. Setup\nImport the necessary Python libraries.\n\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom osgeo import gdal\nimport earthaccess\nimport rasterio as rio\nimport rioxarray as rxr\nimport holoviews as hv\nimport hvplot\nimport hvplot.xarray\nimport hvplot.pandas\nfrom skimage import exposure\n\nsys.path.append('../modules/')\nfrom emit_tools import emit_xarray\n\nDownload the EMIT L1B Radiance and L2B Methane Enhancement products for the scene we’re going to look at.\n\nurls = ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL1BRAD.001/EMIT_L1B_RAD_001_20220815T042838_2222703_003/EMIT_L1B_RAD_001_20220815T042838_2222703_003.nc',\n        'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2BCH4ENH.001/EMIT_L2B_CH4ENH_001_20220815T042838_2222703_003/EMIT_L2B_CH4ENH_001_20220815T042838_2222703_003.tif']\n\n# Authenticate and create an https session\nearthaccess.login(persist=True)\nfs = earthaccess.get_requests_https_session()\n\nfor url in urls:\n# Retrieve granule asset ID from URL (to maintain existing naming convention)\n    granule_asset_id = url.split('/')[-1]\n    # Define Local Filepath\n    fp = f'../../data/{granule_asset_id}'\n    # Download the Granule Asset if it doesn't exist\n    if not os.path.isfile(fp):\n        with fs.get(url,stream=True) as src:\n            with open(fp,'wb') as dst:\n                for chunk in src.iter_content(chunk_size=64*1024*1024):\n                    dst.write(chunk)\n\nSet the filepath for the radiance and methane enhancement files.\n\nrad_fp = '../../data/EMIT_L1B_RAD_001_20220815T042838_2222703_003.nc'\nenh_fp = '../../data/EMIT_L2B_CH4ENH_001_20220815T042838_2222703_003.tif'",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "7. Generating Methane Spectral Fingerprint"
    ]
  },
  {
    "objectID": "external/Generating_Methane_Spectral_Fingerprint.html#opening-emit-data",
    "href": "external/Generating_Methane_Spectral_Fingerprint.html#opening-emit-data",
    "title": "Generating Methane Spectral Fingerprint",
    "section": "2. Opening EMIT Data",
    "text": "2. Opening EMIT Data\nThe EMIT L1B At-Sensor Radiance data is distributed in a non-orthorectified spatially raw netCDF4 (.nc) format consisting of the data and its associated metadata. Inside the L1B file, there are 3 groups.\n\nThe root group that can be considered the main dataset contains the radiance data described by the downtrack, crosstrack, and bands dimensions.\n\nThe sensor_band_parameters group containing the wavelength center and the full-width half maximum (FWHM) of each band.\n\nThe location group contains latitude and longitude values at the center of each pixel described by the crosstrack and downtrack dimensions, as well as a geometry lookup table (GLT) described by the ortho_x and ortho_y dimensions. The GLT is an orthorectified image (EPSG:4326) consisting of 2 layers containing downtrack and crosstrack indices. These index positions allow us to quickly project the raw data onto this geographic grid.\n\nThis data can be opened using the netCDF4 and xarray libraries, or utilizing functions within the emit_tools.py module to organize them into a flattened (no groups) xarray.Dataset object. For this notebook, we will use functions from emit_tools.py to simplify working with the data. For more about the structure and use of netCDF4 and xarray please see the Exploring EMIT L2A Reflectance Jupyter Notebook.\nOpen the radiance file using the emit_xarray function from the emit_tools.py module and orthorecitify it.\n\nrad = emit_xarray(rad_fp, ortho=True)\nrad\n\nThe EMIT L2B Methane Enhancement Data represent an enhancement above background methane concentration for a 1 meter layer in parts-per-million (ppm) meter (m). These units are used rather than ppm because we are unable to measure the vertical extent of plumes. This data is distributed as a single band cloud-optimized geotiff (COG) and it has been orthocorrected. We can open this using the rioxarray library to place the data in an xarray.DataArray object.\nOpen the methane enhancement geotiff file using rioxarray and squeeze the band dimension to remove the extra dimension so our array will only have 2 dimensions.\n\nenh = rxr.open_rasterio(enh_fp).squeeze('band',drop=True) \nenh",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "7. Generating Methane Spectral Fingerprint"
    ]
  },
  {
    "objectID": "external/Generating_Methane_Spectral_Fingerprint.html#extracting-point-data",
    "href": "external/Generating_Methane_Spectral_Fingerprint.html#extracting-point-data",
    "title": "Generating Methane Spectral Fingerprint",
    "section": "3. Extracting Point Data ",
    "text": "3. Extracting Point Data \nOpen the .csv file included in the /data/ directory as a pandas.DataFrame. This file contains the latitude and longitude coordinates for three points of interest. Two that are outside of a methane plume, and one that is inside.\n\n# Define our Points for In-plume and out-of-plume\npoints = pd.read_csv('../../data/methane_tutorial/methane_inout_points.csv')\npoints\n\nSet the index as the ID column.\n\npoints = points.set_index(['ID'])\npoints\n\nNow we can use the sel function from xarray to extract the radiance data at each point in our dataframe.\n\n# Extract target spectra from dataset\npoint_ds = rad.sel(latitude=points.to_xarray().latitude, longitude=points.to_xarray().longitude, method='nearest')\n\nAfter extracting, we can convert the data to a pandas.DataFrame and join it with our ‘in-plume’ column from the original dataframe.\n\npoint_df = point_ds.to_dataframe().join(points['in-plume'],on=['ID'])\npoint_df\n\nAt this point, we can save the data to a .csv file for future use.\n\n# point_df.to_csv('../../data/methane_tutorial/point_df.csv')\n\nNext, visualize these points on an rgb image generated from the radiance file, and the methane enhancement data, just to get a better idea of where these points are located.\nTo do this, first create an RGB data array from the radiance file using the sel function to select the bands nearest to the desired wavelengths.\n\n# Create an RGB from Radiance\nrgb = rad.sel(wavelengths=[650,560,470], method='nearest')\n\nNext, use a function to rescale the brightness, so this image is easier to see.\n\nrgb.radiance.data[rgb.radiance.data == -9999] = 0\nrgb.radiance.data = exposure.rescale_intensity(rgb.radiance.data, in_range='image', out_range=(0,1))\n\nNow that we have the necessary pieces to visualize we can make some spatial visualizations using hvplot. For the enhancement data, set the fill value of -9999 to np.nan to make it transparent.\n\n# Create RGB Plot\nrgb_map = rgb.hvplot.rgb(x='longitude',y='latitude',bands='wavelengths',title='RGB Radiance', geo=True, crs='EPSG:4326')\n\n\n# Create Methane Enhancement Plot\nenh.data[enh.data == -9999] = np.nan\nmethane_map = enh.hvplot.image(x='x',y='y',cmap='viridis', geo=True, crs='EPSG:4326', clim=(0,enh.data.max()), title='Methane Enhancement', clabel='ppm m', xlabel='Longitude', ylabel='Latitude')\n\n\npoint_map = point_df.hvplot.points(x='longitude',y='latitude', color='in-plume', cmap='HighContrast', geo=True, crs='EPSG:4326', hover=False, colorbar=False)\n\nWe can combine these plots with an * operator to overlay them on the same plot. This does require that all are in the same CRS to display properly.\n\n(rgb_map*point_map) + (methane_map*point_map)\n\nNow lets look at the spectra.\n\npoint_df.hvplot.line(x='wavelengths',y='radiance', by=['ID'], color=hv.Cycle('Dark2'), frame_height=400, frame_width=600, title = 'Radiance Spectra, ID 0 is in-plume' , xlabel='Wavelength (nm)', ylabel='Radiance (W/m^2/sr/nm)')",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "7. Generating Methane Spectral Fingerprint"
    ]
  },
  {
    "objectID": "external/Generating_Methane_Spectral_Fingerprint.html#methane-spectral-signature",
    "href": "external/Generating_Methane_Spectral_Fingerprint.html#methane-spectral-signature",
    "title": "Generating Methane Spectral Fingerprint",
    "section": "4. Methane Spectral Signature",
    "text": "4. Methane Spectral Signature\nLet’s open a file containing the modeled methane signature and visualize it. This is the spectral fingerprint that we are looking for in EMIT data to identify methane plumes. Open our absorption coefficient file using pandas, add some column names and set an index.\n\n# Open file \nch4_ac = pd.read_csv('../../data/methane_tutorial/emit20220815t042838_ch4_target', sep='\\s+', header=None)\n# Add Column Names\nch4_ac.columns = ['index','wavelength','value']\n# Set Index\nch4_ac.set_index('index', inplace=True)\nch4_ac\n\nCreate a figure for the absorption coefficient.\n\nac_plot = ch4_ac.hvplot(x='wavelength',y='value', frame_height=400, frame_width=400, line_color='black', line_width=2, xlim=(2150,2450), ylim=(-1.5,0), xlabel='Wavelength (nm)', title='Methane Absorption Coefficient', ylabel='')\n\n\nac_plot\n\nWe can visualize a similar curve by creating a band ratio of in-plume to out-of-plume spectra. dividing the radiance for a pixel with methane by radiance for a pixel outside the plume to generate a diagnostic spectral fingerprint. This spectral fingerprint is confirmation that EMIT is observing methane enhancements with characteristic methane absorption features, which agree well with the modeled methane signature.\nThis agreement is strong in the example we selected due to the similarity of the spectra without the contribution of methane.\nTo create the band ratio, first separate our data into in-plume and out-of-plume dataframes.\n\nin_plume = point_df.loc[point_df['in-plume'] == 1].copy()\nout_plume = point_df.loc[point_df['in-plume'] == 0].copy()\n\nNext, add a column for the in/out band ratio using the in-plume divided by out-of-plume radiance.\n\nout_plume['band_ratio'] = (in_plume.loc[0,'radiance']/out_plume['radiance'])\n\nCreate an hvplot object for our band ratio.\n\nin_out_plot = out_plume.hvplot(x='wavelengths',y='band_ratio', by=['ID'], color=hv.Cycle('Dark2'), frame_height=400, frame_width=400, xlim=(2150,2450), ylim=(0.85,1.05), ylabel='In Plume/Out of Plume Ratio', xlabel='Wavelength (nm)', title='In Plume/Out of Plume Ratio')\n\nOverlay our absorption coefficient to show similarity between the two within the 2150 and 2450 nanometers (nm) range where methane spectral features are present. We can do this by setting our figure xlim.\n\nfrom bokeh.models import GlyphRenderer, LinearAxis, LinearScale, Range1d\n\ndef overlay_hook(plot, element):\n    # Adds right y-axis\n    p = plot.handles[\"plot\"]\n    p.extra_y_scales = {\"right\": LinearScale()}\n    p.extra_y_ranges = {\"right\": Range1d(-1.5,0)}\n    p.add_layout(LinearAxis(y_range_name=\"right\"), \"right\")\n\n   # find the last line and set it to right\n    lines = [p for p in p.renderers if isinstance(p, GlyphRenderer)]\n    lines[-1].y_range_name = \"right\"\n\n# Create Figure\n(in_out_plot.opts(ylim=(0.85,0.95)) * ac_plot.opts(color=\"k\")).opts(hooks=[overlay_hook]).opts(title='In Plume/Out of Plume and Absorption Coefficient') \n\nWe can contrast this with our out-of-plume/out-of-plume band ratios to show how similar the in-plume and out-of-plume ratio is to this spectral signature. Create a column in our dataframe for the out-of-plume/out-of-plume band ratio and visualize it overlayed with our methane absorption feature using hvplot.\n\nout_plume['out-out'] = (out_plume['radiance'] / out_plume.loc[2,'radiance'])\n\n\nout_out_plot = out_plume.hvplot(x='wavelengths',y='out-out', by=['ID'], color=hv.Cycle('Dark2'), frame_height=400, frame_width=400, xlim=(2150,2450), ylim=(0.85,1.05), ylabel='Out/Out 2 Ratio', xlabel='Wavelength (nm)', title='Out of Plume/Out of Plume 2 Ratio')\n\n\nfrom bokeh.models import GlyphRenderer, LinearAxis, LinearScale, Range1d\n\ndef overlay_hook(plot, element):\n    # Adds right y-axis\n    p = plot.handles[\"plot\"]\n    p.extra_y_scales = {\"right\": LinearScale()}\n    p.extra_y_ranges = {\"right\": Range1d(-1.5,0)}\n    p.add_layout(LinearAxis(y_range_name=\"right\"), \"right\")\n\n   # find the last line and set it to right\n    lines = [p for p in p.renderers if isinstance(p, GlyphRenderer)]\n    lines[-1].y_range_name = \"right\"\n\n# Create Figure\n(out_out_plot * ac_plot.opts(color=\"k\")).opts(hooks=[overlay_hook]).opts(title='Out of Plume/Out of Plume and Absorption Coefficient') \n\nThis highlights the similarity of the in-plume/out-of-plume band ratio to the modeled methane signature, as opposed to a band ratio where methane is not present.",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "7. Generating Methane Spectral Fingerprint"
    ]
  },
  {
    "objectID": "external/Generating_Methane_Spectral_Fingerprint.html#contact-info",
    "href": "external/Generating_Methane_Spectral_Fingerprint.html#contact-info",
    "title": "Generating Methane Spectral Fingerprint",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 03-13-2024\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "7. Generating Methane Spectral Fingerprint"
    ]
  },
  {
    "objectID": "external/How_to_Extract_Area.html",
    "href": "external/How_to_Extract_Area.html",
    "title": "How to: Extracting EMIT Spectra using a Shapefile/GeoJSON",
    "section": "",
    "text": "This notebook is from EMIT-Data-Resources\n\n\nSource: How to Extract Area\n\nImported on: 2024-11-07",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "5. Extracting Area"
    ]
  },
  {
    "objectID": "external/How_to_Extract_Area.html#contact-info",
    "href": "external/How_to_Extract_Area.html#contact-info",
    "title": "How to: Extracting EMIT Spectra using a Shapefile/GeoJSON",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 03-13-2024\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "5. Extracting Area"
    ]
  },
  {
    "objectID": "external/How_to_find_and_access_EMIT_data.html",
    "href": "external/How_to_find_and_access_EMIT_data.html",
    "title": "How to: Find and Access EMIT Data",
    "section": "",
    "text": "This notebook is from EMIT-Data-Resources\n\n\nSource: How to Find and Access EMIT Data\n\nImported on: 2024-11-07",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "1. Finding and Accessing EMIT Data"
    ]
  },
  {
    "objectID": "external/How_to_find_and_access_EMIT_data.html#setup",
    "href": "external/How_to_find_and_access_EMIT_data.html#setup",
    "title": "How to: Find and Access EMIT Data",
    "section": "Setup",
    "text": "Setup\nImport the required packages\n\nimport os\nimport earthaccess\nimport numpy as np\nimport pandas as pd\nimport geopandas as gp\nfrom shapely.geometry.polygon import orient\nimport xarray as xr\nimport sys\nsys.path.append('../modules/')\nfrom emit_tools import emit_xarray",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "1. Finding and Accessing EMIT Data"
    ]
  },
  {
    "objectID": "external/How_to_find_and_access_EMIT_data.html#authentication",
    "href": "external/How_to_find_and_access_EMIT_data.html#authentication",
    "title": "How to: Find and Access EMIT Data",
    "section": "Authentication",
    "text": "Authentication\nearthaccess creates and leverages Earthdata Login tokens to authenticate with NASA systems. Earthdata Login tokens expire after a month. To retrieve a token from Earthdata Login, you can either enter your username and password each time you use earthaccess, or use a .netrc file. A .netrc file is a configuration file that is commonly used to store login credentials for remote systems. If you don’t have a .netrc or don’t know if you have one or not, you can use the persist argument with the login function below to create or update an existing one, then use it for authentication.\nIf you do not have an Earthdata Account, you can create one here.\n\nauth = earthaccess.login(persist=True)\nprint(auth.authenticated)\n\nIf you receive a message that your token has expired, use refresh_tokens() like below to generate a new one.\n\n# auth.refresh_tokens",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "1. Finding and Accessing EMIT Data"
    ]
  },
  {
    "objectID": "external/How_to_find_and_access_EMIT_data.html#searching-for-collections",
    "href": "external/How_to_find_and_access_EMIT_data.html#searching-for-collections",
    "title": "How to: Find and Access EMIT Data",
    "section": "Searching for Collections",
    "text": "Searching for Collections\nThe EMIT mission produces several collections or datasets available via the LP DAAC cloud archive.\nTo view what’s available, we can use the search_datasets function and with the keyword and and provider arguments. The provider is the data location, in this case LPCLOUD. Specifying the provider isn’t necessary, but the “emit” keyword can be found in metadata for some other datasets, and additional collections may be returned.\n\n# Retrieve Collections\ncollections = earthaccess.search_datasets(provider='LPCLOUD', keyword='emit')\n# Print Quantity of Results\nprint(f'Collections found: {len(collections)}')\n\nIf you print the collections object you can explore all of the json metadata.\n\n# # Print collections\n# collections\n\nWe can also create a list of the short-name, concept-id, and version of each result collection using list comprehension. These fields are important for specifying and searching for data within collections.\n\ncollections_info = [{n:[c.summary()['short-name'], c.summary()['concept-id'], c.summary()['version'], c['umm']['EntryTitle']]} for n, c in enumerate(collections)]\ncollections_info\n\nThe collection concept-id is the best way to search for data within a collection, as this is unique to each collection. The short-name can be used as well, however the version should be passed as well as there can be multiple versions available with the same short name. After finding the collection you want to search, you can use the concept-id to search for granules within that collection.",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "1. Finding and Accessing EMIT Data"
    ]
  },
  {
    "objectID": "external/How_to_find_and_access_EMIT_data.html#searching-for-granules",
    "href": "external/How_to_find_and_access_EMIT_data.html#searching-for-granules",
    "title": "How to: Find and Access EMIT Data",
    "section": "Searching for Granules",
    "text": "Searching for Granules\nA granule can be thought of as a unique spatiotemporal grouping within a collection. To search for granules, we can use the search_data function from earthaccess and provide the arguments for our search. Its possible to specify search products using several criteria shown in the table below:\n\n\n\n\n\n\n\n\ndataset origin and location\nspatio temporal parameters\ndataset metadata parameters\n\n\n\n\narchive_center\nbounding_box\nconcept_id\n\n\ndata_center\ntemporal\nentry_title\n\n\ndaac\npoint\nkeyword\n\n\nprovider\npolygon\nversion\n\n\ncloud_hosted\nline\nshort_name\n\n\n\n\nPoint Search\nIn this case, we specify the shortname, point, and temporal, as well as count, which limits the maximum number of results returned.\n\n# Search example using a Point\nresults = earthaccess.search_data(\n    short_name='EMITL2ARFL',\n    point=(-62.1123,-39.89402),\n    temporal=('2022-09-03','2022-09-04'),\n    count=100\n)\n\n\n\nBounding Box Search\nYou can also use a bounding box to search. To do this we will first open a geojson file containing our region of interest (ROI) then simplify it to a bounding box by getting the bounds and putting them into a Python object called a tuple. We will use the total_bounds property to get the bounding box of our ROI, and add that to a Python tuple, which is the expected data type for the bounding_box parameter earthaccess search_data.\n\ngeojson = gp.read_file('../../data/isla_gaviota.geojson')\ngeojson.geometry\n\n\nbbox = tuple(list(geojson.total_bounds))\nbbox\n\nNow we can search for granules using the a bounding box.\n\n# Search example using bounding box\nresults = earthaccess.search_data(\n    short_name='EMITL2ARFL',\n    bounding_box=bbox,\n    temporal=('2022-09-03','2022-09-04'),\n    count=100\n)\n\n\n\nPolygon Search\nA polygon can also be used to search. For a simple polygon without holes we can take the geojson we opened and grab the coordinates of the exterior ring vertices and place them in a list. Note that this list of vertices must be in counter-clockwise order to be accepted by the search_data function. If necessary, the external ring vertices of your polygon can be reordered using the orient function from the shapely library.\n\n# Orient External Ring Vertices\noriented = orient(geojson.geometry[0], sign=1.0)\n# Create List of External Ring vertices coordinates\npolygon = list(oriented.exterior.coords)\npolygon\n\nWith this list of coordinate pairs we can use the polygon parameter for our search. &gt; Note that we overwrote the results object, because for all 3 types spatial search, the results are the same for this example.\n\n# Search Example using a Polygon\nresults = earthaccess.search_data(\n    short_name='EMITL2ARFL',\n    polygon=polygon,\n    temporal=('2022-09-03','2022-09-04'),\n    count=100\n)",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "1. Finding and Accessing EMIT Data"
    ]
  },
  {
    "objectID": "external/How_to_find_and_access_EMIT_data.html#working-with-search-results",
    "href": "external/How_to_find_and_access_EMIT_data.html#working-with-search-results",
    "title": "How to: Find and Access EMIT Data",
    "section": "Working with Search Results",
    "text": "Working with Search Results\nAll three of these examples will have the same result, since the spatiotemporal parameters fall within the same single granule. Results is a list, so we can use an index to view a single result.\n\nresult = results[0]\nresult\n\nWe can also retrieve specific metadata for a result using .keys() since this object also acts as a dictionary.\n\nresult.keys()\n\n\nresult['umm'].keys()\n\nLook at the cloud cover percentage for the result granule.\n\nresult['umm']['CloudCover']\n\nFrom here, we can do other things, such as convert the results to a pandas dataframe.\n\npd.json_normalize(results)",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "1. Finding and Accessing EMIT Data"
    ]
  },
  {
    "objectID": "external/How_to_find_and_access_EMIT_data.html#downloading-or-streaming-data",
    "href": "external/How_to_find_and_access_EMIT_data.html#downloading-or-streaming-data",
    "title": "How to: Find and Access EMIT Data",
    "section": "Downloading or Streaming Data",
    "text": "Downloading or Streaming Data\nAfter we have our results, there are 2 ways we an work with the data:\n\nDownload All Assets\nSelectively Download Assets\nAccess in place / Stream the data.\n\nTo download the data we can simply use the download function. This will retrieve all assets associated with a granule, and is nice if you plan to work with the data in this way and need all of the assets included with the product. For the EMIT L2A Reflectance, this includes the Uncertainty and Masks files.\n\n# earthaccess.download(results, '../../data/')\n\nIf we want to stream the data or further filter the assets for download we want to first create a list of URLs nested by granule using list comprehesion.\n\nemit_results_urls = [granule.data_links() for granule in results]\nemit_results_urls\n\nNow we can also split these into results for specific assets or filter out an asset using the following. In this example, we only want to access or download reflectance.\n\nfiltered_asset_links = []\n# Pick Desired Assets - Use underscores to aid in stringmatching of the filenames (_RFL_, _RFLUNCERT_, _MASK_)\ndesired_assets = ['_RFL_']\n# Step through each sublist (granule) and filter based on desired assets.\nfor n, granule in enumerate(emit_results_urls):\n    for url in granule: \n        asset_name = url.split('/')[-1]\n        if any(asset in asset_name for asset in desired_assets):\n            filtered_asset_links.append(url)\nfiltered_asset_links\n\nAfter we have our filtered list, we can stream the reflectance asset or download it. Start an https session then open it to stream the data, or download to save the file.\n\nStream Data\nThis may take a while to load the dataset.\n\n# Get Https Session using Earthdata Login Info\nfs = earthaccess.get_fsspec_https_session()\n# Retrieve granule asset ID from URL (to maintain existing naming convention)\nurl = filtered_asset_links[0]\ngranule_asset_id = url.split('/')[-1]\n# Define Local Filepath\nfp = fs.open(url)\n# Open with `emit_xarray` function\nds = emit_xarray(fp)\nds\n\n\n\nDownload Filtered\n\n# Get requests https Session using Earthdata Login Info\nfs = earthaccess.get_requests_https_session()\n# Retrieve granule asset ID from URL (to maintain existing naming convention)\nfor url in filtered_asset_links:\n    granule_asset_id = url.split('/')[-1]\n    # Define Local Filepath\n    fp = f'../../data/{granule_asset_id}'\n    # Download the Granule Asset if it doesn't exist\n    if not os.path.isfile(fp):\n        with fs.get(url,stream=True) as src:\n            with open(fp,'wb') as dst:\n                for chunk in src.iter_content(chunk_size=64*1024*1024):\n                    dst.write(chunk)",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "1. Finding and Accessing EMIT Data"
    ]
  },
  {
    "objectID": "external/How_to_find_and_access_EMIT_data.html#contact-info",
    "href": "external/How_to_find_and_access_EMIT_data.html#contact-info",
    "title": "How to: Find and Access EMIT Data",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 11-06-2024\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Tutorials",
      "1. Finding and Accessing EMIT Data"
    ]
  },
  {
    "objectID": "external/prerequisites.html",
    "href": "external/prerequisites.html",
    "title": "Prerequisites",
    "section": "",
    "text": "Prerequisites\nTo follow along during the workshop, or to run through the notebooks contained within the repository using the Openscapes 2i2c Cloud JupyterHub (cloud workspace), the following are required. All software or accounts are free.\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov/users/new\nRemember your username and password; you will need them to download or access data during the workshop and beyond.\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com/join. Follow optional advice on choosing your username\nYour GitHub username is used to enable you access to a cloud environment during the workshop. To gain access, please request access to the NASA Openscapes JupyterHub using this form. You will receive an email invitation to join the organization on GitHub. You must join to gain access to the workspace.\n\n\nNetrc file\n\nThis file is needed to access NASA Earthdata assets from a scripting environment like Python.\nThere are multiple methods to create a .netrc file. For this workshop, earthaccess package is used to automatically create a netrc file using your Earthdata login credentials if one does not exist. There are detailed instruction available for creating a .netrc file using other methods here.\n\nLaptop or tablet\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All workshop participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2.",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Setup Instructions",
      "Prerequisites"
    ]
  },
  {
    "objectID": "external/Visualizing_Methane_Plume_Timeseries.html",
    "href": "external/Visualizing_Methane_Plume_Timeseries.html",
    "title": "Visualizing Methane Plume Timeseries",
    "section": "",
    "text": "This notebook is from EMIT-Data-Resources\n\n\nSource: Visualizing Methane Plume Timeseries\n\nImported on: 2024-11-07",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "8. Visualizing Methane Plume Timeseries"
    ]
  },
  {
    "objectID": "external/Visualizing_Methane_Plume_Timeseries.html#set-up",
    "href": "external/Visualizing_Methane_Plume_Timeseries.html#set-up",
    "title": "Visualizing Methane Plume Timeseries",
    "section": "1. Set up",
    "text": "1. Set up\nImport the necessary Python libraries.\n\n# Import required libraries\nimport sys\nimport os\nimport glob\nimport requests\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nfrom osgeo import gdal\nimport geopandas as gpd\n\nfrom datetime import datetime\nimport folium\nimport earthaccess\nimport folium.plugins\nimport rioxarray as rxr\n\nimport hvplot.xarray\nimport hvplot.pandas\n\nfrom branca.element import Figure\nfrom IPython.display import display\nfrom shapely.geometry.polygon import orient\nfrom shapely.geometry import Point\n\nsys.path.append('../modules/')\nfrom emit_tools import emit_xarray, ortho_xr, ortho_browse\nfrom tutorial_utils import list_metadata_fields, results_to_geopandas, convert_bounds\n\nLogin using earthaccess and create a .netrc file if necessary. This file will store your NASA Earthdata Login credentials and use them to authenticate when necessary.\n\nearthaccess.login(persist=True)\n\nAll of the data we use or save will go into the methane_tutorial directory, so we can go ahead and define that filepath now, relative to this notebook.\n\nmethane_dir = '../../data/methane_tutorial/'",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "8. Visualizing Methane Plume Timeseries"
    ]
  },
  {
    "objectID": "external/Visualizing_Methane_Plume_Timeseries.html#search-for-emit-l2b-estimated-methane-plume-complexes",
    "href": "external/Visualizing_Methane_Plume_Timeseries.html#search-for-emit-l2b-estimated-methane-plume-complexes",
    "title": "Visualizing Methane Plume Timeseries",
    "section": "2. Search for EMIT L2B Estimated Methane Plume Complexes",
    "text": "2. Search for EMIT L2B Estimated Methane Plume Complexes\nUse earthaccess to find all EMIT L2B Estimated Methane Plume Complexes (EMITL2BCH4PLM) data available from 2023. Define the date range, and concept-ids (unique product identifier) for the EMIT products that we want to search for, but leave the spatial arguments like polygon and bbox empty so we can preview detected methane plumes globally.\n\n# Data Collections for our search, using a dictionary\nconcept_ids = {'plumes':'C2748088093-LPCLOUD', 'reflectance':'C2408750690-LPCLOUD'}\n# Define Date Range\ndate_range = ('2023-01-01','2023-12-31')\n\n\nresults = earthaccess.search_data(\n    concept_id=concept_ids['plumes'],\n    temporal=date_range,\n    count=2000\n)\n\nConvert the results to a geopandas.GeoDataFrame using a function from our tutorial_utils module. This gives a nice way to organize and visualize the search results.\n\ngdf = results_to_geopandas(results)\ngdf\n\nThe function includes default fields, but you can add more with the fields argument. To see all of the metadata available use the list_metadata_fields function imported from the tutorial_utils.py module.\n\nlist_metadata_fields(results)\n\nAdd an index column to the dataframe to include it in the tooltips for our visualization.\n\n# Specify index so we can reference it with gdf.explore()\ngdf['index'] = gdf.index\n\n\n# Set up Figure and Basemap tiles\nfig = Figure(width=\"1080px\",height=\"540\")\nmap1 = folium.Map(tiles=None)\nfolium.TileLayer(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',name='Google Satellite', attr='Google', overlay=True).add_to(map1)\nfolium.TileLayer(tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}.png',\n                name='ESRI World Imagery',\n                attr='Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community',\n                overlay='True').add_to(map1)\nfig.add_child(map1)\n# Add Search Results gdf\ngdf.explore(\"_single_date_time\",\n            categorical=True,\n            style_kwds={\"fillOpacity\":0.1,\"width\":2},\n            name=\"EMIT L2B CH4PLM\",\n            tooltip=[\n                \"index\",\n                \"native-id\",\n                \"_single_date_time\",\n            ],\n            m=map1,\n            legend=False\n)\n\n# Zoom to Data\nmap1.fit_bounds(bounds=convert_bounds(gdf.unary_union.bounds))\n# Add Layer controls\nmap1.add_child(folium.LayerControl(collapsed=False))\ndisplay(fig)\n\nIn this example we’ll chose a region that looks like it has a several plumes emitting from the same source, a landfill in Jordan. We could create a simple bounding box around our target region by using the plumes that extend furthest in the cardinal directions to generate a bounding box around the region that we can use in our upcoming analysis.\nHowever, to simplify things we will import an existing geojson as a GeoDataFrame with a bounding box around this region because the plume indices may change, which results in inconsistent outputs. A commented out cell below as included as an example of how the GeoDataFrame was created before being written to geojson.\n\n# # Select a list of plumes to create geometry we can use for a spatial subset\n# plumes = [146,198,243]\n# bbox = gdf.loc[plumes].geometry.unary_union.envelope\n# bbox = orient(bbox, sign=1)\n# plume_bbox = gpd.GeoDataFrame({\"name\":['plume_bbox'], \"geometry\":[bbox]},crs=gdf.crs)\n\nOpen the predefined geojson of our plume bounding box as a GeoDataFrame then visualize on our folium figure.\n\n# Open the geojson with our plume bbox\nplume_bbox = gpd.read_file(f'{methane_dir}/plume_bbox.geojson')\n\n\n# Set up Figure and Basemap tiles\nfig = Figure(width=\"1080px\",height=\"540\")\nmap1 = folium.Map(tiles=None)\nfolium.TileLayer(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',name='Google Satellite', attr='Google', overlay=True).add_to(map1)\nfolium.TileLayer(tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}.png',\n                name='ESRI World Imagery',\n                attr='Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community',\n                overlay='True').add_to(map1)\nfig.add_child(map1)\n# Add Search Results gdf\nplume_bbox.explore(\"name\",\n                   name='Plume BBox',\n                   style_kwds={\"fillOpacity\":0,\"width\":2},\n                   m=map1,\n                   legend=False)\n\ngdf.explore(\"_single_date_time\",\n            categorical=True,\n            style_kwds={\"fillOpacity\":0.1,\"width\":2},\n            name=\"EMIT L2B CH4PLM\",\n            tooltip=[\n                \"index\",\n                \"native-id\",\n                \"_single_date_time\",\n            ],\n            m=map1,\n            legend=False\n)\n# Zoom to Data\nmap1.fit_bounds(bounds=convert_bounds(plume_bbox.unary_union.bounds))\n# Add Layer controls\nmap1.add_child(folium.LayerControl(collapsed=False))\ndisplay(fig)\n\nSubset our geodataframe of global plumes to only those that intersect our bounding box.\n\nplm_gdf = gdf[gdf.geometry.intersects(plume_bbox.geometry[0])]\nplm_gdf\n\nLet’s examine the _related_urls column in our geodataframe. This column contains various links to assets related to each plume.\n\nplm_gdf['_related_urls'].iloc[0]\n\nWe can write a function to return the asset URL for a given asset and row in our dataframe.\n\ndef get_asset_url(row,asset, key='Type',value='GET DATA'):\n    \"\"\"\n    Retrieve a url from the list of dictionaries for a row in the _related_urls column.\n    Asset examples: CH4PLM, CH4PLMMETA, RFL, MASK, RFLUNCERT \n    \"\"\"\n    # Add _ to asset so string matching works\n    asset = f\"_{asset}_\"\n    # Retrieve URL matching parameters\n    for _dict in row['_related_urls']:\n        if _dict.get(key) == value and asset in _dict['URL'].split('/')[-1]:\n            return _dict['URL']\n\nWrite another function to make a request to retrieve the json metadata for a given plume, using our get_asset_url function to select the correct URL from the dataframe.\n\n# Function to fetch CH4 Plume Metadata\n# Speed could be improved here by using asyncio/aiohttp\ndef fetch_ch4_metadata(row):\n    response = requests.get(get_asset_url(row, 'CH4PLMMETA'))\n    json = response.json()\n    return json['features'][0]['properties']\n\n\nfetch_ch4_metadata(plm_gdf.iloc[0])\n\nRetrieve additional plume metadata contained in the EMIT L2B Estimated Methane Plume Complexes (EMITL2BCH4PLM) data product, which contains the maximum enhancement value, the uncertainty of the plume complex, and the list of source scenes.\n\n# Apply the function to each row and convert the result to a DataFrame\nplm_meta = plm_gdf.apply(fetch_ch4_metadata, axis=1).apply(pd.Series)\n\n\nplm_meta\n\nWe can add the points with highest methane concentration to our visualization.\nCreate an index column, as we did for the plumes, then convert the latitude and longitude of max concentration to a shapely Point object and add it to our GeoDataFrame.\n\n# Specify index so we can reference it with gdf.explore()\nplm_meta['index'] = plm_meta.index\n# Add Geometry and convert to geodataframe\nplm_meta['geometry'] = plm_meta.apply(lambda row: Point(row['Longitude of max concentration'], row['Latitude of max concentration']), axis=1)\nplm_meta = gpd.GeoDataFrame(plm_meta, geometry='geometry', crs='EPSG:4326')\n\n\nplm_meta\n\nNow add this to our visualization.\n\n# Set up Figure and Basemap tiles\nfig = Figure(width=\"1080px\",height=\"540\")\nmap1 = folium.Map(tiles=None)\nfolium.TileLayer(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',name='Google Satellite', attr='Google', overlay=True).add_to(map1)\nfolium.TileLayer(tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}.png',\n                name='ESRI World Imagery',\n                attr='Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community',\n                overlay='True').add_to(map1)\nfig.add_child(map1)\n# Add Search Results gdf\nplume_bbox.explore(\"name\",\n                   name='Plume BBox',\n                   style_kwds={\"fillOpacity\":0,\"width\":2},\n                   m=map1,\n                   legend=False)\n\nplm_gdf.explore(\"index\",\n            categorical=True,\n            style_kwds={\"fillOpacity\":0.1,\"width\":2},\n            name=\"EMIT L2B CH4PLM\",\n            tooltip=[\n                \"index\",\n                \"native-id\",\n                \"_single_date_time\",\n            ],\n            m=map1,\n            legend=False\n)\n\nplm_meta.explore(\"index\",\n            categorical=True,\n            style_kwds={\"fillOpacity\":0.1,\"width\":2},\n            name=\"Location of Max Concentration (ppm m)\",\n            tooltip=[\n                \"DAAC Scene Names\",\n                \"UTC Time Observed\",\n                \"Max Plume Concentration (ppm m)\",\n                \"Concentration Uncertainty (ppm m)\",\n                \"Orbit\"\n            ],\n            m=map1,\n            legend=False\n)\n# Zoom to Data\nmap1.fit_bounds(bounds=convert_bounds(plume_bbox.unary_union.bounds))\n# Add Layer controls\nmap1.add_child(folium.LayerControl(collapsed=False))\ndisplay(fig)",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "8. Visualizing Methane Plume Timeseries"
    ]
  },
  {
    "objectID": "external/Visualizing_Methane_Plume_Timeseries.html#creating-a-timeseries-from-plume-data",
    "href": "external/Visualizing_Methane_Plume_Timeseries.html#creating-a-timeseries-from-plume-data",
    "title": "Visualizing Methane Plume Timeseries",
    "section": "3. Creating a Timeseries from Plume Data",
    "text": "3. Creating a Timeseries from Plume Data\nWe can visualize a timeseries of these plumes that appear to be from the same source. To do this we’ll generate a list of the COG URLs for the plumes, then use rioxarray to stream the data and build a timeseries dataset based on the timestamps in the filenames.\nUse the get_asset_url function to retrieve the CH4PLM asset URLs by applying it to our dataframe and converting the output to a list.\n\n# Iterate over rows in the plm_gdf and get the CH4PLM URLs and store them in a list\nplm_urls = plm_gdf.apply(lambda row: get_asset_url(row, asset='CH4PLM'), axis=1).tolist()\nplm_urls\n\nNow that we have a list of COG urls we can set our gdal configuration options to pass our NASA Earthdata login credentials when we access each COG.\n\n# GDAL configurations used to successfully access LP DAAC Cloud Assets via vsicurl \ngdal.SetConfigOption('GDAL_HTTP_COOKIEFILE','~/cookies.txt')\ngdal.SetConfigOption('GDAL_HTTP_COOKIEJAR', '~/cookies.txt')\ngdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN','EMPTY_DIR')\ngdal.SetConfigOption('CPL_VSIL_CURL_ALLOWED_EXTENSIONS','TIF')\ngdal.SetConfigOption('GDAL_HTTP_UNSAFESSL', 'YES')\ngdal.SetConfigOption('GDAL_HTTP_MAX_RETRY', '10')\ngdal.SetConfigOption('GDAL_HTTP_RETRY_DELAY', '0.5')\n\nTo build our timeseries we will start by opening all the necessary data. Loop over our list of URLs, open each plume, merge plumes acquired at the same time, and store them in a dictionary where keys correspond to the acquisition time and values are the plume data in an xarray.DataArray.\n\nplm_ts_dict = {}\n# Set max retries for vsicurl errors\nmax_retries=5\n# Iterate over plm urls\nfor url in plm_urls:\n    # retrieve acquisition time from url\n    acquisition_time = url.split('/')[-1].split('.')[-2].split('_')[-2]\n    # list plumes identified in same scene if there are any\n    same_scene = [url for url in plm_urls if acquisition_time in url.split('/')[-1].split('.')[-2].split('_')[-2]]\n    to_merge = []\n    # prevent duplicate processing of plumes from the same scene\n    if acquisition_time not in list(plm_ts_dict.keys()):\n        # Open and merge plumes identified from each scene\n        for _plm in same_scene:\n            print(f\"Opening {_plm.split('/')[-1]}\")\n            # Open COG and squeeze band dimension\n            plm = rxr.open_rasterio(_plm).squeeze('band', drop=True)\n            # Add to list of plumes to merge\n            to_merge.append(plm)\n            # Merge plumes and add to timeseries\n            plm_ts_dict[acquisition_time] = rxr.merge.merge_arrays(to_merge)    \n\nNow that we have a plume object for each date in our timeseries, we need to put them all on a common grid so they are spatially aligned before we can stack them along the time dimension.\nTo do this, we will find the minimum and maximum bounds of each dataarray in our dictionary, then create a common grid to reproject to.\n\nfrom typing import List\ndef create_common_grid(data_arrays: List[xr.DataArray]) -&gt; xr.DataArray:\n    \"\"\"\n    Create a common grid for a list of xarray DataArrays, matching the resolution of the first data array.\n    \"\"\"\n    # Initial Bounds for common grid\n    minx = miny = float('inf')\n    maxx = maxy = float('-inf')\n\n    for array in data_arrays:\n        left, bottom, right, top = array.rio.bounds()\n        minx = min(minx, left)\n        miny = min(miny, bottom)\n        maxx = max(maxx, right)\n        maxy = max(maxy, top)\n\n    bounds = (minx, miny, maxx, maxy)\n\n    res = data_arrays[0].rio.resolution()\n    crs = data_arrays[0].rio.crs\n    nodata = data_arrays[0].rio.nodata\n\n    # Calculate new raster shape using the new extent, maintaining the original resolution\n    height = int(np.ceil((bounds[3] - bounds[1]) / abs(res[1])))\n    width = int(np.ceil((bounds[2] - bounds[0]) / abs(res[0])))\n    data = np.full((height,width),nodata)\n    coords = {'y':(['y'],np.arange(bounds[1], bounds[3], abs(res[1]))),\n              'x':(['x'],np.arange(bounds[0], bounds[2], abs(res[0])))}\n    common_grid = xr.DataArray(data, coords=coords)\n    common_grid.rio.write_crs(crs, inplace=True)\n    return(common_grid)\n\n\ncommon_grid = create_common_grid(list(plm_ts_dict.values()))\ncommon_grid\n\nReproject each of the plume dataarrays to the common grid.\n\nplm_ts_dict = {key: value.rio.reproject_match(common_grid) for key, value in plm_ts_dict.items()}\n\nNow that we have all of our plumes on a standard grid, we can concatenate them along a time dimension to create a timeseries. Create an xarray variable called ‘time’ from our dictionary keys, then use xarray.concat to concatenate all of our plumes along the time dimension.\n\nplm_time = xr.Variable('time', [datetime.strptime(t,'%Y%m%dT%H%M%S') for t in list(plm_ts_dict.keys())])\nplm_time\n\n\nplm_ts_ds = xr.concat(list(plm_ts_dict.values()), dim=plm_time)\nplm_ts_ds\n\nSet our no_data values to np.nan to make sure they are transparent for improved visualization.\n\nplm_ts_ds.data[plm_ts_ds.data == -9999] = np.nan\n\nPlot the plume time series.\n\nplm_ts_plot = plm_ts_ds.hvplot.image(x='x',y='y',geo=True, tiles='ESRI', crs='EPGS:4326', cmap='inferno',clim=(0,np.nanmax(plm_ts_ds.data)),clabel=f'Methane Concentation ({plm_ts_ds.Units})', frame_width=600, frame_height=600, rasterize=True)\n\n\nplm_ts_plot",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "8. Visualizing Methane Plume Timeseries"
    ]
  },
  {
    "objectID": "external/Visualizing_Methane_Plume_Timeseries.html#calculating-the-ime-for-each-plume",
    "href": "external/Visualizing_Methane_Plume_Timeseries.html#calculating-the-ime-for-each-plume",
    "title": "Visualizing Methane Plume Timeseries",
    "section": "4. Calculating the IME for each plume",
    "text": "4. Calculating the IME for each plume\nThe integrated mass enhancement (IME), a sum of the mass of methane present in each plume is calculated by summing the mass of methane present in all plume pixels. The IME in kilograms (kg) can be estimated by using the equation below which includes the mixing ratio length per pixel (ppmm), the area of the pixel (m^2), where 22.4 is the volume of 1 mole of gas at standard temperature and pressure (STP) in liters, and 0.01604 is the molar mass of methane in kg/mol.\nThe IME can be combined with a plume length and windspeed to estimate emissions in units of kg per hour, but in this tutorial, we will not calculate emissions, rather keep things simple and calculate the IME for each plume and observe how these values change over time.\n\\[\\ kg\\ \\ (per \\ \\ pixel) = \\frac{pixel \\ \\ value \\ \\ ppm \\cdot m}{1} \\frac{1}{1 \\cdot 10^6 \\ \\ ppm} \\frac {60 \\ \\ m \\cdot 60 \\ \\ m} {1} \\frac {1000 \\ \\ L} {m^3} \\frac {1 \\ \\ mol} {22.4 \\ \\ L} \\frac {0.01604 \\ \\ kg} {1 \\ \\ mol}\\]\nWe can write this as a function for each pixel, then apply it to the entire timeseries to calculate the IME for each plume.\n\ndef calc_ime(plume_da):\n    molar_volume = 22.4 # L/mol at STP\n    molar_mass_ch4 = 0.01604 #kg/mol\n\n    kg = plume_da * (1/1e6) * (60*60) * (1000) * (1/molar_volume) * molar_mass_ch4\n    ime = np.nansum(kg)\n    return ime\n\n\n# Apply the function along the 'x' and 'y' dimensions\nime_ts = xr.apply_ufunc(calc_ime, plm_ts_ds, input_core_dims=[['y', 'x']], vectorize=True)\nime_ts.name = 'value'\n\n\nime_plot = ime_ts.hvplot.scatter(x='time',y='value', title='Observed Methane IME over 2023', color='black', xticks=list(ime_ts.time.data), rot=90, grid=True, xlabel='Date Observed', ylabel='kg')\n\n\nime_plot",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "8. Visualizing Methane Plume Timeseries"
    ]
  },
  {
    "objectID": "external/Visualizing_Methane_Plume_Timeseries.html#further-investigation-into-plume-detection",
    "href": "external/Visualizing_Methane_Plume_Timeseries.html#further-investigation-into-plume-detection",
    "title": "Visualizing Methane Plume Timeseries",
    "section": "5. Further investigation into plume detection",
    "text": "5. Further investigation into plume detection\nThe timeseries shown above doesn’t necessarily give us a full a full picture methane emissions at the landfill. In addition to cases where no emissions were observed, gaps in data can result from absence of observations due the variable revisit period of the ISS, or clouds. To add more context to this plume timeseries, we will look at all EMIT acquisitions over the target regions and try to determine if there were factors affecting plume detection, or simply no methane emissions during those for any other overpass.\nFor this search we’ll want to restrict our search to a smaller ROI than our bounding box. We want to pick something smaller more centered around the source of the emission, so we avoid retrieving irrelevant data, for example an overpass barely crossing the corner of our large bounding box. To do this we will use the maximum concentration points from our plumes to create a smaller ROI that is likely to include a methane plume if one is present.\nCreate a new polygon using the maximum concentration points from our plumes as vertices, then orienting the points in counter-clockwise order so they are compatible with an earthaccess search. This is just a simple example approach, there are several ways we could define a smaller ROI using the plume data or ancillary information.\n\nmax_conc_poly = plm_meta.geometry.unary_union.envelope\nmax_conc_poly = orient(max_conc_poly, sign=1.0)\nmax_conc_gdf = gpd.GeoDataFrame({\"name\":['max_points'], \"geometry\":[max_conc_poly]},crs=plm_meta.crs)\n\n\nroi = list(max_conc_gdf.geometry[0].exterior.coords)\nroi\n\nNow conduct a search for reflectance data over our ROI and convert the results to a geopandas.GeoDataFrame.\n\nrfl_results = earthaccess.search_data(\n    concept_id=concept_ids['reflectance'],\n    polygon=roi,\n    temporal= date_range, #('2023-03-01','2023-03-31')\n    count=2000\n)\nrfl_gdf = results_to_geopandas(rfl_results)\nrfl_gdf\n\nFrom the results we can see we have 20 scenes that intersect our ROI. We can visualize the footprints of these scenes to gain some insight into coverage over our ROI.\n\n# Set up Figure and Basemap tiles\nfig = Figure(width=\"1080px\",height=\"540\")\nmap1 = folium.Map(tiles=None)\nfolium.TileLayer(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',name='Google Satellite', attr='Google', overlay=True).add_to(map1)\nfolium.TileLayer(tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}.png',\n                name='ESRI World Imagery',\n                attr='Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community',\n                overlay='True').add_to(map1)\nfig.add_child(map1)\n\n# Add Search Reflectance Scenes with no CH4\nrfl_gdf.explore(color='red',\n               style_kwds={\"fillOpacity\":0,\"width\":2},\n               name=\"Scenes with no CH4 Plumes\",\n               tooltip=[\n                \"native-id\",\n                \"_beginning_date_time\",\n                ],\n                m=map1,\n                legend=False)\n\n# Add Plume BBox to Map\nmax_conc_gdf.explore(m=map1,\n                   name='Plumes Bounding Box',\n                   legend=False)\n\n# Zoom to Data\nmap1.fit_bounds(bounds=convert_bounds(rfl_gdf.unary_union.bounds))\n# Add Layer controls\nmap1.add_child(folium.LayerControl(collapsed=False))\ndisplay(fig)\n\nFrom this we can see that there are several scenes that intersect with our ROI, but likely have no relevant information since they only cover a small portion or corner of the ROI.\nWe can use a similar process as with the methane product to construct a time series to better understand the data gathered on each overpass. To do this, we will use the browse imagery and the masks included in the EMITL2ARFL product. The by default the mask files and browse images are not orthorectified, so we must do that as part of our workflow.\nFirst, get the urls for the browse images and masks for each scene in our rfl_gdf search results using the get_asset_urls function.\n\npng_urls = rfl_gdf.apply(lambda row: get_asset_url(row, asset='RFL', value='GET RELATED VISUALIZATION'), axis=1).tolist()\npng_urls\n\n\nmask_urls = rfl_gdf.apply(lambda row: get_asset_url(row, asset='MASK'), axis=1).tolist()\nmask_urls\n\nWe can write these as a text file so we don’t need to search again, although we will use the rfl_gdf GeoDataFrame later in the tutorial.\n\n# # Save URL List\n# with open(f'{methane_dir}rfl_mask_urls.txt', 'w') as f:\n#     for line in mask_urls:\n#         f.write(f\"{line}\\n\")\n\nSince the mask files are not chunked, its quicker to download them to do the processing.\nLogin with earthaccess and download these files.\n\nearthaccess.login(persist=True)\n# Get requests https Session using Earthdata Login Info\nfs = earthaccess.get_requests_https_session()\n# Retrieve granule asset ID from URL (to maintain existing naming convention)\nfor url in mask_urls:\n    granule_asset_id = url.split('/')[-1]\n    # Define Local Filepath\n    fp = f'{methane_dir}{granule_asset_id}'\n    # Download the Granule Asset if it doesn't exist\n    if not os.path.isfile(fp):\n        with fs.get(url,stream=True) as src:\n            with open(fp,'wb') as dst:\n                for chunk in src.iter_content(chunk_size=64*1024*1024):\n                    dst.write(chunk)\n\nFor each of these scenes we want to open EMIT L2A Mask data, then subset spatially and select only the variable we want, in this case we’ll use the Cloud flag from the masks dataarray. There are other flags, such as a cirrus mask, dilated cloud flag, spacecraft flag, water flag, and and aerosol optical depth. We could potentially use some of these as well to inform our decisions about plume detection, but will stick to the Cloud flag in this example for simplicity.\nAs we do this, we will also use the GLT included in the mask file to orthorectify our RGB browse image. We can do this because the browse png files are in the native resolution and can be broadcast onto an orthorectified grid using the GLT. We will use the reproject_match function to reproject the data on a common_grid, which will automatically clip the data to the common_grid extent.\nFirst, get the filepaths for our downloaded mask data.\n\n# List the downloaded files\nfps = glob.glob(f'{methane_dir}*.nc')\n# Make sure only files listed are also in the mask_urls list of files downloaded\nfns = [os.path.basename(fp) for fp in fps if any(os.path.basename(fp) in url.split('/')[-1] for url in mask_urls)]\nfns\n\nCreate a function to loop through our files, orthorectifying the mask and browse image, clipping and reprojecting to our predefined common_grid, and finally saving outputs as a COG.\n\ndef process_scenes(fns, outdir, common_grid):\n    \"\"\"\n    This function will process a list of EMIT Mask scenes, selecting the cloud flag, orthorectifying the mask and browse image, then reprojecting both to a common grid and saving an output.\n    \"\"\"\n    for fn in fns:\n        # Get Granule Asset ID for First Adjacent Scene (may only be one)\n        granule_asset_id = fn.split('.')[-2]\n        # Set Output Path\n        outpath_mask = f\"{outdir}{granule_asset_id}_cloud_flag.tif\"\n        outpath_browse = f\"{outdir}{granule_asset_id}_ortho_browse.tif\"\n        # Check if the file exists\n        if not os.path.isfile(outpath_mask):\n            # Open Mask Dataset\n            emit_ds = emit_xarray(f'{methane_dir}{fn}', ortho=False)\n            # Retrieve GLT, spatial_ref, and geotransform to use on browse image\n            glt = np.nan_to_num(np.stack([emit_ds[\"glt_x\"].data, emit_ds[\"glt_y\"].data], axis=-1),nan=0).astype(int)\n            spatial_ref = emit_ds.spatial_ref\n            gt = emit_ds.geotransform\n            # Select browse image url corresponding to the scene\n            png_url = [url for url in png_urls if fn.split('.')[-2].split('_')[-3] in url][0]\n            # Orthorectify browse and mask\n            rgb = ortho_browse(png_url, glt, spatial_ref, gt, white_background=True)\n            emit_ds = ortho_xr(emit_ds)\n            # Select only mask array and desired quality flag and reproject to match our chosen extent\n            mask_da = emit_ds['mask'].sel(mask_bands='Cloud flag')\n            # Drop elevation\n            mask_da = mask_da.drop_vars('elev')\n            mask_da.name = 'Cloud flag'\n            mask_da.data = np.nan_to_num(mask_da.data, nan=-9999)\n            mask_da = mask_da.rio.reproject_match(common_grid, nodata=-9999)\n            #mask_da.rio.write_nodata(np.nan, inplace=True)\n            # Reproject rgb\n            rgb = rgb.rio.reproject_match(common_grid, nodata=255) # 255 for white background\n            # Write cog outputs        \n            mask_da.rio.to_raster(outpath_mask,driver=\"COG\")\n            rgb.rio.to_raster(outpath_browse,driver=\"COG\")\n\nRun the function.\n\nprocess_scenes(fns, methane_dir, common_grid)\n\nCreate a list of the processed files to use in creation of a timeseries. We’ll use a similar process to what we did for the plumes, adding a time variable to our datasets and concatenating.\n\nmask_files = sorted(glob.glob(f'{methane_dir}*cloud_flag.tif'))\nmask_files\n\n\nrgb_files = sorted(glob.glob(f'{methane_dir}*ortho_browse.tif'))\nrgb_files\n\nBuild a time index from the filenames.\n\ndef time_index_from_filenames(file_names,datetime_pos):\n    \"\"\"\n    Helper function to create a pandas DatetimeIndex\n    \"\"\"\n    return [datetime.strptime(f.split('_')[datetime_pos], '%Y%m%dT%H%M%S') for f in file_names]\n\n\nmask_time = xr.Variable('time', time_index_from_filenames(mask_files, -5))\n\nOpen and concatenate our datasets along the time dimension, then assign fill_values to np.nan to make those sections of the data transparent in our visualization.\n\nquality_ts_da = xr.concat([rxr.open_rasterio(f).squeeze('band', drop=True).rio.reproject_match(common_grid) for f in mask_files], dim=mask_time)\n\nCreate a plot object for the quality timeseries, first setting the quality mask values representing no clouds (0) or no data (-9999) to np.nan so they will be transparent in our visualization.\n\nquality_ts_da.data[quality_ts_da.data &lt; 1] = np.nan\nquality_ts_map = quality_ts_da.hvplot.image(x='x',y='y',cmap='greys',groupby='time',clim=(0,1),geo=True,frame_height=400)\n\nRGB images are a good way to add something more visually understandable than just the mask layers. Follow the same process as above to build an RGB timeseries, then plot it with the bounding box and plume extents.\n\nrgb_ts_ds = xr.concat([rxr.open_rasterio(f).rio.reproject_match(common_grid) for f in rgb_files], dim=mask_time)\nrgb_ts_ds.data[rgb_ts_ds.data == -1] = 255\n\n\nrgb_ts_map = rgb_ts_ds.hvplot.rgb(x='x',y='y', bands='band',groupby='time',geo=True, frame_height=400, crs='EPSG:4326')\n\nLastly, add a new column to our plume geodataframe named time so we are using the same naming convention and can layer our plume polygons from our filtered search on top of our quality data.\n\nplm_gdf['time'] = pd.to_datetime(plm_gdf.loc[:,'_single_date_time'])\n\nBecause we’ve set our RGB to display as white where there is no data, and our cloud mask where no clouds are present as transparent, we can identify any areas with no data over our ROI by white/transparent, and cloudy areas as black. The larger black lines/rectangles are representative of onboard cloud masking, where no data was downlinked due to a high volume of clouds detected.\n\nrgb_ts_map*quality_ts_map*plm_gdf.hvplot(groupby='time', geo=True, line_color='red', fill_color=None)*max_conc_gdf.hvplot(color='red',crs='EPSG:4326',fill_color=None, line_color='yellow')\n\nWith this information, we can build a dataframe assigning a category to each of the dates where there was no plume detected and add this information to our IME timeseries figure.\nCreate a dataframe of dates where no plume was detected by finding the dates where there are no plumes in our mask_time arrays by removing timestamps from our plm_time array.\n\nno_plm_time = np.setdiff1d(mask_time.data,plm_time.data)\nno_plm_time\n\nBuild a dataframe out of these dates where there were no plumes detected\n\n# Build a dataframe\nno_plm_df = pd.DataFrame({'time':no_plm_time})\nno_plm_df\n\nNext, categorize them based on the visualizations we made and remove any times where there wasn’t a good observation of our area of interest.\n\n# add a category column to describe the observation \nno_plm_df['category'] = 'no_data'\nno_plm_df.loc[[0,4],'category'] = 'cloud'\nno_plm_df.loc[7,'category'] = 'no_plume'\n\n\nno_plm_df = no_plm_df[~no_plm_df['category'].str.contains('no_data')]\nno_plm_df['time'] = pd.to_datetime(no_plm_df['time'])\nno_plm_df.reset_index(drop=True, inplace=True)\nno_plm_df\n\nWe can now merge our IME timeseries dataframe with this one, along the time column to add these observations with different categories to our timeseries.\n\nime_df = pd.merge(ime_ts.to_dataframe(), no_plm_df, on=['time'], how='outer')\nime_df = ime_df.sort_values(by='time')\nime_df['category']=ime_df['category'].fillna('plume')\nime_df['value'] = ime_df['value'].fillna(0)\n\n\nime_df\n\nWe can now plot this IME timeseries alongside our plume timeseries.\n\nime_timeline = ime_df.hvplot.scatter(x='time',y='value', by='category', size=100, xlabel='Date Observed', ylabel='kg', title='Observed Methane IME over 2023', rot=90,\n                                     grid=True, frame_height=400, frame_width=800, ylim=(0,11000),xticks=list(ime_df.time))\n\n\nime_timeline.opts(legend_position='bottom') + plm_ts_plot.opts(title='Methane Plumes Observed', frame_height=400, frame_width=400)",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "8. Visualizing Methane Plume Timeseries"
    ]
  },
  {
    "objectID": "external/Visualizing_Methane_Plume_Timeseries.html#contact-info",
    "href": "external/Visualizing_Methane_Plume_Timeseries.html#contact-info",
    "title": "Visualizing Methane Plume Timeseries",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 10-25-2024\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Python Resources",
      "EMIT",
      "8. Visualizing Methane Plume Timeseries"
    ]
  },
  {
    "objectID": "external/workshop_setup.html",
    "href": "external/workshop_setup.html",
    "title": "Cloud Workspace Setup",
    "section": "",
    "text": "If you plan to use this repository with the Openscapes 2i2c JupyterHub Cloud Workspace there are no additional setup requirements for the Python environment. All packages needed are included unless specified within a notebook, in which case a cell will be dedicated to installing the necessary Python libraries using the appropriate package manager.\n\nAfter completing the prerequisites you will have access to the Openscapes 2i2c JupyterHub cloud workspace. Click here to start JupyterLab. Use your email and the provided password to sign in. This password will be provided in the workshop. If you’re interested in using the 2i2c cloud workspace outside of the workshop, please contact us.\nAfter signing in you will be prompted for some server options:\n\n\n\nBe sure to select the radio button for Python and a size of 14.8 GB RAM and up to 3.75 CPUs.\nAt this point you can use the terminal to clone the repository.\n\n\nIf you plan to edit or contribute to the EMIT-Data-Resources repository, we recommend following a fork and pull workflow: first fork the repository, then clone your fork to your local machine, make changes, push changes to your fork, then make a pull request back to the main repository. An example can be found in the CONTRIBUTING.md file.\nIf you just want to work with the notebooks or modules, you can simply clone the repository.\nTo clone the repository, navigate to the directory where you want to store the repository on your local machine, then type the following:\ngit clone https://github.com/nasa/EMIT-Data-Resources.git\n\n\n\nWe recommend Shutting down all kernels after running each notebook. This will clear the memory used by the previous notebook, and is necessary to run some of the more memory intensive notebooks.\n\n\n\nNo single notebook exceeds roughly the limit using the provided data, but if you choose to use your own data in the notebook, or have 2 notebooks open and do not shut down the kernel, you may get an out of memory error.\nIf you elect to try this on your own data/ROI, you may need to select a larger server size. This will often happen if you are using the last EMIT scene from an orbit. In some cases those can be almost double the size of a normal scene. Please select the smallest possible.\n\n\n\n\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 05-24-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  },
  {
    "objectID": "external/workshop_setup.html#cloning-the-emit-data-resources-repository",
    "href": "external/workshop_setup.html#cloning-the-emit-data-resources-repository",
    "title": "Cloud Workspace Setup",
    "section": "",
    "text": "If you plan to edit or contribute to the EMIT-Data-Resources repository, we recommend following a fork and pull workflow: first fork the repository, then clone your fork to your local machine, make changes, push changes to your fork, then make a pull request back to the main repository. An example can be found in the CONTRIBUTING.md file.\nIf you just want to work with the notebooks or modules, you can simply clone the repository.\nTo clone the repository, navigate to the directory where you want to store the repository on your local machine, then type the following:\ngit clone https://github.com/nasa/EMIT-Data-Resources.git",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  },
  {
    "objectID": "external/workshop_setup.html#troubleshooting",
    "href": "external/workshop_setup.html#troubleshooting",
    "title": "Cloud Workspace Setup",
    "section": "",
    "text": "We recommend Shutting down all kernels after running each notebook. This will clear the memory used by the previous notebook, and is necessary to run some of the more memory intensive notebooks.\n\n\n\nNo single notebook exceeds roughly the limit using the provided data, but if you choose to use your own data in the notebook, or have 2 notebooks open and do not shut down the kernel, you may get an out of memory error.\nIf you elect to try this on your own data/ROI, you may need to select a larger server size. This will often happen if you are using the last EMIT scene from an orbit. In some cases those can be almost double the size of a normal scene. Please select the smallest possible.",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  },
  {
    "objectID": "external/workshop_setup.html#contact-info",
    "href": "external/workshop_setup.html#contact-info",
    "title": "Cloud Workspace Setup",
    "section": "",
    "text": "Email: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 05-24-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "WICSIS 2024 EMIT Access Workshop",
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  },
  {
    "objectID": "guides/bulk_download_using_wget.html",
    "href": "guides/bulk_download_using_wget.html",
    "title": "How to bulk download LP DAAC data using wget",
    "section": "",
    "text": "This guide shows how to bulk download LP DAAC data using wget from the command line. The wget command is a tool developed by the GNU Project to download files from the web. Wget allows you to automate retrieving content and files from web servers using a command-line interface.\n\n\n\nInstall wget. View Frequently Asked Questions About Downloading GNU Wget for more details.\nNASA Earthdata Login credentials are required to access data from all NASA DAACs. You can create an account here."
  },
  {
    "objectID": "guides/bulk_download_using_wget.html#requirements",
    "href": "guides/bulk_download_using_wget.html#requirements",
    "title": "How to bulk download LP DAAC data using wget",
    "section": "",
    "text": "Install wget. View Frequently Asked Questions About Downloading GNU Wget for more details.\nNASA Earthdata Login credentials are required to access data from all NASA DAACs. You can create an account here."
  },
  {
    "objectID": "guides/bulk_download_using_wget.html#step-1-save-the-download-links",
    "href": "guides/bulk_download_using_wget.html#step-1-save-the-download-links",
    "title": "How to bulk download LP DAAC data using wget",
    "section": "Step 1: Save the Download Link(s)",
    "text": "Step 1: Save the Download Link(s)\nSave download links for your data as a text file using Nasa Earthdata Search or Common Metadata Repository (CMR) API. Follow the steps in the Earthdata Search guide to find your data and save the download links. If you prefer to use an API to find your data and save the download links, a tutorial on how to use the CMR API can be found here."
  },
  {
    "objectID": "guides/bulk_download_using_wget.html#step-2-set-up-a-.wgetrc-file-for-authentication",
    "href": "guides/bulk_download_using_wget.html#step-2-set-up-a-.wgetrc-file-for-authentication",
    "title": "How to bulk download LP DAAC data using wget",
    "section": "Step 2: Set up a .wgetrc file for Authentication",
    "text": "Step 2: Set up a .wgetrc file for Authentication\nSet up a .wgetrc file in your home directory.\n\nManual setup:\n\nDownload the .wgetrc template file and save it in your home directory.\n\nOpen the .wgetrc file in a text editor and replace &lt;USERNAME&gt; with your NASA Earthdata Login username and &lt;PASSWORD&gt; with your NASA Earthdata Login password.\n\n\nCreate .wgetrc file from Command Line:\n\nTo Create a .wgetrc file, enter the following in Terminal.\n\nWindows\nNUL &gt;&gt; .wgetrc\nMacOS or Linux\nTo Create a .netrc file, enter the following in the command line.\ntouch .wgetrc | chmod og-rw .wgetrc\n\nTo insert your NASA Earthdata Login username and password into the file, enter the following in the Command Prompt and replace your username and password.\necho http-user=Insert_Your_Username &gt;&gt; .wgetrc | echo http-password=Insert_Your_Password &gt;&gt; .wgetrc"
  },
  {
    "objectID": "guides/bulk_download_using_wget.html#step-3-download-lp-daac-data",
    "href": "guides/bulk_download_using_wget.html#step-3-download-lp-daac-data",
    "title": "How to bulk download LP DAAC data using wget",
    "section": "Step 3: Download LP DAAC Data",
    "text": "Step 3: Download LP DAAC Data\nYou should now be able to run wget commands to download data directly from the LP DAAC. - Navigate to the directory you want to save the data using cd Insert_Your_Directory. - To download a single file, replace the Insert_the_Download_Link in the command below with the URL to the data file you wish to download text   wget Insert_the_Download_Link Example: text   wget https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T12SWF.2023189T175919.v2.0/HLS.S30.T12SWF.2023189T175919.v2.0.B08.tif - To download multiple files, replace Insert_Text_File in the command below with the full path to the text file saved previously in Step 1. text   wget -i Insert_Text_File Example: ```text wget -i data/Granule-DownloadLinks.txt\n```"
  },
  {
    "objectID": "guides/bulk_download_using_wget.html#download-lp-daac-data",
    "href": "guides/bulk_download_using_wget.html#download-lp-daac-data",
    "title": "How to bulk download LP DAAC data using wget",
    "section": "Download LP DAAC Data",
    "text": "Download LP DAAC Data\n\nNavigate to the directory you want to save the data using cd Insert_Your_Directory.\nReplace your Earthdata login username with “Insert_Your_Username” below.\n\nTo download a single file, replace the Insert_the_Download_Link in the command below with the URL to the data file you wish to download\nwget --http-user=Insert_Your_Username --ask-password --keep-session-cookies Insert_the_Download_Link\nExample:\nwget --http-user=MYUSERNAME --ask-password --keep-session-cookies https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T12SWF.2023189T175919.v2.0/HLS.S30.T12SWF.2023189T175919.v2.0.B08.tif\nTo download multiple files, replace Insert_Your_Username with your Earthdata Login username in the command below. Also, replace Insert_Text_File with the full path to the text file saved previously in Step 1. You will be asked to enter your password (i.e., you Earthdata Login password) after running the command. You’ll press enter again to download your files.\nwget --http-user=Insert_Your_Username --ask-password --keep-session-cookies -i Insert_Text_File\nExample:\nwget --http-user=MYUSERNAME --ask-password --keep-session-cookies -i data/Granule-DownloadLinks.txt\nAlternatively, you can replace --ask-password with --http-passwd=Insert_Your_Password and enter your password directly in the command line."
  },
  {
    "objectID": "guides/bulk_download_using_wget.html#contact-info",
    "href": "guides/bulk_download_using_wget.html#contact-info",
    "title": "How to bulk download LP DAAC data using wget",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 07-12-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LP DAAC Data Resources",
    "section": "",
    "text": "This web-book is a place to find resources that demonstrate how to use LP DAAC tools, services, and data, as well as view content presented at previous LP DAAC webinars and workshops. The repository and web-book are still under active development. All notebooks and scripts should be functional, however, changes or additions may be made.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#available-resources",
    "href": "index.html#available-resources",
    "title": "LP DAAC Data Resources",
    "section": "Available Resources",
    "text": "Available Resources\nIn addition to this web-book, Github Repositories containing examples of working with various datasets are available. Eventually we plan to integrate all of those notebooks into this web-book. In the meantime, some general resources can be found in the respotitories listed in the following table. Additionally, there is a separate web-book for the VSWIR Imaging and Thermal Applications, Learning, and Science (VITALS) Repository, which focuses on the compounded benefits of using VSWIR and TIR data together.\nContent include in this repository are listed below.\n\n\n\nRepository Contents\nType\nSummary\n\n\n\n\nData_Discovery_CMR_API_Request.ipynb\nJupyter Notebook\nDemonstrates how to search for Earthdata data collections and granules using CMR API and Request Python package\n\n\nData_Discovery_CMR_API_Bulk_Query.ipynb\nJupyter Notebook\nDemonstrates how to search and extract data URLs for an entire collection using Python’s asyncio package\n\n\nbulk_download_using_curl.md\nMarkdown\nDemonstrates how to bulk download LP DAAC data using Curl from command line\n\n\nbulk_download_using_wget.md\nMarkdown\nDemonstrates how to bulk download LP DAAC data using Wget from command line\n\n\n\nThe other guides, tutorials, how-tos and scripts can be accessed in our mission specific repositories.\n\n\n\nResource Repository\nSummary\nServices and Tools\n\n\n\n\nAppEEARS Data Resources\nHow to use the Application for Extracting and Exploring Analysis Ready Samples (AppEEARS)\nTutorials, AppEEARS API, Direct S3 Access\n\n\nEMIT Data\nHow to find, access, and work with EMIT data (Earth Surface Mineral Dust Source Investigation)\nTutorials, Scripts, Direct S3 Access\n\n\nGEDI Data\nHow to find, access, and work with GEDI data (Global Ecosystem Dynamics Investigation)\nTutorials\n\n\nHLS Data\nHow to find, access, and work with HLS data (Harmonized Landsat Sentinel-2)\nTutorials, Scripts, Direct S3 Access\n\n\nECOSTRESS Data\nHow to find, access, and work with ECOSTRESS data (The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station)\nTutorials, Scripts, Direct S3 Access",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contact-info",
    "href": "index.html#contact-info",
    "title": "LP DAAC Data Resources",
    "section": "Contact Info",
    "text": "Contact Info\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 06-24-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "python/how-tos/Earthdata_Authentication__Create_netrc_file.html",
    "href": "python/how-tos/Earthdata_Authentication__Create_netrc_file.html",
    "title": "Authentication for NASA Earthdata",
    "section": "",
    "text": "This notebook creates a hidden .netrc file containing your Earthdata Login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin &lt;USERNAME&gt;\npassword &lt;PASSWORD&gt;\n&lt;USERNAME&gt; and &lt;PASSWORD&gt; would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "python/how-tos/Earthdata_Authentication__Create_netrc_file.html#summary",
    "href": "python/how-tos/Earthdata_Authentication__Create_netrc_file.html#summary",
    "title": "Authentication for NASA Earthdata",
    "section": "",
    "text": "This notebook creates a hidden .netrc file containing your Earthdata Login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin &lt;USERNAME&gt;\npassword &lt;PASSWORD&gt;\n&lt;USERNAME&gt; and &lt;PASSWORD&gt; would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "python/how-tos/Earthdata_Authentication__Create_netrc_file.html#import-required-packages",
    "href": "python/how-tos/Earthdata_Authentication__Create_netrc_file.html#import-required-packages",
    "title": "Authentication for NASA Earthdata",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\nfrom netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\n\nThe code below will:\n\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\nnetrc_name = \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | echo machine {1} &gt;&gt; {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} &gt;&gt; {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'&gt;&gt; {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n    # Set restrictive permissions\n    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n\n    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} &gt;&gt; {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} &gt;&gt; {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'&gt;&gt; {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\n\nSee if the file was created\nIf the file was created, we’ll see a .netrc file in the list printed below.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n!ls -al ~/"
  },
  {
    "objectID": "python/how-tos/Earthdata_Cloud__List_bucket_objects.html",
    "href": "python/how-tos/Earthdata_Cloud__List_bucket_objects.html",
    "title": "Listing Objects in S3 Using to boto3",
    "section": "",
    "text": "import boto3\nimport requests\nfrom getpass import getpass"
  },
  {
    "objectID": "python/how-tos/Earthdata_Cloud__List_bucket_objects.html#enter-earthdata-login-credentials",
    "href": "python/how-tos/Earthdata_Cloud__List_bucket_objects.html#enter-earthdata-login-credentials",
    "title": "Listing Objects in S3 Using to boto3",
    "section": "Enter Earthdata Login Credentials",
    "text": "Enter Earthdata Login Credentials\n\nuser = getpass(prompt='Enter your NASA Earthdata Login Username')\npassword = getpass(prompt='Enter your NASA Earthdata Login Password')"
  },
  {
    "objectID": "python/how-tos/Earthdata_Cloud__List_bucket_objects.html#get-earthdata-cloud-temporary-credentials",
    "href": "python/how-tos/Earthdata_Cloud__List_bucket_objects.html#get-earthdata-cloud-temporary-credentials",
    "title": "Listing Objects in S3 Using to boto3",
    "section": "Get Earthdata Cloud Temporary Credentials",
    "text": "Get Earthdata Cloud Temporary Credentials\n\nurl = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\nurl = requests.get(url, allow_redirects=False).headers['Location']\ncreds = requests.get(url, auth=(user, password)).json()"
  },
  {
    "objectID": "python/how-tos/Earthdata_Cloud__List_bucket_objects.html#create-a-boto3-session",
    "href": "python/how-tos/Earthdata_Cloud__List_bucket_objects.html#create-a-boto3-session",
    "title": "Listing Objects in S3 Using to boto3",
    "section": "Create a boto3 Session",
    "text": "Create a boto3 Session\nWe will use a session to store our S3 credentials and other configurations options. Our session will be used to create a boto3 client which act as our interface to AWS services used to, for example, download files or list objects in S3 specified S3 buckets.\nNOTE, it is important to specify the prefix and delimiter parameter options. The list_object_v2 methods will fail without those options being specified.\n\nsession = boto3.Session(aws_access_key_id=creds['accessKeyId'], \n                        aws_secret_access_key=creds['secretAccessKey'], \n                        aws_session_token=creds['sessionToken'], \n                        region_name='us-west-2')\nclient = session.client('s3')\nbucket = 'lp-prod-protected'\nprefix = ''\ndelimiter = '/'\n\nNow we can list all of the collections within the lp-prod-protected bucket.\n\nbucket_list=client.list_objects_v2(Bucket=bucket, Prefix=prefix, Delimiter=delimiter)\nbucket_list\n\nbucket_list is a dictionary where all available collections can be found with the CommonPrefixes key.\n\nbucket_list.keys()\n\nWe can use the CommonPrefixes key to pull all of the collections into a list.\n\n# Check for common prefixes (directories) found\nif 'CommonPrefixes' not in bucket_list:\n    print ('No directories found')\nelse:\n    dir_list=[]\n    for dir_name in bucket_list['CommonPrefixes']:\n        dir_list.append('%s ' % (dir_name['Prefix']))\n        print(dir_name['Prefix'])    \n\nprint('Dir count = ',len(dir_list))\n\nTo see what is contained within each collection, we’ll update the Prefix option to include the a collection name.\n\nprefix = \"ECO_L2_LSTE.002/\"\n\n\ncol_prefix =client.list_objects_v2(Bucket=bucket, Prefix=prefix, Delimiter=delimiter)\n#col_prefix\n\n\ndir_list=[]\nfor dir_name in col_prefix['CommonPrefixes']:\n            dir_list.append(f\"{dir_name['Prefix']}\")\n\nYou’ll notice that the list of Prefixes (or granules) is quite long. The list_objects_v2 method will return only 1000 objects by default. Often collections include well over 1000 granules. We can set up some code the ‘page’ through the entire collection and add the granules to dir_list.\n\n# If the list is longer than the returned list (&gt;1000) ask about pagination\nif col_prefix['IsTruncated'] :\n    cont = input('Continue (Y/n):')\n\n# Paginate\n#while col_prefix['IsTruncated'] and ( cont == 'Y' or cont == 'y' or cont == ''):\nwhile 'NextContinuationToken' in col_prefix:\n    continuation = col_prefix['NextContinuationToken']\n    col_prefix = client.list_objects_v2(Bucket=bucket, Prefix=prefix, Delimiter=delimiter, ContinuationToken=continuation)\n\n    # List Directories\n    if 'CommonPrefixes' not in col_prefix:\n        print ('No directories found')\n    else:\n        for dir_name in col_prefix['CommonPrefixes']:\n            dir_list.append(f\"{dir_name['Prefix']}\")\n            #print(dir_name['Prefix'])        \n            #print('Dir count = ',len(dir_list))\n\n    #cont = input('Continue (Y/n):')\n\nWe now have a list of Prefixes (granule paths) that we can use to find files\n\nlen(dir_list)\n\n\ndir_list[:10]    # Print the first 10\n\nWe can find the files by updating the prefix again. This time we’ll use the path from our dir_list to list the files associated with the first item in our list.\n\nprefix = dir_list[0]\n\n\nfiles = client.list_objects_v2(Bucket=bucket, Prefix=prefix, Delimiter=delimiter)\n\n\nfiles\n\nThere are many files associated with this granule. Now we can get the key to a data asset in S3.\n\n[f['Key'] for f in files['Contents'] if f['Key'].endswith('.h5')]"
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Request.html",
    "href": "python/tutorials/Data_Discovery_CMR_API_Request.html",
    "title": "Data Discovery with NASA’s CMR Using Request Python Package",
    "section": "",
    "text": "In this notebook, we will walk through how to search for Earthdata data collections and granules. Along the way we will explore the available search parameters, information return, and specific contrains when using the CMR API. Our object is to identify assets to access that we would downloaded, or perform S3 direct access, within an analysis workflow"
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Request.html#summary",
    "href": "python/tutorials/Data_Discovery_CMR_API_Request.html#summary",
    "title": "Data Discovery with NASA’s CMR Using Request Python Package",
    "section": "",
    "text": "In this notebook, we will walk through how to search for Earthdata data collections and granules. Along the way we will explore the available search parameters, information return, and specific contrains when using the CMR API. Our object is to identify assets to access that we would downloaded, or perform S3 direct access, within an analysis workflow"
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Request.html#learning-objectives",
    "href": "python/tutorials/Data_Discovery_CMR_API_Request.html#learning-objectives",
    "title": "Data Discovery with NASA’s CMR Using Request Python Package",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand what CMR/CMR API is and what CMR/CMR API can be used for\nHow to use the requests package to search data collections and granules\nHow to parse the results of these searches."
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Request.html#what-is-cmr",
    "href": "python/tutorials/Data_Discovery_CMR_API_Request.html#what-is-cmr",
    "title": "Data Discovery with NASA’s CMR Using Request Python Package",
    "section": "What is CMR",
    "text": "What is CMR\nCMR is the Common Metadata Repository. It catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). It is the backend of Earthdata Search, the GUI search interface. More information about CMR can be found here.\nUnfortunately, the GUI for Earthdata Search is not accessible from a cloud instance - at least not without some work. Earthdata Search is also not immediately reproducible. What I mean by that is if you create a search using the GUI you would have to note the search criteria (date range, search area, collection name, etc), take a screenshot, copy the search url, or save the list of data granules returned by the search, in order to recreate the search. This information would have to be re-entered each time you or someone else wanted to do the search. You could make typos or other mistakes. A cleaner, reproducible solution is to search CMR programmatically using the CMR API."
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Request.html#what-is-the-cmr-api",
    "href": "python/tutorials/Data_Discovery_CMR_API_Request.html#what-is-the-cmr-api",
    "title": "Data Discovery with NASA’s CMR Using Request Python Package",
    "section": "What is the CMR API",
    "text": "What is the CMR API\nAPI stands for Application Programming Interface. It allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results."
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Request.html#getting-started-how-to-search-cmr-from-python",
    "href": "python/tutorials/Data_Discovery_CMR_API_Request.html#getting-started-how-to-search-cmr-from-python",
    "title": "Data Discovery with NASA’s CMR Using Request Python Package",
    "section": "Getting Started: How to search CMR from Python",
    "text": "Getting Started: How to search CMR from Python\nThe first step is to import python packages. We will use:\n- requests This package does most of the work for us accessing the CMR API using HTTP methods. - pprint to pretty print the results of the search.\nA more in-depth tutorial on requests is here\n\nimport requests\nimport json\nfrom pprint import pprint\n\nTo conduct a search using the CMR API, requests needs the url for the root CMR search endpoint. We’ll assign this url to a python variable as a string.\n\nCMR_OPS = 'https://cmr.earthdata.nasa.gov/search'"
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Request.html#query-cmr-for-cloud-hosted-collections",
    "href": "python/tutorials/Data_Discovery_CMR_API_Request.html#query-cmr-for-cloud-hosted-collections",
    "title": "Data Discovery with NASA’s CMR Using Request Python Package",
    "section": "Query CMR for Cloud Hosted Collections",
    "text": "Query CMR for Cloud Hosted Collections\nIn this tutorial, we use different search parameters to search for collections in different ways. Below, we want to retrieve the collections that are hosted in the cloud ('cloud_hosted': 'True') that has granules availble ('has_granules': 'True'). We also want to get the content in json (pronounced “jason”) format, so I pass a dictionary to the header keyword argument to say that I want results returned as json ('Accept': 'application/json').\nThe .get() method is used to send this information to the CMR API. get() calls the HTTP method GET.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                        },\n                        headers={\n                            'Accept': 'application/json'\n                        }\n                       )\n\nThe request returns a Response object.\nTo check that our request was successful we can print the response variable we saved the request to.\n\nresponse\n\n&lt;Response [200]&gt;\n\n\nA 200 response is what we want. This means that the requests was successful. For more information on HTTP status codes see https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\nA more explict way to check the status code is to use the status_code attribute. Both methods return a HTTP status code.\n\nresponse.status_code\n\n200\n\n\nThe response from requests.get returns the results of the search and metadata about those results in the headers.\nMore information about the response object can be found by typing help(response).\nheaders contains useful information in a case-insensitive dictionary. We requested (above) that the information be return in json which means the object return is a dictionary in our Python environment. We’ll iterate through the returned dictionary, looping throught each field (k) and its associated value (v). For more on interating through dictionary object click here.\n\nfor k, v in response.headers.items():\n    print(f'{k}: {v}')\n\nContent-Type: application/json;charset=utf-8\nTransfer-Encoding: chunked\nConnection: keep-alive\nDate: Wed, 05 Jul 2023 21:47:57 GMT\nX-Frame-Options: SAMEORIGIN\nAccess-Control-Allow-Origin: *\nX-XSS-Protection: 1; mode=block\nCMR-Request-Id: 6a23e6cd-89b8-4acb-9ce8-932e1028f7c3\nStrict-Transport-Security: max-age=31536000\nCMR-Search-After: [0.0,10400.0,\"VNP03IMG\",\"2\",2105092163,2]\nCMR-Hits: 2772\nAccess-Control-Expose-Headers: CMR-Hits, CMR-Request-Id, X-Request-Id, CMR-Scroll-Id, CMR-Search-After, CMR-Timed-Out, CMR-Shapefile-Original-Point-Count, CMR-Shapefile-Simplified-Point-Count\nX-Content-Type-Options: nosniff\nCMR-Took: 1418\nX-Request-Id: D4zM66TwPjOtdYSk2YmXO74wtETxxxDNDE_ItkfC5JUVmLglIpN6ig==\nVary: Accept-Encoding, User-Agent\nContent-Encoding: gzip\nServer: ServerTokens ProductOnly\nX-Cache: Miss from cloudfront\nVia: 1.1 7cc224be3664680df186a12039cdc424.cloudfront.net (CloudFront)\nX-Amz-Cf-Pop: MSP50-P2\nX-Amz-Cf-Id: D4zM66TwPjOtdYSk2YmXO74wtETxxxDNDE_ItkfC5JUVmLglIpN6ig==\n\n\nEach item in the dictionary can be accessed in the normal way you access a python dictionary but the keys uniquely case-insensitive. Let’s take a look at the commonly used CMR-Hits key.\n\nresponse.headers['CMR-Hits']\n\n'2772'\n\n\nNote that “cmr-hits” works as well!\n\nresponse.headers['cmr-hits']\n\n'2772'"
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Request.html#query-cmr-using-provider",
    "href": "python/tutorials/Data_Discovery_CMR_API_Request.html#query-cmr-using-provider",
    "title": "Data Discovery with NASA’s CMR Using Request Python Package",
    "section": "Query CMR Using Provider",
    "text": "Query CMR Using Provider\nIn some situations the response to your query can return a very large number of result, some of which may not be relevant. We can add additional query parameters to restrict the information returned. We’re going to restrict the search by the provider parameter.\nYou can modify the code below to explore all Earthdata data products hosted by the various providers. When searching by provider, use Cloud Provider to search for cloud-hosted datasets and On-Premises Provider to search for datasets archived at the DAACs. A partial list of providers is given below.\n\n\n\n\n\n\n\n\n\nDAAC\nShort Name\nCloud Provider\nOn-Premises Provider\n\n\n\n\nNSIDC\nNational Snow and Ice Data Center\nNSIDC_CPRD\nNSIDC_ECS\n\n\nGHRC DAAC\nGlobal Hydrometeorology Resource Center\nGHRC_DAAC\nGHRC_DAAC\n\n\nPO DAAC\nPhysical Oceanography Distributed Active Archive Center\nPOCLOUD\nPODAAC\n\n\nASF\nAlaska Satellite Facility\nASF\nASF\n\n\nORNL DAAC\nOak Ridge National Laboratory\nORNL_CLOUD\nORNL_DAAC\n\n\nLP DAAC\nLand Processes Distributed Active Archive Center\nLPCLOUD\nLPDAAC_ECS\n\n\nGES DISC\nNASA Goddard Earth Sciences (GES) Data and Information Services Center (DISC)\nGES_DISC\nGES_DISC\n\n\nOB DAAC\nNASA’s Ocean Biology Distributed Active Archive Center\n\nOB_DAAC\n\n\nSEDAC\nNASA’s Socioeconomic Data and Applications Center\n\nSEDAC\n\n\n\nWe’ll assign the provider to a variable as a string and insert the variable into the parameter argument in the request.\n\nprovider = 'LPCLOUD'\n\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                        },\n                        headers= {\n                            'Accept': 'application/json'\n                            }\n                       )\nresponse\n\n&lt;Response [200]&gt;\n\n\nLet’s see how many collections are available through LPCOUD provider.\n\nresponse.headers['cmr-hits']\n\n'209'\n\n\nSearch results are contained in the content part of the Response object. However, response.content returns information in bytes.\n\nresponse.content\n\nb'{\"feed\":{\"updated\":\"2023-07-05T21:47:59.100Z\",\"id\":\"https://cmr.earthdata.nasa.gov:443/search/collections.json?cloud_hosted=True&has_granules=True&provider=LPCLOUD\",\"title\":\"ECHO dataset metadata\",\"entry\":[{\"processing_level_id\":\"3\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2013-04-11T00:00:00.000Z\",\"version_id\":\"2.0\",\"updated\":\"2015-12-03T10:57:07.000Z\",\"dataset_id\":\"HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"associations\":{\"tools\":[\"TL1860232272-LPDAAC_ECS\"]},\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"HLSL30\",\"organizations\":[\"LP DAAC\",\"NASA/IMPACT\"],\"title\":\"HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The Harmonized Landsat Sentinel-2 (HLS) project provides consistent surface reflectance (SR) and top of atmosphere (TOA) brightness data from a virtual constellation of satellite sensors. The Operational Land Imager (OLI) is housed aboard the joint NASA/USGS Landsat 8 and Landsat 9 satellites, while the Multi-Spectral Instrument (MSI) is mounted aboard Europe\\xe2\\x80\\x99s Copernicus Sentinel-2A and Sentinel-2B satellites. The combined measurement enables global observations of the land every 2\\xe2\\x80\\x933 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment.\\\\r\\\\n\\\\r\\\\nThe HLSL30 product provides 30-m Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Landsat 8/9 OLI data products. The HLSS30 and HLSL30 products are gridded to the same resolution and Military Grid Reference System (MGRS)(https://hls.gsfc.nasa.gov/products-description/tiling-system/) tiling system, and thus are \\xe2\\x80\\x9cstackable\\xe2\\x80\\x9d for time series analysis.\\\\r\\\\n\\\\r\\\\nThe HLSL30 product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate file. There are 11 bands included in the HLSL30 product along with one quality assessment (QA) band and four angle bands. See the User Guide for a more detailed description of the individual bands provided in the HLSL30 product.\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2021957657-LPCLOUD\",\"has_formats\":false,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"LANDSAT-8\",\"LANDSAT-9\"],\"association_details\":{\"tools\":[{\"concept_id\":\"TL1860232272-LPDAAC_ECS\"}]},\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2021957657-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/HLS/HLSL30.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1326/HLS_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/769/HLS_ATBD_V15_provisional.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1117/HLS_Quick_Guide_v02.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-super-script/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G2095313663-LPCLOUD?h=512&w=512\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-bulk-download/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/resources/e-learning/getting-started-with-cloud-native-harmonized-landsat-sentinel-2-hls-data-in-r/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"}]},{\"processing_level_id\":\"3\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2015-11-28T00:00:00.000Z\",\"version_id\":\"2.0\",\"updated\":\"2020-03-04T07:19:53.396Z\",\"dataset_id\":\"HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"associations\":{\"tools\":[\"TL1860232272-LPDAAC_ECS\"]},\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"HLSS30\",\"organizations\":[\"LP DAAC\",\"NASA/IMPACT\"],\"title\":\"HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The Harmonized Landsat Sentinel-2 (HLS) project provides consistent surface reflectance data from the Operational Land Imager (OLI) aboard the joint NASA/USGS Landsat 8 satellite and the Multi-Spectral Instrument (MSI) aboard Europe\\xe2\\x80\\x99s Copernicus Sentinel-2A and Sentinel-2B satellites. The combined measurement enables global observations of the land every 2\\xe2\\x80\\x933 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment. \\\\r\\\\n\\\\r\\\\nThe HLSS30 product provides 30-m Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Sentinel-2A and Sentinel-2B MSI data products. The HLSS30 and HLSL30 products are gridded to the same resolution and Military Grid Reference System (MGRS) (https://hls.gsfc.nasa.gov/products-description/tiling-system/) tiling system, and thus are \\xe2\\x80\\x9cstackable\\xe2\\x80\\x9d for time series analysis.\\\\r\\\\n\\\\r\\\\nThe HLSS30 product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate COG. There are 13 bands included in the HLSS30 product along with four angle bands and a quality assessment (QA) band. See the User Guide for a more detailed description of the individual bands provided in the HLSS30 product.\\\\r\\\\n\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2021957295-LPCLOUD\",\"has_formats\":false,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"Sentinel-2A\",\"Sentinel-2B\"],\"association_details\":{\"tools\":[{\"concept_id\":\"TL1860232272-LPDAAC_ECS\"}]},\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2021957295-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/HLS/HLSS30.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1326/HLS_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/769/HLS_ATBD_V15_provisional.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1117/HLS_Quick_Guide_v02.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-super-script/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G2095548655-LPCLOUD?h=512&w=512\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-bulk-download/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/resources/e-learning/getting-started-with-cloud-native-harmonized-landsat-sentinel-2-hls-data-in-r/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"}]},{\"processing_level_id\":\"3\",\"cloud_hosted\":true,\"boxes\":[\"-83 -180 82 180\"],\"time_start\":\"2000-03-01T00:00:00.000Z\",\"version_id\":\"003\",\"updated\":\"2015-09-30T10:42:35.418Z\",\"dataset_id\":\"ASTER Global Digital Elevation Model V003\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ASTGTM\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ASTER\"],\"title\":\"ASTER Global Digital Elevation Model V003\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ASTER Global Digital Elevation Model (GDEM) Version 3 (ASTGTM) provides a global digital elevation model (DEM) of land areas on Earth at a spatial resolution of 1 arc second (approximately 30 meter horizontal posting at the equator).\\\\r\\\\n\\\\r\\\\nThe development of the ASTER GDEM data products is a collaborative effort between National Aeronautics and Space Administration (NASA) and Japan\\xe2\\x80\\x99s Ministry of Economy, Trade, and Industry (METI). The ASTER GDEM data products are created by the Sensor Information Laboratory Corporation (SILC) in Tokyo. \\\\r\\\\n\\\\r\\\\nThe ASTER GDEM Version 3 data product was created from the automated processing of the entire ASTER Level 1A (https://doi.org/10.5067/ASTER/AST_L1A.003) archive of scenes acquired between March 1, 2000, and November 30, 2013. Stereo correlation was used to produce over one million individual scene based ASTER DEMs, to which cloud masking was applied. All cloud screened DEMs and non-cloud screened DEMs were stacked. Residual bad values and outliers were removed. In areas with limited data stacking, several existing reference DEMs were used to supplement ASTER data to correct for residual anomalies. Selected data were averaged to create final pixel values before partitioning the data into 1 degree latitude by 1 degree longitude tiles with a one pixel overlap. To correct elevation values of water body surfaces, the ASTER Global Water Bodies Database (ASTWBD) (https://doi.org/10.5067/ASTER/ASTWBD.001) Version 1 data product was also generated. \\\\r\\\\n\\\\r\\\\nThe geographic coverage of the ASTER GDEM extends from 83\\xc2\\xb0 North to 83\\xc2\\xb0 South. Each tile is distributed in GeoTIFF format and projected on the 1984 World Geodetic System (WGS84)/1996 Earth Gravitational Model (EGM96) geoid. Each of the 22,912 tiles in the collection contain at least 0.01% land area. \\\\r\\\\n\\\\r\\\\nProvided in the ASTER GDEM product are layers for DEM and number of scenes (NUM). The NUM layer indicates the number of scenes that were processed for each pixel and the source of the data.\\\\r\\\\n\\\\r\\\\nWhile the ASTER GDEM Version 3 data products offer substantial improvements over Version 2, users are advised that the products still may contain anomalies and artifacts that will reduce its usability for certain applications. \\\\r\\\\n\\\\r\\\\nImprovements/Changes from Previous Versions \\\\r\\\\n\\xe2\\x80\\xa2 Expansion of acquisition coverage to increase the amount of cloud-free input scenes from about 1.5 million in Version 2 to about 1.88 million scenes in Version 3.\\\\r\\\\n\\xe2\\x80\\xa2 Separation of rivers from lakes in the water body processing. \\\\r\\\\n\\xe2\\x80\\xa2 Minimum water body detection size decreased from 1 km2 to 0.2 km2. \",\"time_end\":\"2013-11-30T23:59:59.999Z\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1711961296-LPCLOUD\",\"has_formats\":false,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"Terra\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q= C1711961296-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ASTER/ASTGTM.003\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://asterweb.jpl.nasa.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/434/ASTGTM_User_Guide_V3.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ASTGTM.003/ASTGTMV003_N03E021.1.jpg\"}]},{\"processing_level_id\":\"3\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2002-07-04T00:00:00.000Z\",\"version_id\":\"061\",\"updated\":\"2016-01-20T09:38:26.775Z\",\"dataset_id\":\"MODIS/Aqua Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid V061\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"MYD11A1\",\"organizations\":[\"LP DAAC\",\"NASA/GSFC/SED/ESD/TISL/MODAPS\"],\"title\":\"MODIS/Aqua Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid V061\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The MYD11A1 Version 6.1 product provides daily per-pixel Land Surface Temperature and Emissivity (LST&E) with 1 kilometer (km) spatial resolution in a 1,200 by 1,200 km grid. The pixel temperature value is derived from the MYD11_L2 (https://doi.org/10.5067/MODIS/MYD11_L2.061) swath product. Above 30 degrees latitude, some pixels may have multiple observations where the criteria for clear-sky are met. When this occurs, the pixel value is a result of the average of all qualifying observations. Provided along with the daytime and nighttime surface temperature bands are associated quality control assessments, observation times, view zenith angles, and clear-sky coverages along with bands 31 and 32 emissivities from land cover types. Validation at stage 2 (https://modis-land.gsfc.nasa.gov/MODLAND_val.html) has been achieved for all MODIS Land Surface Temperature and Emissivity products. Further details regarding MODIS land product validation for the MYD11 data products are available from the MODIS Land Team Validation site (https://modis-land.gsfc.nasa.gov/ValStatus.php?ProductID=MOD11). Improvements/Changes from Previous Versions * The Version 6.1 Level-1B (L1B) products have been improved by undergoing various calibration changes that include: changes to the response-versus-scan angle (RVS) approach that affects reflectance bands for Aqua and Terra MODIS, corrections to adjust for the optical crosstalk in Terra MODIS infrared (IR) bands, and corrections to the Terra MODIS forward look-up table (LUT) update for the period 2012 - 2017. * A polarization correction has been applied to the L1B Reflective Solar Bands (RSB). \",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1748046084-LPCLOUD\",\"has_formats\":false,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"Aqua\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/MODIS/MYD11A1.061\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://e4ftl01.cr.usgs.gov/MOLA/MYD11A1.061/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search/granules?p=C1748046084-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://earthexplorer.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/service#\",\"hreflang\":\"en-US\",\"href\":\"https://opendap.cr.usgs.gov/opendap/hyrax/DP128/MOLA/MYD11A1.061/contents.html\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/119/MOD11_ATBD.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/715/MOD11_User_Guide_V61.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://ladsweb.modaps.eosdis.nasa.gov/filespec/MODIS/61/MYD11A1\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://modis-land.gsfc.nasa.gov/ValStatus.php?ProductID=MOD11\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://modis-land.gsfc.nasa.gov/MODLAND_val.html\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://landweb.modaps.eosdis.nasa.gov/cgi-bin/QA_WWW/qaFlagPage.cgi?sat=aqua&ver=C6\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G2357187151-LPCLOUD?h=500&w=500\"}]},{\"processing_level_id\":\"3\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2000-02-18T00:00:00.000Z\",\"version_id\":\"061\",\"updated\":\"2015-09-30T10:47:59.761Z\",\"dataset_id\":\"MODIS/Terra Vegetation Indices 16-Day L3 Global 250m SIN Grid V061\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"MOD13Q1\",\"organizations\":[\"LP DAAC\",\"NASA/GSFC/SED/ESD/TISL/MODAPS\"],\"title\":\"MODIS/Terra Vegetation Indices 16-Day L3 Global 250m SIN Grid V061\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The Terra Moderate Resolution Imaging Spectroradiometer (MODIS) Vegetation Indices (MOD13Q1) Version 6.1 data are generated every 16 days at 250 meter (m) spatial resolution as a Level 3 product. The MOD13Q1 product provides two primary vegetation layers. The first is the Normalized Difference Vegetation Index (NDVI) which is referred to as the continuity index to the existing National Oceanic and Atmospheric Administration-Advanced Very High Resolution Radiometer (NOAA-AVHRR) derived NDVI. The second vegetation layer is the Enhanced Vegetation Index (EVI), which has improved sensitivity over high biomass regions. The algorithm chooses the best available pixel value from all the acquisitions from the 16 day period. The criteria used is low clouds, low view angle, and the highest NDVI/EVI value.\\\\n\\\\nAlong with the vegetation layers and the two quality layers, the HDF file will have MODIS reflectance bands 1 (red), 2 (near-infrared), 3 (blue), and 7 (mid-infrared), as well as four observation layers. \\\\n\\\\nValidation at stage 3 (https://landweb.modaps.eosdis.nasa.gov/cgi-bin/QA_WWW/newPage.cgi?fileName=maturity) has been achieved for all MOD13 vegetation products. Further details regarding product validation for the MOD13Q1 data product is available from the MODIS land team validation site (https://landval.gsfc.nasa.gov/ProductStatus.php?ProductID=MOD13).\\\\n\\\\nImprovements/Changes from Previous Versions\\\\n\\\\n* The Version 6.1 Level-1B (L1B) products have been improved by undergoing various calibration changes that include: changes to the response-versus-scan angle (RVS) approach that affects reflectance bands for Aqua and Terra MODIS, corrections to adjust for the optical crosstalk in Terra MODIS infrared (IR) bands, and corrections to the Terra MODIS forward look-up table (LUT) update for the period 2012 - 2017.\\\\n* A polarization correction has been applied to the L1B Reflective Solar Bands (RSB).\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1748066515-LPCLOUD\",\"has_formats\":false,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"Terra\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/MODIS/MOD13Q1.061\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://e4ftl01.cr.usgs.gov/MOLT/MOD13Q1.061/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search/granules?p=C1748066515-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://earthexplorer.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/service#\",\"hreflang\":\"en-US\",\"href\":\"https://opendap.cr.usgs.gov/opendap/hyrax/MOD13Q1.061/contents.html\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/104/MOD13_ATBD.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/621/MOD13_User_Guide_V61.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://ladsweb.modaps.eosdis.nasa.gov/filespec/MODIS/61/MOD13Q1\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://modis-land.gsfc.nasa.gov/ValStatus.php?ProductID=MOD13Q1\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://modis-land.gsfc.nasa.gov/MODLAND_val.html\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://landweb.modaps.eosdis.nasa.gov/cgi-bin/QA_WWW/qaFlagPage.cgi?sat=terra&ver=C6\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G2346280125-LPCLOUD?h=500&w=500\"}]},{\"processing_level_id\":\"2G\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2000-02-24T00:00:00.000Z\",\"version_id\":\"061\",\"updated\":\"2015-09-30T10:47:32.717Z\",\"dataset_id\":\"MODIS/Terra Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V061\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"MOD09GA\",\"organizations\":[\"LP DAAC\",\"NASA/GSFC/SED/ESD/TISL/MODAPS\"],\"title\":\"MODIS/Terra Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V061\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The MOD09GA Version 6.1 product provides an estimate of the surface spectral reflectance of Terra Moderate Resolution Imaging Spectroradiometer (MODIS) Bands 1 through 7, corrected for atmospheric conditions such as gasses, aerosols, and Rayleigh scattering. Provided along with the 500 meter (m) surface reflectance, observation, and quality bands are a set of ten 1 kilometer (km) observation bands and geolocation flags. The reflectance layers from the MOD09GA are used as the source data for many of the MODIS land products. \\\\n\\\\nValidation at stage 3 (https://modis-land.gsfc.nasa.gov/MODLAND_val.html) has been achieved for the MODIS Surface Reflectance products. Further details regarding MODIS land product validation for the MOD09 data product is available from the MODIS Land Team Validation site (https://modis-land.gsfc.nasa.gov/ValStatus.php?ProductID=MOD09).\\\\n\\\\n Improvements/Changes from Previous Versions\\\\n\\\\n* The Version 6.1 Level-1B (L1B) products have been improved by undergoing various calibration changes that include: changes to the response-versus-scan angle (RVS) approach that affects reflectance bands for Aqua and Terra MODIS, corrections to adjust for the optical crosstalk in Terra MODIS infrared (IR) bands, and corrections to the Terra MODIS forward look-up table (LUT) update for the period 2012 - 2017.\\\\n* A polarization correction has been applied to the L1B Reflective Solar Bands (RSB).\\\\n\\\\n\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2202497474-LPCLOUD\",\"has_formats\":false,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"Terra\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/MODIS/MOD09GA.061\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://e4ftl01.cr.usgs.gov/MOLT/MOD09GA.061/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search/granules?p=C2202497474-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://earthexplorer.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/service#\",\"hreflang\":\"en-US\",\"href\":\"https://opendap.cr.usgs.gov/opendap/hyrax/MOD09GA.061/contents.html\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/305/MOD09_ATBD.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/925/MOD09_User_Guide_V61.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://ladsweb.modaps.eosdis.nasa.gov/filespec/MODIS/61/MOD09GA\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://modis-land.gsfc.nasa.gov/ValStatus.php?ProductID=MOD09\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://modis-land.gsfc.nasa.gov/MODLAND_val.html\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://landweb.modaps.eosdis.nasa.gov/cgi-bin/QA_WWW/qaFlagPage.cgi?sat=terra&ver=C6\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G2358429497-LPCLOUD?h=500&w=500\"}]},{\"processing_level_id\":\"2G\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2000-02-24T00:00:00.000Z\",\"version_id\":\"061\",\"updated\":\"2016-03-21T09:32:15.906Z\",\"dataset_id\":\"MODIS/Terra Surface Reflectance Daily L2G Global 250m SIN Grid V061\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"MOD09GQ\",\"organizations\":[\"LP DAAC\",\"NASA/GSFC/SED/ESD/TISL/MODAPS\"],\"title\":\"MODIS/Terra Surface Reflectance Daily L2G Global 250m SIN Grid V061\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The MOD09GQ Version 6.1 product provides an estimate of the surface spectral reflectance of Terra Moderate Resolution Imaging Spectroradiometer (MODIS) 250 meter (m) bands 1 and 2, corrected for atmospheric conditions such as gasses, aerosols, and Rayleigh scattering. Along with the 250 m surface reflectance bands are the Quality Assurance (QA) layer and five observation layers. This product is intended to be used in conjunction with the quality and viewing geometry information of the 500 m product (MOD09GA). \\\\n\\\\nValidation at stage 3 (https://modis-land.gsfc.nasa.gov/MODLAND_val.html) has been achieved for the MODIS Surface Reflectance products. Further details regarding MODIS land product validation for the MOD09 data product is available from the MODIS Land Team Validation site (https://modis-land.gsfc.nasa.gov/ValStatus.php?ProductID=MOD09).\\\\n\\\\nImprovements/Changes from Previous Versions\\\\n\\\\n* The Version 6.1 Level-1B (L1B) products have been improved by undergoing various calibration changes that include: changes to the response-versus-scan angle (RVS) approach that affects reflectance bands for Aqua and Terra MODIS, corrections to adjust for the optical crosstalk in Terra MODIS infrared (IR) bands, and corrections to the Terra MODIS forward look-up table (LUT) update for the period 2012 - 2017.\\\\n* A polarization correction has been applied to the L1B Reflective Solar Bands (RSB).\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2343115666-LPCLOUD\",\"has_formats\":false,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"Terra\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://e4ftl01.cr.usgs.gov/MOLT/MOD09GQ.061/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2343115666-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://earthexplorer.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/MODIS/MOD09GQ.061\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/925/MOD09_User_Guide_V61.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/305/MOD09_ATBD.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://ladsweb.modaps.eosdis.nasa.gov/filespec/MODIS/61/MOD09GQ\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://modis-land.gsfc.nasa.gov/MODLAND_val.html\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://modis-land.gsfc.nasa.gov/ValStatus.php?ProductID=MOD09\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/service#\",\"hreflang\":\"en-US\",\"href\":\"https://opendap.cr.usgs.gov/opendap/hyrax/DP124/MOLT/MOD09GQ.061/contents.html\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G2509506318-LPCLOUD?h=85&w=85\"}]},{\"processing_level_id\":\"3\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2000-02-24T00:00:00.000Z\",\"version_id\":\"061\",\"updated\":\"2015-09-30T10:47:37.333Z\",\"dataset_id\":\"MODIS/Terra Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid V061\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"MOD11A1\",\"organizations\":[\"LP DAAC\",\"NASA/GSFC/SED/ESD/TISL/MODAPS\"],\"title\":\"MODIS/Terra Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid V061\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The MOD11A1 Version 6.1 product provides daily per-pixel Land Surface Temperature and Emissivity (LST&E) with 1 kilometer (km) spatial resolution in a 1,200 by 1,200 km grid. The pixel temperature value is derived from the MOD11_L2 (https://doi.org/10.5067/MODIS/MOD11_L2.006) swath product. Above 30 degrees latitude, some pixels may have multiple observations where the criteria for clear-sky are met. When this occurs, the pixel value is a result of the average of all qualifying observations. Provided along with the daytime and nighttime surface temperature bands are associated quality control assessments, observation times, view zenith angles, and clear-sky coverages along with bands 31 and 32 emissivities from land cover types. Validation at stage 2 (https://modis-land.gsfc.nasa.gov/MODLAND_val.html) has been achieved for all MODIS Land Surface Temperature and Emissivity products. Further details regarding MODIS land product validation for the MOD11 data products are available from the MODIS Land Team Validation site (https://modis-land.gsfc.nasa.gov/ValStatus.php?ProductID=MOD11). Improvements/Changes from Previous Versions * The Version 6.1 Level-1B (L1B) products have been improved by undergoing various calibration changes that include: changes to the response-versus-scan angle (RVS) approach that affects reflectance bands for Aqua and Terra MODIS, corrections to adjust for the optical crosstalk in Terra MODIS infrared (IR) bands, and corrections to the Terra MODIS forward look-up table (LUT) update for the period 2012 - 2017. * A polarization correction has been applied to the L1B Reflective Solar Bands (RSB). \",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1748058432-LPCLOUD\",\"has_formats\":false,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"Terra\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/MODIS/MOD11A1.061\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://e4ftl01.cr.usgs.gov/MOLT/MOD11A1.061/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search/granules?p=C1748058432-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://earthexplorer.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/service#\",\"hreflang\":\"en-US\",\"href\":\"https://opendap.cr.usgs.gov/opendap/hyrax/DP128/MOLT/MOD11A1.061/contents.html\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/119/MOD11_ATBD.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/715/MOD11_User_Guide_V61.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://ladsweb.modaps.eosdis.nasa.gov/filespec/MODIS/61/MOD11A1\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://modis-land.gsfc.nasa.gov/ValStatus.php?ProductID=MOD11\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://modis-land.gsfc.nasa.gov/MODLAND_val.html\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://landweb.modaps.eosdis.nasa.gov/cgi-bin/QA_WWW/qaFlagPage.cgi?sat=terra&ver=C6\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G2357124367-LPCLOUD?h=500&w=500\"}]},{\"processing_level_id\":\"2G\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2002-07-04T00:00:00.000Z\",\"version_id\":\"061\",\"updated\":\"2015-09-30T10:48:18.602Z\",\"dataset_id\":\"MODIS/Aqua Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V061\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"MYD09GA\",\"organizations\":[\"LP DAAC\",\"NASA/GSFC/SED/ESD/TISL/MODAPS\"],\"title\":\"MODIS/Aqua Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V061\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The MYD09GA Version 6.1 product provides an estimate of the surface spectral reflectance of Aqua Moderate Resolution Imaging Spectroradiometer (MODIS) Bands 1 through 7, corrected for atmospheric conditions such as gasses, aerosols, and Rayleigh scattering. Provided along with the 500 meter (m) surface reflectance, observation, and quality bands are a set of ten 1 km observation bands and geolocation flags. The reflectance layers from the MYD09GA are used as the source data for many of the MODIS land products. \\\\n\\\\nValidation at stage 3 (https://modis-land.gsfc.nasa.gov/MODLAND_val.html) has been achieved for the MODIS Surface Reflectance products. Further details regarding MODIS land product validation for the MYD09 data product is available from the MODIS Land Team Validation site (https://modis-land.gsfc.nasa.gov/ValStatus.php?ProductID=MOD09).\\\\n\\\\nImprovements/Changes from Previous Versions\\\\n\\\\n* The Version 6.1 Level-1B (L1B) products have been improved by undergoing various calibration changes that include: changes to the response-versus-scan angle (RVS) approach that affects reflectance bands for Aqua and Terra MODIS, corrections to adjust for the optical crosstalk in Terra MODIS infrared (IR) bands, and corrections to the Terra MODIS forward look-up table (LUT) update for the period 2012 - 2017.\\\\n* A polarization correction has been applied to the L1B Reflective Solar Bands (RSB).\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2202498116-LPCLOUD\",\"has_formats\":false,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"Aqua\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/MODIS/MYD09GA.061\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://e4ftl01.cr.usgs.gov/MOLA/MYD09GA.061/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search/granules?p=C2202498116-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://earthexplorer.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/service#\",\"hreflang\":\"en-US\",\"href\":\"https://opendap.cr.usgs.gov/opendap/hyrax/MYD09GA.061/contents.html\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/305/MOD09_ATBD.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/925/MOD09_User_Guide_V61.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://ladsweb.modaps.eosdis.nasa.gov/filespec/MODIS/61/MYD09GA\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://modis-land.gsfc.nasa.gov/ValStatus.php?ProductID=MYD09\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://modis-land.gsfc.nasa.gov/MODLAND_val.html\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://landweb.modaps.eosdis.nasa.gov/cgi-bin/QA_WWW/qaFlagPage.cgi?sat=aqua&ver=C6\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G2358467881-LPCLOUD?h=00&w=500\"}]},{\"processing_level_id\":\"2G\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2002-07-04T00:00:00.000Z\",\"version_id\":\"061\",\"updated\":\"2015-09-30T10:48:19.848Z\",\"dataset_id\":\"MODIS/Aqua Surface Reflectance Daily L2G Global 250m SIN Grid V061\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"MYD09GQ\",\"organizations\":[\"LP DAAC\",\"NASA/GSFC/SED/ESD/TISL/MODAPS\"],\"title\":\"MODIS/Aqua Surface Reflectance Daily L2G Global 250m SIN Grid V061\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The MYD09GQ Version 6.1 product provides an estimate of the surface spectral reflectance of Aqua Moderate Resolution Imaging Spectroradiometer (MODIS) 250 meter (m) bands 1 and 2, corrected for atmospheric conditions such as gasses, aerosols, and Rayleigh scattering. Along with the 250 m bands are the Quality Assurance (QA) layer and five observation layers. This product is intended to be used in conjunction with the quality and viewing geometry information of the 500 m product (MYD09GA). \\\\r\\\\n\\\\r\\\\nValidation at stage 3 (https://modis-land.gsfc.nasa.gov/MODLAND_val.html) has been achieved for the MODIS Surface Reflectance products. Further details regarding MODIS land product validation for the MYD09 data product is available from the MODIS Land Team Validation site (https://modis-land.gsfc.nasa.gov/ValStatus.php?ProductID=MOD09).\\\\r\\\\n\\\\r\\\\nImprovements/Changes from Previous Versions\\\\r\\\\n\\\\r\\\\n* The Version 6.1 Level-1B (L1B) products have been improved by undergoing various calibration changes that include: changes to the response-versus-scan angle (RVS) approach that affects reflectance bands for Aqua and Terra MODIS, corrections to adjust for the optical crosstalk in Terra MODIS infrared (IR) bands, and corrections to the Terra MODIS forward look-up table (LUT) update for the period 2012 - 2017.\\\\r\\\\n* A polarization correction has been applied to the L1B Reflective Solar Bands (RSB).\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2343109950-LPCLOUD\",\"has_formats\":false,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"Aqua\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/MODIS/MYD09GQ.061\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://e4ftl01.cr.usgs.gov/MOLA/MYD09GQ.061/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search/granules?p=C2343109950-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://earthexplorer.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/service#\",\"hreflang\":\"en-US\",\"href\":\"https://opendap.cr.usgs.gov/opendap/hyrax/MYD09GQ.061/contents.html\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/305/MOD09_ATBD.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/925/MOD09_User_Guide_V61.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://ladsweb.modaps.eosdis.nasa.gov/filespec/MODIS/61/MYD09GQ\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://modis-land.gsfc.nasa.gov/ValStatus.php?ProductID=MYD09\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://modis-land.gsfc.nasa.gov/MODLAND_val.html\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://landweb.modaps.eosdis.nasa.gov/cgi-bin/QA_WWW/qaFlagPage.cgi?sat=aqua&ver=C6\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G2359475509-LPCLOUD?h=500&w=500\"}]}]}}'\n\n\nA more convenient way to work with this information is to use json formatted data. I’m using pretty print pprint to print the data in an easy to read way.\nNote - response.json() will format our response in json - ['feed']['entry'] returns all entries that CMR returned in the request (not the same as CMR-Hits) - [0] returns the first entry. Reminder that python starts indexing at 0, not 1!\n\npprint(response.json()['feed']['entry'][0])\n\n{'archive_center': 'LP DAAC',\n 'association_details': {'tools': [{'concept_id': 'TL1860232272-LPDAAC_ECS'}]},\n 'associations': {'tools': ['TL1860232272-LPDAAC_ECS']},\n 'boxes': ['-90 -180 90 180'],\n 'browse_flag': True,\n 'cloud_hosted': True,\n 'collection_data_type': 'SCIENCE_QUALITY',\n 'consortiums': ['GEOSS', 'EOSDIS'],\n 'coordinate_system': 'CARTESIAN',\n 'data_center': 'LPCLOUD',\n 'dataset_id': 'HLS Landsat Operational Land Imager Surface Reflectance and '\n               'TOA Brightness Daily Global 30m v2.0',\n 'has_formats': False,\n 'has_spatial_subsetting': False,\n 'has_temporal_subsetting': False,\n 'has_transforms': False,\n 'has_variables': False,\n 'id': 'C2021957657-LPCLOUD',\n 'links': [{'href': 'https://search.earthdata.nasa.gov/search?q=C2021957657-LPCLOUD',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': 'https://doi.org/10.5067/HLS/HLSL30.002',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1326/HLS_User_Guide_V2.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/769/HLS_ATBD_V15_provisional.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1117/HLS_Quick_Guide_v02.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-super-script/browse',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G2095313663-LPCLOUD?h=512&w=512',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n           {'href': 'https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-bulk-download/browse',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/resources/e-learning/getting-started-with-cloud-native-harmonized-landsat-sentinel-2-hls-data-in-r/',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://appeears.earthdatacloud.nasa.gov/',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n 'online_access_flag': True,\n 'orbit_parameters': {},\n 'organizations': ['LP DAAC', 'NASA/IMPACT'],\n 'original_format': 'UMM_JSON',\n 'platforms': ['LANDSAT-8', 'LANDSAT-9'],\n 'processing_level_id': '3',\n 'service_features': {'esi': {'has_formats': False,\n                              'has_spatial_subsetting': False,\n                              'has_temporal_subsetting': False,\n                              'has_transforms': False,\n                              'has_variables': False},\n                      'harmony': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False},\n                      'opendap': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False}},\n 'short_name': 'HLSL30',\n 'summary': 'The Harmonized Landsat Sentinel-2 (HLS) project provides '\n            'consistent surface reflectance (SR) and top of atmosphere (TOA) '\n            'brightness data from a virtual constellation of satellite '\n            'sensors. The Operational Land Imager (OLI) is housed aboard the '\n            'joint NASA/USGS Landsat 8 and Landsat 9 satellites, while the '\n            'Multi-Spectral Instrument (MSI) is mounted aboard Europe’s '\n            'Copernicus Sentinel-2A and Sentinel-2B satellites. The combined '\n            'measurement enables global observations of the land every 2–3 '\n            'days at 30-meter (m) spatial resolution. The HLS project uses a '\n            'set of algorithms to obtain seamless products from OLI and MSI '\n            'that include atmospheric correction, cloud and cloud-shadow '\n            'masking, spatial co-registration and common gridding, '\n            'illumination and view angle normalization, and spectral bandpass '\n            'adjustment.\\r\\n'\n            '\\r\\n'\n            'The HLSL30 product provides 30-m Nadir Bidirectional Reflectance '\n            'Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is '\n            'derived from Landsat 8/9 OLI data products. The HLSS30 and HLSL30 '\n            'products are gridded to the same resolution and Military Grid '\n            'Reference System '\n            '(MGRS)(https://hls.gsfc.nasa.gov/products-description/tiling-system/) '\n            'tiling system, and thus are “stackable” for time series '\n            'analysis.\\r\\n'\n            '\\r\\n'\n            'The HLSL30 product is provided in Cloud Optimized GeoTIFF (COG) '\n            'format, and each band is distributed as a separate file. There '\n            'are 11 bands included in the HLSL30 product along with one '\n            'quality assessment (QA) band and four angle bands. See the User '\n            'Guide for a more detailed description of the individual bands '\n            'provided in the HLSL30 product.',\n 'time_start': '2013-04-11T00:00:00.000Z',\n 'title': 'HLS Landsat Operational Land Imager Surface Reflectance and TOA '\n          'Brightness Daily Global 30m v2.0',\n 'updated': '2015-12-03T10:57:07.000Z',\n 'version_id': '2.0'}\n\n\nThe first response contains a lot more information than we need. We’ll narrow in on a few fields to get a feel for what we have. We’ll print the name of the dataset (dataset_id) and the concept id (id). We can build this variable and print statement like we did above with the url variable.\n\ncollections = response.json()['feed']['entry']\n\n\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} | {collection[\"dataset_id\"]} | {collection[\"short_name\"]} |{collection[\"id\"]}')\n\nLP DAAC | HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 | HLSL30 |C2021957657-LPCLOUD\nLP DAAC | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | HLSS30 |C2021957295-LPCLOUD\nLP DAAC | ASTER Global Digital Elevation Model V003 | ASTGTM |C1711961296-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid V061 | MYD11A1 |C1748046084-LPCLOUD\nLP DAAC | MODIS/Terra Vegetation Indices 16-Day L3 Global 250m SIN Grid V061 | MOD13Q1 |C1748066515-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V061 | MOD09GA |C2202497474-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance Daily L2G Global 250m SIN Grid V061 | MOD09GQ |C2343115666-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid V061 | MOD11A1 |C1748058432-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V061 | MYD09GA |C2202498116-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance Daily L2G Global 250m SIN Grid V061 | MYD09GQ |C2343109950-LPCLOUD\n\n\nCMR-Hits showed 209 data collections above but CMR restricts the number of results returned by each query. The default is 10 but it can be set to a maximum of 2000 by adding page_size parameter. We can set the page_size parameter to 300 (higher than the number of results returned) so we get all results in a single query.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                            'page_size': 300\n                        },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nresponse\n\n&lt;Response [200]&gt;\n\n\nNow, when we can re-run our for loop for the collections we now have all of the available collections listed.\n\ncollections = response.json()['feed']['entry']\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} | {collection[\"dataset_id\"]} | {collection[\"short_name\"]} |{collection[\"id\"]}')\n\nLP DAAC | HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 | HLSL30 |C2021957657-LPCLOUD\nLP DAAC | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | HLSS30 |C2021957295-LPCLOUD\nLP DAAC | ASTER Global Digital Elevation Model V003 | ASTGTM |C1711961296-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid V061 | MYD11A1 |C1748046084-LPCLOUD\nLP DAAC | MODIS/Terra Vegetation Indices 16-Day L3 Global 250m SIN Grid V061 | MOD13Q1 |C1748066515-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V061 | MOD09GA |C2202497474-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance Daily L2G Global 250m SIN Grid V061 | MOD09GQ |C2343115666-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid V061 | MOD11A1 |C1748058432-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V061 | MYD09GA |C2202498116-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance Daily L2G Global 250m SIN Grid V061 | MYD09GQ |C2343109950-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Ref Daily L3 Global - 500m V061 | MCD43A4 |C2218719731-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity 8-Day L3 Global 1km SIN Grid V061 | MOD11A2 |C2269056084-LPCLOUD\nLP DAAC | MODIS/Terra Vegetation Indices 16-Day L3 Global 1km SIN Grid V061 | MOD13A2 |C2565788905-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity 8-Day L3 Global 1km SIN Grid V061 | MYD11A2 |C2269057787-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance 8-Day L3 Global 500m SIN Grid V061 | MOD09A1 |C2343111356-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Aerosol Optical Depth Daily L2G Global 1km SIN Grid V061 | MCD19A2 |C2324689816-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Albedo Daily L3 Global - 500m V061 | MCD43A3 |C2278860820-LPCLOUD\nLP DAAC | MODIS/Terra Thermal Anomalies/Fire 5-Min L2 Swath 1km V061 | MOD14 |C2271754179-LPCLOUD\nLP DAAC | MODIS/Aqua Thermal Anomalies/Fire 5-Min L2 Swath 1km V061 | MYD14 |C2278858993-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Model Parameters Daily L3 Global - 500m V061 | MCD43A1 |C2343116130-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Quality Daily L3 Global - 500m V061 | MCD43A2 |C2343116525-LPCLOUD\nLP DAAC | MODIS/Terra Leaf Area Index/FPAR 8-Day L4 Global 500m SIN Grid V061 | MOD15A2H |C2218777082-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/3-Band Emissivity Daily L3 Global 1km SIN Grid Day V061 | MOD21A1D |C2545303088-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/3-Band Emissivity Daily L3 Global 1km SIN Grid Night V061 | MOD21A1N |C2545303093-LPCLOUD\nLP DAAC | MODIS/Aqua Vegetation Indices 16-Day L3 Global 250m SIN Grid V061 | MYD13Q1 |C2307290656-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/3-Band Emissivity Daily L3 Global 1km SIN Grid Day V061 | MYD21A1D |C2565805783-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Leaf Area Index/FPAR 4-Day L4 Global 500m SIN Grid V061 | MCD15A3H |C2343110937-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance 8-Day L3 Global 250m SIN Grid V061 | MOD09Q1 |C2343112831-LPCLOUD\nLP DAAC | MODIS/Terra Vegetation Indices 16-Day L3 Global 500m SIN Grid V061 | MOD13A1 |C2565788901-LPCLOUD\nLP DAAC | MODIS/Terra Thermal Anomalies/Fire Daily L3 Global 1km SIN Grid V061 | MOD14A1 |C2565791013-LPCLOUD\nLP DAAC | MODIS/Terra Net Evapotranspiration 8-Day L4 Global 500m SIN Grid V061 | MOD16A2 |C2343113232-LPCLOUD\nLP DAAC | MODIS/Terra Gross Primary Productivity 8-Day L4 Global 500m SIN Grid V061 | MOD17A2H |C2565791027-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance 8-Day L3 Global 500m SIN Grid V061 | MYD09A1 |C2343113743-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity 5-Min L2 Swath 1km V061 | MYD11_L2 |C2343114808-LPCLOUD\nLP DAAC | MODIS/Aqua Vegetation Indices 16-Day L3 Global 1km SIN Grid V061 | MYD13A2 |C2565794049-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/3-Band Emissivity Daily L3 Global 1km SIN Grid Night V061 | MYD21A1N |C2565805789-LPCLOUD\nLP DAAC | MODIS/Terra Thermal Anomalies/Fire 8-Day L3 Global 1km SIN Grid V061 | MOD14A2 |C2565791018-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance 8-Day L3 Global 250m SIN Grid V061 | MYD09Q1 |C2343114343-LPCLOUD\nLP DAAC | MODIS/Aqua Thermal Anomalies/Fire Daily L3 Global 1km SIN Grid V061 | MYD14A1 |C2565794059-LPCLOUD\nLP DAAC | MODIS/Aqua Thermal Anomalies/Fire 8-Day L3 Global 1km SIN Grid V061 | MYD14A2 |C2565794060-LPCLOUD\nLP DAAC | MODIS/Aqua Leaf Area Index/FPAR 8-Day L4 Global 500m SIN Grid V061 | MYD15A2H |C2565794061-LPCLOUD\nLP DAAC | MODIS/Aqua Net Evapotranspiration 8-Day L4 Global 500m SIN Grid V061 | MYD16A2 |C2565794064-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Cover Type Yearly L3 Global 500m SIN Grid V061 | MCD12Q1 |C2484079608-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Direct Broadcast Burned Area Monthly L3 Global 500m SIN Grid V061 | MCD64A1 |C2565786756-LPCLOUD\nLP DAAC | MODIS/Terra Vegetation Indices Monthly L3 Global 0.05Deg CMG V061 | MOD13C2 |C2565788914-LPCLOUD\nLP DAAC | MODIS/Terra Net Primary Production Gap-Filled Yearly L4 Global 500m SIN Grid V061 | MOD17A3HGF |C2565791034-LPCLOUD\nLP DAAC | MODIS/Aqua Gross Primary Productivity 8-Day L4 Global 500m SIN Grid V061 | MYD17A2H |C2565794796-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Albedo Daily L3 Global 0.05Deg CMG V061 | MCD43C3 |C2532068039-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance Daily L3 Global 0.05Deg CMG V061 | MOD09CMG |C2565788876-LPCLOUD\nLP DAAC | MODIS/Terra Vegetation Indices Monthly L3 Global 1km SIN Grid V061 | MOD13A3 |C2327962326-LPCLOUD\nLP DAAC | MODIS/Aqua Vegetation Indices 16-Day L3 Global 500m SIN Grid V061 | MYD13A1 |C2565794046-LPCLOUD\nLP DAAC | ASTER Global Digital Elevation Model NetCDF V003 | ASTGTM_NC |C2439422590-LPCLOUD\nLP DAAC | EMIT L1B At-Sensor Calibrated Radiance and Geolocation Data 60 m V001 | EMITL1BRAD |C2408009906-LPCLOUD\nLP DAAC | EMIT L2A Estimated Surface Reflectance and Uncertainty and Masks 60 m V001 | EMITL2ARFL |C2408750690-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Cover Type Yearly L3 Global 0.05Deg CMG V061 | MCD12C1 |C2484078896-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Cover Dynamics Yearly L3 Global 500m SIN Grid V061 | MCD12Q2 |C2484079943-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Leaf Area Index/FPAR 8-Day L4 Global 500m SIN Grid V061 | MCD15A2H |C2222147000-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Photosynthetically Active Radiation Daily/3-Hour L3 Global 1km SIN Grid V061 | MCD18A2 |C2484080763-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Photosynthetically Active Radiation Daily/3-Hour L3 Global 0.05Deg CMG V061 | MCD18C2 |C2484081543-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity Monthly L3 Global 0.05Deg CMG V061 | MOD11C3 |C2565788897-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity 5-Min L2 Swath 1km V061 | MOD11_L2 |C2343115255-LPCLOUD\nLP DAAC | MODIS/Terra Vegetation Indices 16-Day L3 Global 0.05Deg CMG V061 | MOD13C1 |C2565788912-LPCLOUD\nLP DAAC | MODIS/Terra Net Evapotranspiration Gap-Filled 8-Day L4 Global 500m SIN Grid V061 | MOD16A2GF |C2565791021-LPCLOUD\nLP DAAC | MODIS/Terra Net Evapotranspiration Gap-Filled Yearly L4 Global 500m SIN Grid V061 | MOD16A3GF |C2565791024-LPCLOUD\nLP DAAC | MODIS/Terra Gross Primary Productivity Gap-Filled 8-Day L4 Global 500m SIN Grid V061 | MOD17A2HGF |C2565791029-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/3-Band Emissivity 5-Min L2 1km V061 | MOD21 |C2565791036-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/3-Band Emissivity Daily L3 Global 0.05Deg CMG V061 | MOD21C1 |C2565791044-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance Daily L3 Global 0.05Deg CMG V061 | MYD09CMG |C2565794001-LPCLOUD\nLP DAAC | ASTER Global Digital Elevation Model Attributes NetCDF V003 | ASTGTM_NUMNC |C2439429778-LPCLOUD\nLP DAAC | ECOSTRESS Swath Attitude and Ephemeris Instantaneous L1B Global V002 | ECO_L1B_ATT |C2076117996-LPCLOUD\nLP DAAC | ECOSTRESS Swath Geolocation Instantaneous L1B Global 70 m V002 | ECO_L1B_GEO |C2076087338-LPCLOUD\nLP DAAC | ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m V002 | ECO_L1B_RAD |C2076116385-LPCLOUD\nLP DAAC | ECOSTRESS Gridded Top of Atmosphere Calibrated Radiance Instantaneous L1C Global 70 m V002 | ECO_L1CG_RAD |C2595678497-LPCLOUD\nLP DAAC | ECOSTRESS Tiled Top of Atmosphere Calibrated Radiance Instantaneous L1C Global 70 m V002 | ECO_L1CT_RAD |C2595678301-LPCLOUD\nLP DAAC | ECOSTRESS Gridded Cloud Mask Instantaneous L2 Global 70 m V002 | ECO_L2G_CLOUD |C2076113561-LPCLOUD\nLP DAAC | ECOSTRESS Gridded Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2G_LSTE |C2076113037-LPCLOUD\nLP DAAC | ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2T_LSTE |C2076090826-LPCLOUD\nLP DAAC | ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m V002 | ECO_L2_CLOUD |C2076115306-LPCLOUD\nLP DAAC | ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2_LSTE |C2076114664-LPCLOUD\nLP DAAC | EMIT L1B Corrected Spacecraft Attitude and Ephemeris V001 | EMITL1BATT |C2408031090-LPCLOUD\nLP DAAC | EMIT L2B Estimated Mineral Identification and Band Depth and Uncertainty 60 m V001 | EMITL2BMIN |C2408034484-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Surface Radiation Daily/3-Hour L3 Global 1km SIN Grid V061 | MCD18A1 |C2484080427-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Downward Shortwave Radiation Daily/3-Hour L3 Global 0.05Deg CMG V061 | MCD18C1 |C2484081120-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Surface BRF Daily L2G Global 500m and 1km SIN Grid V061 | MCD19A1 |C2484086031-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF Model Parameters Daily L3 Global 1km SIN Grid V061 | MCD19A3D |C2484086411-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Model Parameters Daily L3 Global 0.05Deg CMG V061 | MCD43C1 |C2532015377-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Snow-free Model Parameters Daily L3 Global 0.05Deg CMG V061 | MCD43C2 |C2532059394-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Ref Daily L3 Global 0.05Deg CMG V061 | MCD43C4 |C2532449179-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter 1 Band 1 Daily L3 Global 30 ArcSec CMG V061 | MCD43D01 |C2532021230-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter 2 Band 1 Daily L3 Global 30 ArcSec CMG V061 | MCD43D02 |C2532020158-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter 3 Band 1 Daily L3 Global 30 ArcSec CMG V061 | MCD43D03 |C2532019021-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter 1 Band 2 Daily L3 Global 30 ArcSec CMG V061 | MCD43D04 |C2532014841-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter 2 Band 2 Daily L3 Global 30 ArcSec CMG V061 | MCD43D05 |C2532011588-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter 3 Band 2 Daily L3 Global 30 ArcSec CMG V061 | MCD43D06 |C2532007810-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter1 Band3 Daily L3 Global 30ArcSec CMG V061 | MCD43D07 |C2539207575-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter2 Band3 Daily L3 Global 30ArcSec CMG V061 | MCD43D08 |C2539208411-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter3 Band3 Daily L3 Global 30ArcSec CMG V061 | MCD43D09 |C2539209209-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter1 Band4 Daily L3 Global 30ArcSec CMG V061 | MCD43D10 |C2539209814-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter2 Band4 Daily L3 Global 30ArcSec CMG V061 | MCD43D11 |C2539902420-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter3 Band4 Daily L3 Global 30ArcSec CMG V061 | MCD43D12 |C2539907890-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter1 Band5 Daily L3 Global 30ArcSec CMG V061 | MCD43D13 |C2539907921-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter2 Band5 Daily L3 Global 30ArcSec CMG V061 | MCD43D14 |C2539907928-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter3 Band5 Daily L3 Global 30ArcSec CMG V061 | MCD43D15 |C2539907934-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter1 Band6 Daily L3 Global 30ArcSec CMG V061 | MCD43D16 |C2539907940-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter2 Band6 Daily L3 Global 30ArcSec CMG V061 | MCD43D17 |C2539907945-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter3 Band6 Daily L3 Global 30ArcSec CMG V061 | MCD43D18 |C2539907952-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter1 Band7 Daily L3 Global 30ArcSec CMG V061 | MCD43D19 |C2539907958-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter2 Band7 Daily L3 Global 30ArcSec CMG V061 | MCD43D20 |C2539907962-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter3 Band7 Daily L3 Global 30ArcSec CMG V061 | MCD43D21 |C2540268544-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter1 VIS Daily L3 Global 30ArcSec CMG V061 | MCD43D22 |C2540268550-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter2 VIS Daily L3 Global 30ArcSec CMG V061 | MCD43D23 |C2540268554-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter3 VIS Daily L3 Global 30ArcSec CMG V061 | MCD43D24 |C2540268560-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter1 NIR Daily L3 Global 30ArcSec CMG V061 | MCD43D25 |C2540268566-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter2 NIR Daily L3 Global 30ArcSec CMG V061 | MCD43D26 |C2540268573-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter3 NIR Daily L3 Global 30ArcSec CMG V061 | MCD43D27 |C2540268577-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter1 Shortwave Daily L3 Global 30ArcSec CMG V061 | MCD43D28 |C2540268581-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter2 Shortwave Daily L3 Global 30ArcSec CMG V061 | MCD43D29 |C2540268586-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Parameter3 Shortwave Daily L3 Global 30ArcSec CMG V061 | MCD43D30 |C2540268595-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo QA BRDFQuality Daily L3 Global 30ArcSec CMG V061 | MCD43D31 |C2540270738-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo QA LocalSolarNoon Daily L3 Global 30ArcSec CMG V061 | MCD43D32 |C2540270742-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo QA ValidobsBand1 Daily L3 Global 30ArcSec CMG V061 | MCD43D33 |C2540270747-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo QA ValidobsBand2 Daily L3 Global 30ArcSec CMG V061 | MCD43D34 |C2540270751-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo QA ValidobsBand3 Daily L3 Global 30ArcSec CMG V061 | MCD43D35 |C2540270757-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo QA ValidobsBand4 Daily L3 Global 30ArcSec CMG V061 | MCD43D36 |C2540270762-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo QA ValidobsBand5 Daily L3 Global 30ArcSec CMG V061 | MCD43D37 |C2540270766-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo QA ValidobsBand6 Daily L3 Global 30ArcSec CMG V061 | MCD43D38 |C2540270771-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo QA ValidobsBand7 Daily L3 Global 30ArcSec CMG V061 | MCD43D39 |C2540270775-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo QA SnowStatus Daily L3 Global 30ArcSec CMG V061 | MCD43D40 |C2540270779-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo QA Uncertainty Daily L3 Global 30ArcSec CMG V061 | MCD43D41 |C2540271801-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Black Sky Albedo Band1 Daily L3 Global 30ArcSec CMG V061 | MCD43D42 |C2540271806-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Black Sky Albedo Band2 Daily L3 Global 30ArcSec CMG V061 | MCD43D43 |C2540271810-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Black Sky Albedo Band3 Daily L3 Global 30ArcSec CMG V061 | MCD43D44 |C2540271815-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Black Sky Albedo Band4 Daily L3 Global 30ArcSec CMG V061 | MCD43D45 |C2540271820-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Black Sky Albedo Band5 Daily L3 Global 30ArcSec CMG V061 | MCD43D46 |C2540271825-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Black Sky Albedo Band6 Daily L3 Global 30ArcSec CMG V061 | MCD43D47 |C2540271830-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Black Sky Albedo Band7 Daily L3 Global 30ArcSec CMG V061 | MCD43D48 |C2540271835-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Black Sky Albedo VIS Daily L3 Global 30ArcSec CMG V061 | MCD43D49 |C2540271839-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Black Sky Albedo NIR Daily L3 Global 30ArcSec CMG V061 | MCD43D50 |C2540271843-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Black Sky Albedo Shortwave Daily L3 Global 30ArcSec CMG V061 | MCD43D51 |C2540273055-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo White Sky Albedo Band1 Daily L3 Global 30ArcSec CMG V061 | MCD43D52 |C2540273061-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo White Sky Albedo Band2 Daily L3 Global 30ArcSec CMG V061 | MCD43D53 |C2540273066-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo White Sky Albedo Band3 Daily L3 Global 30ArcSec CMG V061 | MCD43D54 |C2540273075-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo White Sky Albedo Band4 Daily L3 Global 30ArcSec CMG V061 | MCD43D55 |C2540273116-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo White Sky Albedo Band5 Daily L3 Global 30ArcSec CMG V061 | MCD43D56 |C2540273121-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo White Sky Albedo Band6 Daily L3 Global 30ArcSec CMG V061 | MCD43D57 |C2540273128-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo White Sky Albedo Band7 Daily L3 Global 30ArcSec CMG V061 | MCD43D58 |C2540273133-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo White Sky Albedo VIS Daily L3 Global 30ArcSec CMG V061 | MCD43D59 |C2540273183-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo White Sky Albedo NIR Daily L3 Global 30ArcSec CMG V061 | MCD43D60 |C2540273187-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo White Sky Albedo Shortwave Daily L3 Global 30ArcSec CMG V061 | MCD43D61 |C2540275672-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Ref Band1 Daily L3 Global 30ArcSec CMG V061 | MCD43D62 |C2540275683-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Ref Band2 Daily L3 Global 30ArcSec CMG V061 | MCD43D63 |C2540275688-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Ref Band3 Daily L3 Global 30ArcSec CMG V061 | MCD43D64 |C2540275694-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Ref Band4 Daily L3 Global 30ArcSec CMG V061 | MCD43D65 |C2540275719-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Ref Band5 Daily L3 Global 30ArcSec CMG V061 | MCD43D66 |C2540275742-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Ref Band6 Daily L3 Global 30ArcSec CMG V061 | MCD43D67 |C2540275748-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Ref Band7 Daily L3 Global 30ArcSec CMG V061 | MCD43D68 |C2540275753-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity Daily L3 Global 6km SIN Grid V061 | MOD11B1 |C2524245159-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity 8-Day L3 Global 6km SIN Grid V061 | MOD11B2 |C2565788881-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity Monthly L3 Global 6km SIN Grid V061 | MOD11B3 |C2565788885-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity Daily L3 Global 0.05Deg CMG V061 | MOD11C1 |C2565788888-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity 8-Day L3 Global 0.05Deg CMG V061 | MOD11C2 |C2565788893-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/3-Band Emissivity 8-Day L3 Global 1km SIN Grid V061 | MOD21A2 |C2565791040-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/3-Band Emissivity 8-Day L3 Global 0.05Deg CMG V061 | MOD21C2 |C2565791047-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/3-Band Emissivity Monthly L3 Global 0.05Deg CMG V061 | MOD21C3 |C2565791050-LPCLOUD\nLP DAAC | MODIS/Terra Water Reservoir 8-Day L3 Global V061 | MOD28C2 |C2565791054-LPCLOUD\nLP DAAC | MODIS/Terra Water Reservoir Monthly L3 Global V061 | MOD28C3 |C2565791057-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity Daily L3 Global 6km SIN Grid V061 | MYD11B1 |C2565794007-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity 8-Day L3 Global 6km SIN Grid V061 | MYD11B2 |C2565794018-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity Monthly L3 Global 6km SIN Grid V061 | MYD11B3 |C2565794030-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity Daily L3 Global 0.05Deg CMG V061 | MYD11C1 |C2565794038-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity 8-Day L3 Global 0.05Deg CMG V061 | MYD11C2 |C2565794042-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity Monthly L3 Global 0.05Deg CMG V061 | MYD11C3 |C2565794044-LPCLOUD\nLP DAAC | MODIS/Aqua Vegetation Indices Monthly L3 Global 1km SIN Grid V061 | MYD13A3 |C2327957988-LPCLOUD\nLP DAAC | MODIS/Aqua Vegetation Indices 16-Day L3 Global 0.05Deg CMG V061 | MYD13C1 |C2565794051-LPCLOUD\nLP DAAC | MODIS/Aqua Vegetation Indices Monthly L3 Global 0.05Deg CMG V061 | MYD13C2 |C2565794055-LPCLOUD\nLP DAAC | MODIS/Aqua Net Evapotranspiration Gap-Filled 8-Day L4 Global 500m SIN Grid V061 | MYD16A2GF |C2565794067-LPCLOUD\nLP DAAC | MODIS/Aqua Net Evapotranspiration Gap-Filled Yearly L4 Global 500m SIN Grid V061 | MYD16A3GF |C2565794069-LPCLOUD\nLP DAAC | MODIS/Aqua Gross Primary Productivity Gap-Filled 8-Day L4 Global 500m SIN Grid V061 | MYD17A2HGF |C2565794824-LPCLOUD\nLP DAAC | MODIS/Aqua Net Primary Production Gap-Filled Yearly L4 Global 500m SIN Grid V061 | MYD17A3HGF |C2565794850-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/3-Band Emissivity 5-Min L2 1km V061 | MYD21 |C2565805776-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/3-Band Emissivity 8-Day L3 Global 1km SIN Grid V061 | MYD21A2 |C2565805799-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/3-Band Emissivity Daily L3 Global 0.05Deg CMG V061 | MYD21C1 |C2565805805-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/3-Band Emissivity 8-Day L3 Global 0.05Deg CMG V061 | MYD21C2 |C2565805807-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/3-Band Emissivity Monthly L3 Global 0.05Deg CMG V061 | MYD21C3 |C2565805812-LPCLOUD\nLP DAAC | MODIS/Aqua Water Reservoir 8-Day L3 Global V061 | MYD28C2 |C2565805818-LPCLOUD\nLP DAAC | MODIS/Aqua Water Reservoir Monthly L3 Global V061 | MYD28C3 |C2565805823-LPCLOUD\nLP DAAC | OPERA Land Surface Disturbance Alert from Harmonized Landsat Sentinel-2 provisional product (Version 0) | OPERA_L3_DIST-ALERT-HLS_PROVISIONAL_V0 |C2517904291-LPCLOUD\nLP DAAC | VIIRS/JPSS1 Surface Reflectance 8-Day L3 Global 1km SIN Grid V002 | VJ109A1 |C2501959919-LPCLOUD\nLP DAAC | VIIRS/JPSS1 Surface Reflectance Daily L3 Global 0.05Deg CMG V002 | VJ109CMG |C2519121257-LPCLOUD\nLP DAAC | VIIRS/JPSS1 Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V002 | VJ109GA |C2631841524-LPCLOUD\nLP DAAC | VIIRS/JPSS1 Surface Reflectance 8-Day L3 Global 500m SIN Grid V002 | VJ109H1 |C2519120226-LPCLOUD\nLP DAAC | VIIRS/JPSS1 Land Surface Temperature and Emissivity 6-Min L2 Swath 750m V002 | VJ121 |C2545310883-LPCLOUD\nLP DAAC | VIIRS/JPSS1 Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid Day V002 | VJ121A1D |C2545310887-LPCLOUD\nLP DAAC | VIIRS/JPSS1 Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid Night V002 | VJ121A1N |C2545310892-LPCLOUD\nLP DAAC | VIIRS/JPSS1 Land Surface Temperature/Emissivity 8-Day L3 Global 1km SIN Grid V002 | VJ121A2 |C2545310897-LPCLOUD\nLP DAAC | VIIRS/JPSS1 Land Surface Temperature/Emissivity Daily L3 Global 0.05Deg CMG V002 | VJ121C1 |C2545310901-LPCLOUD\nLP DAAC | VIIRS/JPSS1 Land Surface Temperature/Emissivity 8-Day L3 Global 0.05Deg CMG V002 | VJ121C2 |C2545310905-LPCLOUD\nLP DAAC | VIIRS/JPSS1 Land Surface Temperature/Emissivity Monthly L3 Global 0.05Deg CMG V002 | VJ121C3 |C2545310909-LPCLOUD\nLP DAAC | VIIRS/NPP Surface Reflectance 8-Day L3 Global 1km SIN Grid V002 | VNP09A1 |C2519124793-LPCLOUD\nLP DAAC | VIIRS/NPP Surface Reflectance Daily L3 Global 0.05Deg CMG V002 | VNP09CMG |C2519126793-LPCLOUD\nLP DAAC | VIIRS/NPP Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V002 | VNP09GA |C2631841556-LPCLOUD\nLP DAAC | VIIRS/NPP Surface Reflectance 8-Day L3 Global 500m SIN Grid V002 | VNP09H1 |C2519125808-LPCLOUD\nLP DAAC | VIIRS/NPP Land Surface Temperature and Emissivity 6-Min L2 Swath 750m V002 | VNP21 |C2545314550-LPCLOUD\nLP DAAC | VIIRS/NPP Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid Day V002 | VNP21A1D |C2545314555-LPCLOUD\nLP DAAC | VIIRS/NPP Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid Night V002 | VNP21A1N |C2545314559-LPCLOUD\nLP DAAC | VIIRS/NPP Land Surface Temperature/Emissivity 8-Day L3 Global 1km SIN Grid V002 | VNP21A2 |C2545314562-LPCLOUD\nLP DAAC | VIIRS/NPP Land Surface Temperature/Emissivity Daily L3 Global 0.05Deg CMG V002 | VNP21C1 |C2545314566-LPCLOUD\nLP DAAC | VIIRS/NPP Land Surface Temperature/Emissivity 8-Day L3 Global 0.05Deg CMG V002 | VNP21C2 |C2545314570-LPCLOUD\nLP DAAC | VIIRS/NPP Land Surface Temperature/Emissivity Monthly L3 Global 0.05Deg CMG V002 | VNP21C3 |C2545314573-LPCLOUD"
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Request.html#query-cmr-using-project",
    "href": "python/tutorials/Data_Discovery_CMR_API_Request.html#query-cmr-using-project",
    "title": "Data Discovery with NASA’s CMR Using Request Python Package",
    "section": "Query CMR Using Project",
    "text": "Query CMR Using Project\nCollections can also be queried using project name. Below, we look for data Collections for ECOSTRESS and SNWG/OPERA projects distributed by LP DAAC and stored in cloud. Please note that all collections do not have a project parameter defined necessarily.\n\nproject = ['ECOSTRESS', 'SNWG/OPERA']\n\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                            'project': project, \n                            'page_size': 50\n                        },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nresponse\n\n&lt;Response [200]&gt;\n\n\n\ncollections = response.json()['feed']['entry']\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} | {collection[\"dataset_id\"]} | {collection[\"short_name\"]} |{collection[\"id\"]}')\n\nLP DAAC | ECOSTRESS Swath Geolocation Instantaneous L1B Global 70 m V002 | ECO_L1B_GEO |C2076087338-LPCLOUD\nLP DAAC | ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m V002 | ECO_L1B_RAD |C2076116385-LPCLOUD\nLP DAAC | ECOSTRESS Gridded Top of Atmosphere Calibrated Radiance Instantaneous L1C Global 70 m V002 | ECO_L1CG_RAD |C2595678497-LPCLOUD\nLP DAAC | ECOSTRESS Gridded Cloud Mask Instantaneous L2 Global 70 m V002 | ECO_L2G_CLOUD |C2076113561-LPCLOUD\nLP DAAC | ECOSTRESS Gridded Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2G_LSTE |C2076113037-LPCLOUD\nLP DAAC | ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2T_LSTE |C2076090826-LPCLOUD\nLP DAAC | ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m V002 | ECO_L2_CLOUD |C2076115306-LPCLOUD\nLP DAAC | ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2_LSTE |C2076114664-LPCLOUD\nLP DAAC | OPERA Land Surface Disturbance Alert from Harmonized Landsat Sentinel-2 provisional product (Version 0) | OPERA_L3_DIST-ALERT-HLS_PROVISIONAL_V0 |C2517904291-LPCLOUD\n\n\ncollection IDs are what we need for searching for granules."
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Request.html#get-urls-to-cloud-data-assets",
    "href": "python/tutorials/Data_Discovery_CMR_API_Request.html#get-urls-to-cloud-data-assets",
    "title": "Data Discovery with NASA’s CMR Using Request Python Package",
    "section": "Get URLs to cloud data assets",
    "text": "Get URLs to cloud data assets\nNow that we have a list of granules filtered spatially and temporally for our collection, we can save the links to access the data. Below, HTTPS and S3 links are stored in two different lists. HTTPS links can be used to access data locally while S3 links can be used to access data in the cloud. View LP DAAC Data Resources for resources available for accessing and working with data collections in the Earthdata Cloud.\n\nhttps_urls = [l['href'] for l in granules[13]['links'] if 'https' in l['href'] and '.tif' in l['href']]\nhttps_urls\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_water.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_cloud.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_height.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST_err.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_EmisWB.tif']\n\n\n\ns3_urls = [l['href'] for l in granules[13]['links'] if 's3' in l['href'] and '.tif' in l['href']]\ns3_urls\n\n['s3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_water.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_cloud.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_height.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_QC.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST_err.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_EmisWB.tif']"
  },
  {
    "objectID": "python/tutorials/Data_Discovery_CMR_API_Request.html#contact-info",
    "href": "python/tutorials/Data_Discovery_CMR_API_Request.html#contact-info",
    "title": "Data Discovery with NASA’s CMR Using Request Python Package",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 7-5-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "setup/setup_instructions_r.html",
    "href": "setup/setup_instructions_r.html",
    "title": "R resources Setup Instructions",
    "section": "",
    "text": "The R tutorials and how-tos in all repositories developed by LP DAAC team require R and RStudio, compatible R environment with packages used in the scripts, and installation of Git. More details can be found below.\n\n\n\nTo access or download NASA Earth data, a .netrc file with your NASA Earthdata Login information is needed. You can created an account here if you do not have one.\n\nSetting up a netrc file:\nR tutorials leverages a netrc file storing your Earthdata login credentials for authentication. This file is assumed to be stored in the user profile directory on Window OS or in the home directory on Mac OS. Run source(.../R/modules/earthdata_netrc_setup.R) to ensure you have the proper netrc file set up. If you are prompted for your NASA Earthdata Login Username and Password, hit enter once you have submitted your credentials. If neither HOME nor Userprofile are recognized by R, the current working directory is used.\nIf you want to manually create your own netrc file, download the .netrc file template (saved in Data folder), add your credentials, and save to your Userprofile/HOME directory. You should also make sure that both HOME directory and Userprofile directories are as same as a directory you are saving the .netrc. To do that run Sys.setenv(\"HOME\" = \"YOUR DIRECTORY\") and Sys.setenv(\"userprofile\" = \"YOUR DIRECTORY\").\n\nR and RStudio are required to execute this tutorial. Installation details can be found here.\n\nWindows\n\nInstall and load installr:\n\ninstall.packages(\"installr\");library(installr)\n\n\nCopy/Update the existing packages to the new R installation:\n\nupdateR()\n\nOpen RStudio, go to Help &gt; Check for Updates to install newer version of RStudio (if available).\n\nMac\n\nGo to https://cloud.r-project.org/bin/macosx/.\n\nDownload the latest release (R-4.0.1.pkg) and finish the installation.\nOpen RStudio, go to Help &gt; Check for Updates to install newer version of RStudio (if available).\nTo update packages, go to Tools &gt; Check for Package Updates. If updates are available, select All, and click Install Updates.\n\n\nCheck the version of R by typing version into the console and RStudio by typing RStudio.Version() into the console and update them if needed.\n\nThe R tutorials have been tested on Windows using R Version 4.2.2 and RStudio version 2022.12.0.353.\n\n\n\n\n\n\n\nClone or download HLS_Tutorial_R Repository from the LP DAAC Data User Resources Repository.\nWhen you open this Rmarkdown notebook in RStudio, you can click the little green “Play” button in each grey code chunk to execute the code. The result can be printed either in the R Console or inline in the RMarkdown notebook, depending on your RStudio preferences.\n\n\n\n\n\n\nRequired packages:\n\ngetPass\n\nsys\n\nhttr\nraster\nreadr\ntidyr\ndplyr\njsonlite\ngeojsonlint\n\ngeojsonio\n\ngeojsonR\n\nrgdal\nsp\nterra\nwarnings\n\nTo read and visualize the GeoTIFF:\n\nraster\nrasterVis\nRColorBrewer\nggplot2\n\n\nRun the code below in a cell to identify any missing packages to install, and then load all of the required packages. Alternatively, you can use install.packages('package name') command in the console or use install.packages(c('package #1', 'package #2',...)) to download multiple packages.\npackages &lt;- c('terra','getPass','httr','jsonlite','ggplot2','dplyr','tidyr','readr','geojsonio','geojsonR','rgdal',\n             'sp', 'raster', 'rasterVis', 'RColorBrewer', 'jsonlite', 'geojsonlint', 'magrittr', 'xml2', \n             'dygraphs', 'xts','lubridate','DT','rmarkdown', 'rprojroot','imager')\n\n# Identify missing (not installed) packages\nnew.packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\n\n# Loop through and Install new (not installed) packages\nif (length(new.packages)[1]==0){\n  message('All packages already installed')\n}else{\n  for (i in 1:length(new.packages)){\n    message(paste0('Installing: ', new.packages))\n    install.packages(new.packages[i], repos='http://cran.rstudio.com/')\n  }\n}\nThe code below is also saved in AppEEARS_API_Install.R\n\n\n\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 09-07-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "setup/setup_instructions_r.html#prerequisites",
    "href": "setup/setup_instructions_r.html#prerequisites",
    "title": "R resources Setup Instructions",
    "section": "",
    "text": "To access or download NASA Earth data, a .netrc file with your NASA Earthdata Login information is needed. You can created an account here if you do not have one.\n\nSetting up a netrc file:\nR tutorials leverages a netrc file storing your Earthdata login credentials for authentication. This file is assumed to be stored in the user profile directory on Window OS or in the home directory on Mac OS. Run source(.../R/modules/earthdata_netrc_setup.R) to ensure you have the proper netrc file set up. If you are prompted for your NASA Earthdata Login Username and Password, hit enter once you have submitted your credentials. If neither HOME nor Userprofile are recognized by R, the current working directory is used.\nIf you want to manually create your own netrc file, download the .netrc file template (saved in Data folder), add your credentials, and save to your Userprofile/HOME directory. You should also make sure that both HOME directory and Userprofile directories are as same as a directory you are saving the .netrc. To do that run Sys.setenv(\"HOME\" = \"YOUR DIRECTORY\") and Sys.setenv(\"userprofile\" = \"YOUR DIRECTORY\").\n\nR and RStudio are required to execute this tutorial. Installation details can be found here.\n\nWindows\n\nInstall and load installr:\n\ninstall.packages(\"installr\");library(installr)\n\n\nCopy/Update the existing packages to the new R installation:\n\nupdateR()\n\nOpen RStudio, go to Help &gt; Check for Updates to install newer version of RStudio (if available).\n\nMac\n\nGo to https://cloud.r-project.org/bin/macosx/.\n\nDownload the latest release (R-4.0.1.pkg) and finish the installation.\nOpen RStudio, go to Help &gt; Check for Updates to install newer version of RStudio (if available).\nTo update packages, go to Tools &gt; Check for Package Updates. If updates are available, select All, and click Install Updates.\n\n\nCheck the version of R by typing version into the console and RStudio by typing RStudio.Version() into the console and update them if needed.\n\nThe R tutorials have been tested on Windows using R Version 4.2.2 and RStudio version 2022.12.0.353."
  },
  {
    "objectID": "setup/setup_instructions_r.html#procedures",
    "href": "setup/setup_instructions_r.html#procedures",
    "title": "R resources Setup Instructions",
    "section": "",
    "text": "Clone or download HLS_Tutorial_R Repository from the LP DAAC Data User Resources Repository.\nWhen you open this Rmarkdown notebook in RStudio, you can click the little green “Play” button in each grey code chunk to execute the code. The result can be printed either in the R Console or inline in the RMarkdown notebook, depending on your RStudio preferences."
  },
  {
    "objectID": "setup/setup_instructions_r.html#r-environment-setup",
    "href": "setup/setup_instructions_r.html#r-environment-setup",
    "title": "R resources Setup Instructions",
    "section": "",
    "text": "Required packages:\n\ngetPass\n\nsys\n\nhttr\nraster\nreadr\ntidyr\ndplyr\njsonlite\ngeojsonlint\n\ngeojsonio\n\ngeojsonR\n\nrgdal\nsp\nterra\nwarnings\n\nTo read and visualize the GeoTIFF:\n\nraster\nrasterVis\nRColorBrewer\nggplot2\n\n\nRun the code below in a cell to identify any missing packages to install, and then load all of the required packages. Alternatively, you can use install.packages('package name') command in the console or use install.packages(c('package #1', 'package #2',...)) to download multiple packages.\npackages &lt;- c('terra','getPass','httr','jsonlite','ggplot2','dplyr','tidyr','readr','geojsonio','geojsonR','rgdal',\n             'sp', 'raster', 'rasterVis', 'RColorBrewer', 'jsonlite', 'geojsonlint', 'magrittr', 'xml2', \n             'dygraphs', 'xts','lubridate','DT','rmarkdown', 'rprojroot','imager')\n\n# Identify missing (not installed) packages\nnew.packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\n\n# Loop through and Install new (not installed) packages\nif (length(new.packages)[1]==0){\n  message('All packages already installed')\n}else{\n  for (i in 1:length(new.packages)){\n    message(paste0('Installing: ', new.packages))\n    install.packages(new.packages[i], repos='http://cran.rstudio.com/')\n  }\n}\nThe code below is also saved in AppEEARS_API_Install.R"
  },
  {
    "objectID": "setup/setup_instructions_r.html#contact-info",
    "href": "setup/setup_instructions_r.html#contact-info",
    "title": "R resources Setup Instructions",
    "section": "",
    "text": "Email: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 09-07-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "web_book/emit_resources.html",
    "href": "web_book/emit_resources.html",
    "title": "EMIT Data Resources",
    "section": "",
    "text": "All of the content in this section can be found in the EMIT-Data-Resources repository."
  },
  {
    "objectID": "web_book/igarss_2024.html",
    "href": "web_book/igarss_2024.html",
    "title": "Mapping Minerals with Space-Based Imaging Spectroscopy",
    "section": "",
    "text": "7 July, 2024 12:30 - 15:30 UTC +3\nThe Earth Surface Mineral Dust Source Investigation (EMIT) instrument aboard the International Space Station (ISS) measures visible to short-wave infrared (VSWIR) wavelengths and can be used to map Earth’s surface mineralogy in detail. Here we explore the science behind the EMIT mineralogy products and apply them in a repeatable scientific workflow. We will introduce imaging spectroscopy concepts and sensor specific considerations for exploring variation in surface mineralogy. Participants will learn the basics of VSWIR imaging spectroscopy, how minerals are identified and band depths are calculated, and how band depths are translated into mineral abundances. Participants will also learn how to find, access, and apply EMIT mineralogical data using open source resources.",
    "crumbs": [
      "Past Workshops",
      "2024 IGARSS Workshop"
    ]
  },
  {
    "objectID": "web_book/igarss_2024.html#agenda",
    "href": "web_book/igarss_2024.html#agenda",
    "title": "Mapping Minerals with Space-Based Imaging Spectroscopy",
    "section": "Agenda",
    "text": "Agenda\n\n\n\nTime\nDescription\nLeads/Instructors\n\n\n\n\n12:30\nIntroduction to EMIT\nPhil Brodrick\n\n\n12:45\nCloud Environment\nErik Bolch\n\n\n13:00\nNotebook 1: Finding EMIT L2B Data\nErik Bolch\n\n\n13:30\nBreak\n\n\n\n13:40\nUnderstanding Mineralogy Data\nPhil Brodrick\n\n\n14:00\nNotebook 2: Working with EMIT L2B Mineralogy\nErik Bolch\n\n\n14:45\nBreak\n\n\n\n14:55\nQuestions and Research Discussion\nAll",
    "crumbs": [
      "Past Workshops",
      "2024 IGARSS Workshop"
    ]
  },
  {
    "objectID": "web_book/igarss_2024.html#slides",
    "href": "web_book/igarss_2024.html#slides",
    "title": "Mapping Minerals with Space-Based Imaging Spectroscopy",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Past Workshops",
      "2024 IGARSS Workshop"
    ]
  },
  {
    "objectID": "web_book/igarss_2024.html#learning-outcomes",
    "href": "web_book/igarss_2024.html#learning-outcomes",
    "title": "Mapping Minerals with Space-Based Imaging Spectroscopy",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nIn this tutorial, we will explain some of the nuance regarding the spectral library and methods used for mineral identification, show how to orthorectify the data, explain how to interpret band depth, aggregate the targets identified by the classification into the EMIT 10 minerals related to surface dust, and translate band depth into spectral abundance. The EMIT Level 2B Estimated Mineral Identification and Band Depth and Uncertainty (EMITL2BMIN) Version 1 data product provides estimated mineral identification and band depths in a spatially raw, non-orthocorrected format. Mineral identification is performed on two spectral groups, which correspond to different regions of the spectra but often co-occur on the landscape. These estimates are generated using the Tetracorder system(code) and are based on EMITL2ARFL reflectance values. The EMIT_L2B_MINUNCERT file provides band depth uncertainty estimates calculated using surface Reflectance Uncertainty values from the EMITL2ARFL data product. The band depth uncertainties are presented as standard deviations. The fit score for each mineral identification is also provided as the coefficient of determination (r2) of the match between the continuum normalized library reference and the continuum normalized observed spectrum. Associated metadata indicates the name and reference information for each identified mineral, and additional information about aggregating minerals into different categories is available in the emit-sds-l2b repository and will be available as subsequent data products.",
    "crumbs": [
      "Past Workshops",
      "2024 IGARSS Workshop"
    ]
  },
  {
    "objectID": "web_book/igarss_2024.html#prerequisites",
    "href": "web_book/igarss_2024.html#prerequisites",
    "title": "Mapping Minerals with Space-Based Imaging Spectroscopy",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe prerequisites for this tutorial include: a basic familiarity with remote sensing and python, an Earthdata Login account, a GitHub account. All participants need to bring their laptop on the day of event.",
    "crumbs": [
      "Past Workshops",
      "2024 IGARSS Workshop"
    ]
  },
  {
    "objectID": "web_book/igarss_2024.html#contact-info",
    "href": "web_book/igarss_2024.html#contact-info",
    "title": "Mapping Minerals with Space-Based Imaging Spectroscopy",
    "section": "Contact Info",
    "text": "Contact Info\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 06-26-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Past Workshops",
      "2024 IGARSS Workshop"
    ]
  },
  {
    "objectID": "web_book/vitals.html",
    "href": "web_book/vitals.html",
    "title": "LP DAAC Data Resources",
    "section": "",
    "text": "Please see the standalone VITALS Web-book.",
    "crumbs": [
      "Past Workshops",
      "2023/2024 VITALS Workshop"
    ]
  },
  {
    "objectID": "workshops/NASA_DEVELOP_Fall_2024/NASA_DEVELOP_Land_Products_Guide.html",
    "href": "workshops/NASA_DEVELOP_Fall_2024/NASA_DEVELOP_Land_Products_Guide.html",
    "title": "NASA DEVELOP Land Data Products Guide - Fall 2024",
    "section": "",
    "text": "Data Catalog: https://lpdaac.usgs.gov/product_search/?status=Operational\nNews: https://lpdaac.usgs.gov/news/\nHelp: https://lpdaac.usgs.gov/lpdaac-contact-us/\n\n\n\n\n\n\nDescription\nEarthdata Search is NASA’s comprehensive data discovery and access platform. Search, visualize, and download Earth science data from NASA’s vast archives, covering climate, weather, land, ocean, and more.\nInterface\n\nGraphical User Interface (GUI)\n\nCapabilities\n\nSearch\nTransformations\nDownload\n\n\n\n\n\nDescription\nAppEEARS simplifies geospatial data access and transformation, allowing users to easily subset datasets from multiple federal archives by point or area locations, time, and layers. The tool provides interactive data previews with quality decoding, and enables users to explore their data before downloading.\nInterface\n\nGUI (Documentation)\nApplication Programming Interface (API) (Documentation)\n\nCapabilities\n\nSearch\nProcessing (point/area extraction, mosaic, clip, reproject, quality decoding, data format )\nExplore\nDownload/Access\n\n\n\n\n\nDescription\nEarthExplorer allows users to search, download, and order data held in USGS archives\nthrough many query options Interface\n\nGUI (Documentation)\nAPI (Machine2Machine API Resources)\n\nCapabilities\n\nSearch\nVisualization (Footprint)\nDownload\n\n\nNOTE: NASA LP DAAC data products were removed from USGS’s EarthExplorer and Machine-to-Machine (M2M) API on August 30, 2024 (News Announcement). Users are encouraged to use NASA’s Earthdata Search and/or AppEEARS to discover and download LP DAAC data products. For those who used M2M for programmatic search and access, please explore the AppEEARS API, the CMR API, or the earthaccess Python package\n\n\n\n\n\n\n\n\n\n\n\n\nHLSL30 v2.0\nHLSS30 v2.0\n\n\n\n\n\n\n\nEarthdata Search\nAppEEARS\n\n\n\n\n\nHLS Data Resources GitHub Repository\n\n\n\n\n\n\n\n\n\n\nECOSTRESS v2.0 Products - All Available Products\nECOSTRESS v1.0 Products\n\n\n\n\n\n\n\nEarthdata Search\nAppEEARS\n\n\n\n\n\nECOSTRESS Data Resources GitHub Repository\nECOSTRESS Tutorials managed by NASA JPL\nVSWIR Imaging and Thermal Applications, Learning, and Science (VITALS) GitHub Repository\n\n\n\n\n\n\n\n\n\n\nEMIT v1.0 Products - All Available Products\nEMIT L2A Estimated Surface Reflectance (EMITL2ARFL.001)\n\n\n\n\n\n\n\nEarthdata Search\nAppEEARS\n\n\n\n\n\nEMIT Data Resources GitHub Repository\nVSWIR Imaging and Thermal Applications, Learning, and Science (VITALS) GitHub Repository\n\n\n\n\n\n\n\n\n\n\nLandsat Collection 2 Information\n\n\n\n\n\n\n\nEarthExplorer\nAppEEARS - Has access to Landsat Collection 2 U.S. Analysis Ready Data\n\n\n\n\n\nhttps://code.usgs.gov/eros-user-services/\n\n\n\n\n\n\n\n\n\nearthaccess GitHub Repository\nearthaccess Documentation\nearthaccess tutorial\n\n\n\n\n\nLP DAAC Data Use Resources\n\n\n\n\n\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 09-23-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "workshops/NASA_DEVELOP_Fall_2024/NASA_DEVELOP_Land_Products_Guide.html#general-tools-and-applications",
    "href": "workshops/NASA_DEVELOP_Fall_2024/NASA_DEVELOP_Land_Products_Guide.html#general-tools-and-applications",
    "title": "NASA DEVELOP Land Data Products Guide - Fall 2024",
    "section": "",
    "text": "Data Catalog: https://lpdaac.usgs.gov/product_search/?status=Operational\nNews: https://lpdaac.usgs.gov/news/\nHelp: https://lpdaac.usgs.gov/lpdaac-contact-us/\n\n\n\n\n\n\nDescription\nEarthdata Search is NASA’s comprehensive data discovery and access platform. Search, visualize, and download Earth science data from NASA’s vast archives, covering climate, weather, land, ocean, and more.\nInterface\n\nGraphical User Interface (GUI)\n\nCapabilities\n\nSearch\nTransformations\nDownload\n\n\n\n\n\nDescription\nAppEEARS simplifies geospatial data access and transformation, allowing users to easily subset datasets from multiple federal archives by point or area locations, time, and layers. The tool provides interactive data previews with quality decoding, and enables users to explore their data before downloading.\nInterface\n\nGUI (Documentation)\nApplication Programming Interface (API) (Documentation)\n\nCapabilities\n\nSearch\nProcessing (point/area extraction, mosaic, clip, reproject, quality decoding, data format )\nExplore\nDownload/Access\n\n\n\n\n\nDescription\nEarthExplorer allows users to search, download, and order data held in USGS archives\nthrough many query options Interface\n\nGUI (Documentation)\nAPI (Machine2Machine API Resources)\n\nCapabilities\n\nSearch\nVisualization (Footprint)\nDownload\n\n\nNOTE: NASA LP DAAC data products were removed from USGS’s EarthExplorer and Machine-to-Machine (M2M) API on August 30, 2024 (News Announcement). Users are encouraged to use NASA’s Earthdata Search and/or AppEEARS to discover and download LP DAAC data products. For those who used M2M for programmatic search and access, please explore the AppEEARS API, the CMR API, or the earthaccess Python package"
  },
  {
    "objectID": "workshops/NASA_DEVELOP_Fall_2024/NASA_DEVELOP_Land_Products_Guide.html#data-collection-resources",
    "href": "workshops/NASA_DEVELOP_Fall_2024/NASA_DEVELOP_Land_Products_Guide.html#data-collection-resources",
    "title": "NASA DEVELOP Land Data Products Guide - Fall 2024",
    "section": "",
    "text": "HLSL30 v2.0\nHLSS30 v2.0\n\n\n\n\n\n\n\nEarthdata Search\nAppEEARS\n\n\n\n\n\nHLS Data Resources GitHub Repository\n\n\n\n\n\n\n\n\n\n\nECOSTRESS v2.0 Products - All Available Products\nECOSTRESS v1.0 Products\n\n\n\n\n\n\n\nEarthdata Search\nAppEEARS\n\n\n\n\n\nECOSTRESS Data Resources GitHub Repository\nECOSTRESS Tutorials managed by NASA JPL\nVSWIR Imaging and Thermal Applications, Learning, and Science (VITALS) GitHub Repository\n\n\n\n\n\n\n\n\n\n\nEMIT v1.0 Products - All Available Products\nEMIT L2A Estimated Surface Reflectance (EMITL2ARFL.001)\n\n\n\n\n\n\n\nEarthdata Search\nAppEEARS\n\n\n\n\n\nEMIT Data Resources GitHub Repository\nVSWIR Imaging and Thermal Applications, Learning, and Science (VITALS) GitHub Repository\n\n\n\n\n\n\n\n\n\n\nLandsat Collection 2 Information\n\n\n\n\n\n\n\nEarthExplorer\nAppEEARS - Has access to Landsat Collection 2 U.S. Analysis Ready Data\n\n\n\n\n\nhttps://code.usgs.gov/eros-user-services/"
  },
  {
    "objectID": "workshops/NASA_DEVELOP_Fall_2024/NASA_DEVELOP_Land_Products_Guide.html#additional-resources",
    "href": "workshops/NASA_DEVELOP_Fall_2024/NASA_DEVELOP_Land_Products_Guide.html#additional-resources",
    "title": "NASA DEVELOP Land Data Products Guide - Fall 2024",
    "section": "",
    "text": "earthaccess GitHub Repository\nearthaccess Documentation\nearthaccess tutorial\n\n\n\n\n\nLP DAAC Data Use Resources"
  },
  {
    "objectID": "workshops/NASA_DEVELOP_Fall_2024/NASA_DEVELOP_Land_Products_Guide.html#contact-info",
    "href": "workshops/NASA_DEVELOP_Fall_2024/NASA_DEVELOP_Land_Products_Guide.html#contact-info",
    "title": "NASA DEVELOP Land Data Products Guide - Fall 2024",
    "section": "",
    "text": "Email: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 09-23-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I."
  }
]