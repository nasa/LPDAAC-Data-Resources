{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a27fa42-7be7-4d5c-9371-24afc9089d16",
   "metadata": {},
   "source": [
    "# Introduction to NASA `earthaccess`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea388888-13ed-4959-a88d-5528e97e39b2",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "[`earthaccess`](https://github.com/nsidc/earthaccess) is a Python library that provides a unified and reproducible approach to find and access NASA Earth science data. It handles authentication, and provides an efficient way to search [NASA Common Metadata Repository (CMR)](https://cmr.earthdata.nasa.gov/) and access the data via bulk download or direct access. Specifically, this notebook demonstrates how to search for Earth science data collections and data using the `earthaccess` package.\n",
    "\n",
    "A NASA Earthdata Login account ([EDL](https://urs.earthdata.nasa.gov/profile)) is required to download or access data. Earthdata Login accounts are free and can be set up in only a few minutes. Remember your EDL username and password as they are needed for authentication in this and other data access resources.\n",
    "\n",
    "**Requirements**  \n",
    "\n",
    "- A NASA [Earthdata Login](https://urs.earthdata.nasa.gov/) account is required.   \n",
    "- A compatible Python environment. Please see the [Python setup instructions](https://github.com/nasa/LPDAAC-Data-Resources/blob/main/setup/setup_instructions_python.md) to create a compatible local environment.\n",
    "\n",
    "**Learning Objectives**  \n",
    "\n",
    "- How to search data collections using a keyword with `earthaccess`\n",
    "- How to search for data using spatiotemporal parameters\n",
    "- How to work with `earthaccess` request objects\n",
    "- How do direct access or download data\n",
    "\n",
    "**Contents**\n",
    "\n",
    "1. [Setup](#setup)\n",
    "2. [Authentication](#auth)\n",
    "3. [Searching for Datasets AKA Collections](#collections)\n",
    "4. [Searching for Data AKA Granules](#granules)\n",
    "5. [Working with Search Results](#results)\n",
    "6. [Downloading and Streaming Data](#download)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba2387-54e8-4aa5-aedb-c7d90644536f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Setup <a name=\"setup\"></a>\n",
    "\n",
    "Let's start by loading the required packages. If you need to set up a local Python environment, please see the [Python setup instructions](https://github.com/nasa/LPDAAC-Data-Resources/blob/main/setup/setup_instructions_python.md) to create a compatible environment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f6c9ed-fe58-4e03-b29b-c6c447061f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "from shapely.geometry.polygon import orient\n",
    "import hvplot.pandas\n",
    "\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "import hvplot.xarray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95121ff7-5222-4778-a4de-25625e23884b",
   "metadata": {},
   "source": [
    "## 2. Authentication <a name=\"auth\"></a>\n",
    "\n",
    "`earthaccess` creates and leverages Earthdata Login tokens to authenticate with NASA systems. Earthdata Login tokens expire after a month, but `earthaccess` will refresh these for you. To retrieve a token from Earthdata Login, you can either enter your username and password each time you use `earthaccess`, or use a `.netrc` file. A `.netrc` file is a configuration file that is commonly used to store login credentials for remote systems. If you don't have a `.netrc` or don't know if you have one or not, you can use the `persist` argument with the `login` function below to create or update an existing one, then use it for authentication.\n",
    "\n",
    "If you do not have an Earthdata Account, you can [create one](https://urs.earthdata.nasa.gov/home)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a0fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = earthaccess.login(persist = True)\n",
    "# are we authenticated?\n",
    "print(auth.authenticated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc8b461-8c68-4719-94e5-34057159dac7",
   "metadata": {},
   "source": [
    "## 3. Searching for Datasets, AKA Collections <a name=\"collections\"></a>\n",
    "\n",
    "We need information about the collection or dataset we're interested in before we can do an effective search for data. We'll use the `search_datasets()` function with the `keyword` and `provider` arguments to query for collections that match. The provider argument isn't necessary, but if you know which DAAC hosts data you can provide this to further filter results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8889b3fb-9c86-4de3-a5cf-ec53fb9fb487",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = earthaccess.search_datasets(\n",
    "    provider='LPCLOUD',    # LPCLOUD is the LP DAAC Archive in Earthdata Cloud\n",
    "    keyword='ecostress',\n",
    ")\n",
    "# Print Quantity of Results\n",
    "print(f'Collections found: {len(collections)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26962664-cbe8-453f-b617-80d473df9c75",
   "metadata": {},
   "source": [
    "Results from `search_datasets` are returned as enhanced python dictionaries which, can be interacted with like any Python dictionary. Let's take a look at the first collection in `collections`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceadab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = collections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18354eef-d687-4ab2-944d-dc1f20322c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153bc19-b07d-4bf4-9daf-264546026b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection['umm'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb5154c-f131-44ad-a68f-cf0fa21ce18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection['umm']['ShortName']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d45a6f-ac37-4744-bcfe-88ac3dd6ac07",
   "metadata": {},
   "source": [
    "The `DataCollections` class also has some handy helper methods.\n",
    "\n",
    "```python \n",
    "collection.concept_id() # returns the concept-id, used to search for data granules\n",
    "collection.abstract() # returns the abstract\n",
    "collection.landing_page() # returns the landing page if present in the UMM fields\n",
    "collection.get_data() # returns the portal where data can be accessed.\n",
    "```\n",
    "\n",
    "The same results can be obtained using the `dict` syntax:\n",
    "\n",
    "```python\n",
    "collection[\"meta\"][\"concept-id\"] # concept-id\n",
    "collection[\"umm\"][\"RelatedUrls\"] # URLs, with GET DATA, LANDING PAGE etc\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07003f4b-ae18-4175-aad4-c2e99b1f3eb1",
   "metadata": {},
   "source": [
    "Another helpful method is `summary()`. This method prints some of the more common collection metadata information used for additional queries against the individual collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318d9c5-107b-4338-b0e5-eb373481976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd78afe0-04e5-4e76-9b6c-2b668e55fd4c",
   "metadata": {},
   "source": [
    "The most common way to query for information or metadata related to a collection is to use the **concept-id**, the collection **doi**, or a combination of both the **short-name** and **version**. The `summary()` method gives us the **concept-id**, **short-name**, and **version**. We can use this knowledge to create a list containing this information for later queries against the specific collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1ddc8-5478-4e3d-9239-ce29834b7bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections_info = [\n",
    "    {\n",
    "        'short_name': c.summary()['short-name'],\n",
    "        'collection_concept_id': c.summary()['concept-id'],\n",
    "        'version': c.summary()['version'],\n",
    "        'entry_title': c['umm']['EntryTitle']\n",
    "    }\n",
    "    for c in collections\n",
    "]\n",
    "# Preview First Collection\n",
    "collections_info[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf09ce9",
   "metadata": {},
   "source": [
    "We can also turn this into a dataframe for easy visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c7f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview as a dataframe\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "pd.DataFrame(collections_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb9c3bb-ac8b-48e8-8233-8c44da8fb7bc",
   "metadata": {},
   "source": [
    "## 4. Searching for Granules <a name=\"granules\"></a>\n",
    "\n",
    "A `granule` can be thought of as a unique spatiotemporal grouping within a collection. To search for `granules`, we can use the `search_data` function from `earthaccess` and provide the arguments for our search based on the collection information we retrieved above. Its possible to specify search products using several criteria shown in the table below:\n",
    "\n",
    "|dataset origin and location|spatio temporal parameters|dataset metadata parameters|\n",
    "|:---|:---|:---|\n",
    "|archive_center|bounding_box|concept_id\n",
    "|data_center|temporal|entry_title\n",
    "|daac|point|granule_name\n",
    "|provider|polygon|version\n",
    "|cloud_hosted|line|short_name\n",
    "\n",
    "In this example, we will use the **short_name** and the **version** to query for data granules from [`ECO_L2T_LSTE`](https://doi.org/10.5067/ECOSTRESS/ECO_L2_LSTE.002) version `002` dataset. You can search data granules using just a **short_name** but there is the potential that multiple versions of the data collection will be return. To query for granules in a more explicit way, a **concept-id** would be the best option.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364d737-5a79-4089-853f-76d2ad1c85a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build our query\n",
    "granules_request = earthaccess.search_data(\n",
    "    short_name='ECO_L2T_LSTE',\n",
    "    version='002',\n",
    "    provider='LPCLOUD',\n",
    "    count=1\n",
    ")\n",
    "print(f'Granules found: {len(granules_request)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c678a5-cb93-4986-bb3f-2a76623acdcc",
   "metadata": {},
   "source": [
    "Without our **count** argument used to restrict the quantity of results, this query would have returned a lot of granules, over 13 million, and taken a long time to complete. Let's refine our search using **bounding box** and **temporal** arguments as constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f32a4-026d-4a5f-af66-69026cabe966",
   "metadata": {},
   "source": [
    "### Spatiotemporal queries\n",
    "\n",
    "The `earthaccess.results.DataGranule` and `earthaccess.results.DataCollection` classes accept the same spatial and temporal arguments as CMR, so we can search for granules that match spatiotemporal criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ddb145-c7dd-4a13-93bc-1e257dcde432",
   "metadata": {},
   "source": [
    "#### Specifying Spatial Parameters\n",
    "\n",
    "Search queries can be refined by using spatial parameters. `earthaccess` accepts point and area arguments. For point features a longitude and latitude coordinate pair must be passes as a tuple to the `point` parameter. For example:\n",
    "\n",
    "```\n",
    "point = (-105.64788824641289,39.98286247719818)\n",
    "```\n",
    "\n",
    "For area features, queries can leverage the `bounding_box` or a `polygon`.\n",
    "\n",
    "The `bounding_box` argument accepts a tuple containing coordinates in the order lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat:\n",
    "\n",
    "```\n",
    "bbox = (-105.58650854045227, 40.05184049311418, -105.5832945453711, 40.05424331298521)\n",
    "```\n",
    "\n",
    "For this example we'll use a `polygon` for a spatial parameter.\n",
    "\n",
    "\n",
    "The `polygon` argument accepts a list of vertices in counter-clockwise order:\n",
    "\n",
    "```\n",
    "polygon = [(-105.58501133091315, 40.05184049311418),\n",
    " (-105.5832945453711, 40.053984736232714),\n",
    " (-105.58351661636773, 40.05424331298521),\n",
    " (-105.5839952821831, 40.05403439678874),\n",
    " (-105.5864783735272, 40.05234816435479),\n",
    " (-105.58650854045227, 40.052056831297655),\n",
    " (-105.58559965168544, 40.051883708267695),\n",
    " (-105.58501133091315, 40.05184049311418)]\n",
    "\n",
    "```\n",
    "This list or the bbox can be retrieved from a geojson or shapefile easily using the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b55f3-c2a4-4a45-9f1f-4e5fd7b10a1b",
   "metadata": {},
   "source": [
    "Open a **geojson** file and visualize it to understand our region of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac54bfc2-48e6-423d-8309-3b5fa3b9f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson = gp.read_file('../../data/NIWO_box.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7594563-2e98-4c7d-b881-50939bf787b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_plot = geojson.hvplot(tiles='ESRI', color='yellow', alpha=0.5, crs='EPSG:4326')\n",
    "geojson_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068e1d7-3d85-442d-9c4b-9df963e646ce",
   "metadata": {},
   "source": [
    "A **shapefile** can also be opened similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648b67d-55cc-4b1b-89d0-416701d2df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp = gp.read_file('../../data/NIWO_ShrubDensity.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7c1d8-d55d-44d3-a005-0bf48c7a2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_plot = shp.hvplot(tiles='ESRI', color='blue', alpha=0.5, crs='EPSG:4326')\n",
    "geojson_plot * shp_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdfd49e-9cda-4626-8a0f-1b36517c3fd4",
   "metadata": {},
   "source": [
    "We can get the bounding box that encompasses all of the features using the `total_bounds` method and pass it to the `bounding_box` parameter in our search query.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38921a-0a0c-4c34-8f0a-2cb18f95935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = tuple(list(shp.total_bounds))\n",
    "bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be705a5b",
   "metadata": {},
   "source": [
    "Retrieving the `polygon` can be done also, but we need to ensure the vertices are in counter-clockwise order. For this example we actually have 4 geometries, so we can either search for each polygon, which can be beneficial in some scenarios, or create a convex hull around them to get the smallest spatial area including all 4, which we will do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ad2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convex Hull \n",
    "shp_ch = shp.union_all().convex_hull\n",
    "shp_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon = orient(shp_ch, sign=1.0)\n",
    "polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e573f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the exterior coordinates \n",
    "polygon_coords = list(polygon.exterior.coords)\n",
    "polygon_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3843f120",
   "metadata": {},
   "source": [
    "We can now use this as our `polygon` in the search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eaf940-33db-49d4-b19f-85822c122a79",
   "metadata": {},
   "source": [
    "#### Specify Temporal Parameters  \n",
    "\n",
    "To specify the date and time period we are interested in, we pass a tuple containing the start and end datetime in ISO-8601 format (YYYY-MM-DDTHH:mm:ss). We can stop at any increment and the rest will be autocompleted. Examples: `2023-05-01T12:15:30.000`, `2023-05-01T12:15`, or `2023-05-01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42def03f-cc00-4c0f-958e-93b65b872916",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ('2023-05-01','2023-09-30')    # tuple containing the start and end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b2a52-0fdc-4b4b-a4ff-6ffafe2e1aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    short_name='ECO_L2T_LSTE',\n",
    "    version='002',\n",
    "    provider='LPCLOUD',\n",
    "    polygon=polygon_coords,\n",
    "    temporal=dates,\n",
    ")\n",
    "# Print Quantity of Results\n",
    "print(f'Granules found: {len(results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ec429",
   "metadata": {},
   "source": [
    "If we wanted to perform a query for a point or bounding_box, we would only need to swap out the `polygon` parameter for the `point` or `bounding_box` parameter.\n",
    "\n",
    "> **Note that we are limiting our results to a count of 100.**\n",
    "\n",
    "Additionally, a `granule_name` argument can be used to filter based on the names of the granules. This is handy for tiled datasets like the one we're working with, ECOSTRESS L2T LSTE, which uses the [Harmonized Landsat Sentinel-2 tiling system](https://hls.gsfc.nasa.gov/products-description/tiling-system/). This argument can also be used for other tiled datasets like MODIS or VIIRS, or any other string contained in the granule name. We can specify a tile, like below, using asterisks to indicate a wildcard on either side of the specified string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8fface",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_results = earthaccess.search_data(\n",
    "    short_name='ECO_L2T_LSTE',\n",
    "    version='002',\n",
    "    provider='LPCLOUD',\n",
    "    granule_name='*13TDE*',\n",
    "    temporal=dates)\n",
    "# Print Quantity of Results\n",
    "print(f'Granules found: {len(tile_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ea25e",
   "metadata": {},
   "source": [
    "## 5. Working with Search Results <a name=\"results\"></a>\n",
    "\n",
    "Similarly to our search for collections, our search for granules is returned as an enhanced python dictionary, so we can interact with them in the same ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc41f61-2a9e-4883-b7e5-d4c1cd716eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'results is a {type(results)} of {type(results[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1a1d8-468f-4530-808f-f0b490eb98f5",
   "metadata": {},
   "source": [
    "Let's look at the first granule from our bounding box query.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927f65d8-a9d1-461a-ac34-bf4f31b8c0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "granule = results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f351a16c",
   "metadata": {},
   "source": [
    "Since we are in a notebook we can take advantage of it to see a more user friendly version of the granules by calling the object or using the built-in function `display`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef15051",
   "metadata": {},
   "outputs": [],
   "source": [
    "granule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e968b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display multiple granules\n",
    "[display(granule) for granule in granules_request[0:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31abdf3",
   "metadata": {},
   "source": [
    "Now inspect the dictionary returned by looking at the different metadata available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e7a05-3134-45d1-ae1f-cadc89f92de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "granule.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99196be-d618-49fe-a8cd-7e9a90a1d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "granule['umm'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0fee2-ee65-4a4d-a443-82bf1c4f2b27",
   "metadata": {},
   "source": [
    "The `DataGranule` class returned by our search also has several convenience methods. The `data_links()` method can extract all of the data links associated with each granule. We can specify `access` as `direct` to retrieve S3 links rather than https links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d714974-de98-40e0-a0dc-3aa2888b7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access='direct' for s3 links.\n",
    "granule.data_links(access='indirect')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c971cb-9936-4d4f-904f-b30cffca2dff",
   "metadata": {},
   "source": [
    "Granules for **ECO_L2T_LSTE** are made up of multiple files. This is the case for several of the `LPCLOUD` provider collection. Other collections may only have a single file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eed971",
   "metadata": {},
   "source": [
    "At this stage, we can take advantage of the `earthaccess.open` function to provide credentials and stream the data, or use `earthaccess.download` to download our result; however, when collections have multiple files per granule, its often helpful to first filter down to only the desired variables. We can see in the list of links above that the dataset has several variables:\n",
    "- water mask\n",
    "- cloud mask\n",
    "- view zenith\n",
    "- height\n",
    "- quality\n",
    "- land surface temperature (LST)\n",
    "- LST error\n",
    "- emissivity\n",
    "\n",
    "If we only want the LST, quality, and cloud mask we can use string matching to only keep those links. \n",
    "\n",
    "Before we do this, lets first get all of the links from our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d67f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [granule.data_links() for granule in results]\n",
    "links[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e764d59f",
   "metadata": {},
   "source": [
    "Note that this returns a nested list of granules, where each granule has a list of links. We can also flatten this list if we want to work without nesting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [link for granule in results for link in granule.data_links()]\n",
    "links[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d48f6",
   "metadata": {},
   "source": [
    "Now use string matching to select the variables (links) we want from the list (LST, cloud, QC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee952e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Desired Links/Assets\n",
    "variables = ['LST.tif','QC.tif','cloud.tif']\n",
    "# Filter Links\n",
    "filtered_links = [link for link in links if any(var in link for var in variables)]\n",
    "# Display first 10 links\n",
    "filtered_links[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b247449d-448b-41c5-b52d-a5b4e4dff079",
   "metadata": {},
   "source": [
    "## 6. Downloading or Streaming Data <a name=\"download\"></a>\n",
    "\n",
    "There are two typical routes users can take to access LP DAAC data, Downloading or Streaming.\n",
    "\n",
    "**Downloading** – This has been a supported option since the inception of NASA's DAACs. Users can use the data link(s) to download files to their local working environment. This method works in both cloud and non-cloud environments.\n",
    "\n",
    "**Streaming** – Streaming enables on-the-fly reading of remote files (i.e., files not saved locally). However, the accessed data must fit into the workspace’s memory. Streaming works in both cloud and non-cloud environments, although is more performant if in the cloud and the dataset is in a cloud-optimized format. Streaming data stored in the cloud without downloading is called **in-place access or direct S3 access**. This is only available when working in a cloud environment deployed in AWS us-west-2. If outisde the cloud, \n",
    "\n",
    "\n",
    "### 6.1 Downloading Data\n",
    "\n",
    "As mentioned above, the `earthaccess.download` function can be used immediately after results are returned, but this will retrieve all of the files and often we want to filter these down first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c44f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all results from count=1 example above\n",
    "earthaccess.download(granules_request, local_path='../../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e989842a",
   "metadata": {},
   "source": [
    "We can also pass a list of links to this function. Download the first 6 files from our `filtered_links`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c301801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download first 6 links from filtered results\n",
    "earthaccess.download(filtered_links[:6], local_path='../../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa65c924",
   "metadata": {},
   "source": [
    "### 6.2 Streaming Data\n",
    "\n",
    "There are different approaches for streaming data based upon the filetype or if you are working in-region in the AWS uswest-2 cloud. ECOSTRESS L2T LSTE data are distributed as cloud-optimized geotiff (COG) files, so we'll show an example of streaming COG files with HTTPS links here. Some of our other tutorials have other examples that may be helpful depending on your goals:\n",
    "\n",
    "- [Streaming COG files with S3 Links](https://github.com/nasa/ECOSTRESS-Data-Resources/blob/main/python/how-tos/how_to_direct_access_s3_ecostress_cog.ipynb)\n",
    "- [Streaming NetCDF4 files with HTTPS Links](https://github.com/nasa/VITALS/blob/main/python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.ipynb)\n",
    "- [Streaming NetCDF4 files with S3 Links](https://github.com/nasa/EMIT-Data-Resources/blob/main/python/how-tos/How_to_Direct_S3_Access.ipynb)\n",
    " \n",
    "The Python libraries used to access COG files in Earthdata Cloud leverage GDAL's virtual file systems. Whether you are running this code in the Cloud or in a local workspace, GDAL configurations must be set in order to successfully access the GLanCE30 COG files. For this exercise, we are going to open up a context manager for the notebook using the `rasterio.env` module to store these configurations. The context manager sends this information, including an authentication token or cookie when connecting to a file and can also customize how the file is handled locally. A list of all available config options can be found in the [GDAL config options documentation](https://gdal.org/en/stable/user/configoptions.html).\n",
    "\n",
    "While the context manager is open (env.enter()) we will be able to run the open or get data commands that would typically be executed within a \"with\" statement. Entering the context manager for multiple cells of the notebook allows us to more freely interact with the data. We’ll close the context manager (env.exit()) when we have all of the data loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be624221-013d-43c0-8679-ff945be552d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Context Manager\n",
    "rio_env = rio.Env(\n",
    "    GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\",\n",
    "    GDAL_HTTP_UNSAFESSL=\"YES\",\n",
    "    GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n",
    "    GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'),\n",
    "    GDAL_HTTP_MAX_RETRY=\"10\",\n",
    "    GDAL_HTTP_RETRY_DELAY=\"0.5\",\n",
    ")\n",
    "rio_env.__enter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728b2f0",
   "metadata": {},
   "source": [
    "Let's open an LST file. We can quickly grab just the LST links from our `filtered_links`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ff95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_links = [link for link in filtered_links if 'LST.tif' in link]\n",
    "lst_links[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d82499",
   "metadata": {},
   "source": [
    "Now we'll use `rioxarray` to read the first file in our list. `rioxarray` reads the COG file in as an `xarray` `DataArray`. The `mask_and_scale` argument can be used to automatically set fill-values to `np.nan` and apply any scale factor and offsets included in the metadata. When a COG file is read in using `rioxarray`, a **band** dimension is created. In most circumstances this dimension can be removed. We use the `squeeze` function to remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f3b98c-1465-46b8-9147-632e7ccea52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_da = rxr.open_rasterio(filename=lst_links[0], mask_and_scale=True).squeeze('band', drop=True)\n",
    "lst_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69af654e-5e92-4938-9b1f-bed71427828c",
   "metadata": {},
   "source": [
    "We can make a cursory spatial plot of the data using `hvplot`, reprojecting so it will match basemap tiles. Note that our example shape is very small, so you'll have to zoom in on the center of the scene to see the several small plots from our polygon search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f6075-c149-424a-9889-d6a84e4a63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_opts = dict(frame_height=405, frame_width=720, fontscale=1.2)\n",
    "\n",
    "lst_da.rio.reproject('EPSG:4326').hvplot.image(x='x', y='y', **size_opts, cmap='inferno', tiles='ESRI', crs='EPSG:4326', title=f'{lst_links[0].split('/')[-1]}') * shp.hvplot(color = '#FF000000', crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55444ad4-1c74-4806-8048-b82284f403f0",
   "metadata": {},
   "source": [
    "## Contact Information  \n",
    "\n",
    "**Authors:**  LP DAAC¹  \n",
    "**Contact:** LPDAAC@usgs.gov  \n",
    "**Voice:** +1-866-573-3222  \n",
    "**Organization:** Land Processes Distributed Active Archive Center (LP DAAC)  \n",
    "**Website:** [https://lpdaac.usgs.gov/](https://lpdaac.usgs.gov/)  \n",
    "\n",
    "¹Work performed under USGS contract G15PD00467 for LP DAAC under NASA contract NNG14HH33I.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lpdaac_vitals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
